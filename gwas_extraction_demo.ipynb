{
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GWAS Intelligence)",
   "language": "python",
   "name": "gwas-intelligence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "lastEditStatus": {
   "notebookId": "lzjwxxwhdebmbpf5pn4a",
   "authorId": "203779562157",
   "authorName": "JHOLT",
   "authorEmail": "jason.holt@snowflake.com",
   "sessionId": "aeafa599-8656-4fe1-a50a-983af1f446b4",
   "lastEditTime": 1761837439443
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Project_Overview",
    "collapsed": false
   },
   "source": "# You will need to upload PDF's Manually to a stage in the Upload_PDF_STAGE Step\n\n# üß¨ GWAS Intelligence Pipeline - Standalone Notebook\n\nThis notebook is a **complete, standalone** pipeline for extracting genomic trait data from research papers using Snowflake Cortex AI and multimodal RAG.\n\n## What This Notebook Does\n\n1. **Database Setup** - Creates GWAS database, schemas, stages, and tables\n2. **PDF Processing** - Parses PDFs using Cortex AI (**batch or single-file**)\n3. **Embedding Generation** - Creates text and image embeddings\n4. **Trait Extraction** - Extracts GWAS traits using multimodal RAG\n5. **Analytics** - Provides extracted trait analytics\n\n## ‚ú® NEW: Batch Processing Support\n\nThis notebook now supports **batch processing** of multiple PDFs:\n- ‚úÖ **Automatic file discovery** from Snowflake stage\n- ‚úÖ **Process multiple PDFs** in one run with progress tracking\n- ‚úÖ **Smart skip logic** - automatically skips already-processed files\n- ‚úÖ **Error handling** - one failure doesn't stop the batch\n- ‚úÖ **Comprehensive statistics** - batch summaries and processing metrics\n- ‚úÖ **Flexible modes** - process all files or select specific ones\n\n## Prerequisites\n\n- Snowflake account with Cortex AI access\n- CREATE DATABASE privileges\n- Warehouse for compute\n- `.env` file with credentials (local development) OR\n- Running in Snowflake Notebooks (Container Runtime)\n\n## Quick Start\n\n### Single File Mode (Original)\n1. Configure `.env` file with your Snowflake credentials\n2. Upload a PDF to the stage (instructions in notebook)\n3. Set `PDF_FILENAME` variable\n4. Run all cells in order\n\n### Batch Mode (NEW!)\n1. Configure `.env` file with your Snowflake credentials\n2. Upload multiple PDFs to the stage\n3. Run Section 4a cells (batch processing)\n4. Monitor progress with automatic statistics\n\n---",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Install_External_Packages",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "!pip install PyMuPDF"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell21"
   },
   "source": [
    "## üì¶ CELL 1: Section 1 - Setup & Imports"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell22",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add scripts directory to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.append(str(project_root / \"scripts\" / \"python\"))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"   Project root: {project_root}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell3"
   },
   "source": [
    "## üóÑÔ∏è Step 1: Database & Schema Setup\n",
    "\n",
    "Create the GWAS database and required schemas.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdc58b-d730-4c2a-8aaa-3a98498e6d06",
   "metadata": {
    "name": "Local_Dev_Cells",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD ENVIRONMENT VARIABLES (for local development)\n",
    "# ============================================================================\n",
    "# Automatically loads .env file if it exists\n",
    "# For Snowflake Notebooks, this cell will silently skip\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Look for .env file in current directory or parent\n",
    "    env_path = Path('.env')\n",
    "    if not env_path.exists():\n",
    "        env_path = Path('../.env')\n",
    "    \n",
    "    if env_path.exists():\n",
    "        load_dotenv(env_path)\n",
    "        print(f\"‚úÖ Loaded environment variables from: {env_path.absolute()}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No .env file found (this is OK if running in Snowflake Notebook)\")\n",
    "        print(\"   For local development, create .env file with Snowflake credentials\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  python-dotenv not installed (this is OK if running in Snowflake Notebook)\")\n",
    "    print(\"   For local development: pip install python-dotenv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Build_Session",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONNECT TO SNOWFLAKE (works in both local and Snowflake Notebooks)\n",
    "# ============================================================================\n",
    "from snowflake.snowpark import Session\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # ========================================================================\n",
    "    # METHOD 1: Try to use active session (Snowflake Notebooks / Container Runtime)\n",
    "    # ========================================================================\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    session = get_active_session()\n",
    "    \n",
    "    print(\"‚úÖ Connected to Snowflake using active session\")\n",
    "    print(\"   üèîÔ∏è Running in Snowflake Notebook (Container Runtime)\")\n",
    "    print(f\"   Account: {session.get_current_account()}\")\n",
    "    print(f\"   User: {session.get_current_user()}\")\n",
    "    print(f\"   Role: {session.get_current_role()}\")\n",
    "    print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "    print(f\"   Database: {session.get_current_database() or '(not set)'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # ========================================================================\n",
    "    # METHOD 2: Use credentials from environment (local development)\n",
    "    # ========================================================================\n",
    "    print(\"üíª Running locally - connecting with credentials from .env\")\n",
    "    \n",
    "    # Check if required env vars are set\n",
    "    required_vars = [\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_USER\", \"SNOWFLAKE_PASSWORD\"]\n",
    "    missing_vars = [var for var in required_vars if not os.environ.get(var)]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"\\n‚ùå Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "        print(\"\\nüí° Create a .env file in the project root with:\")\n",
    "        print(\"   SNOWFLAKE_ACCOUNT=your_account\")\n",
    "        print(\"   SNOWFLAKE_USER=your_username\")\n",
    "        print(\"   SNOWFLAKE_PASSWORD=your_password\")\n",
    "        print(\"   SNOWFLAKE_ROLE=ACCOUNTADMIN  # optional\")\n",
    "        print(\"   SNOWFLAKE_WAREHOUSE=COMPUTE_WH  # optional\")\n",
    "        print(\"\\n   Then install: pip install python-dotenv\")\n",
    "        print(\"   And load it: from dotenv import load_dotenv; load_dotenv()\")\n",
    "        raise ValueError(f\"Missing environment variables: {missing_vars}\")\n",
    "    \n",
    "    # Get connection from environment or use defaults\n",
    "    session = Session.builder.configs({\n",
    "        \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\"),\n",
    "        \"user\": os.environ.get(\"SNOWFLAKE_USER\"),\n",
    "        \"password\": os.environ.get(\"SNOWFLAKE_PASSWORD\"),\n",
    "        \"role\": os.environ.get(\"SNOWFLAKE_ROLE\", \"ACCOUNTADMIN\"),\n",
    "        \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\", \"COMPUTE_WH\"),  # Default warehouse\n",
    "    }).create()\n",
    "    \n",
    "    print(\"‚úÖ Connected to Snowflake using credentials\")\n",
    "    print(f\"   Account: {session.get_current_account()}\")\n",
    "    print(f\"   User: {session.get_current_user()}\")\n",
    "    print(f\"   Role: {session.get_current_role()}\")\n",
    "    print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "\n",
    "print(\"\\nüîå Snowflake session ready!\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Config_DB",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DATABASE_NAME = \"GWAS\"  # Name of the database for GWAS analysis\n",
    "SCHEMA_RAW = \"PDF_RAW\"  # Schema for raw PDF data\n",
    "SCHEMA_PROCESSING = \"PDF_PROCESSING\"  # Schema for processed data\n",
    "WAREHOUSE_NAME = \"COMPUTE_WH\"  # Default compute warehouse\n",
    "\n",
    "print(f\"‚úÖ Database configuration set:\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Raw Schema: {SCHEMA_RAW}\")\n",
    "print(f\"   Processing Schema: {SCHEMA_PROCESSING}\")\n",
    "print(f\"   Warehouse: {WAREHOUSE_NAME}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_DB",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create database\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\").collect()\n",
    "print(f\"‚úÖ Database {DATABASE_NAME} created/verified\")\n",
    "\n",
    "# Use database\n",
    "session.sql(f\"USE DATABASE {DATABASE_NAME}\").collect()\n",
    "\n",
    "# Create schemas\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {SCHEMA_RAW}\n",
    "    COMMENT = 'Raw PDF data from AI_PARSE_DOCUMENT'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Schema {SCHEMA_RAW} created/verified\")\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {SCHEMA_PROCESSING}\n",
    "    COMMENT = 'Processed PDF data, embeddings, and analytics'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Schema {SCHEMA_PROCESSING} created/verified\")\n",
    "\n",
    "# Verify schemas exist\n",
    "schemas = session.sql(\"SHOW SCHEMAS\").collect()\n",
    "print(f\"\\nüìä Available schemas in {DATABASE_NAME}:\")\n",
    "for schema in schemas:\n",
    "    print(f\"   - {schema['name']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Database and schemas ready!\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## üì¶ Step 2: Create Stage\n",
    "\n",
    "Create stage for storing PDF files, extracted images, and text files.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_Stage",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create stage for PDF and asset storage\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS PDF_STAGE\n",
    "    DIRECTORY = (ENABLE = TRUE)\n",
    "    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n",
    "    COMMENT = 'Storage for PDF files, extracted images, and text'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Stage PDF_STAGE created/verified in {DATABASE_NAME}.{SCHEMA_RAW}\")\n",
    "\n",
    "# Verify stage exists\n",
    "stages = session.sql(\"SHOW STAGES\").collect()\n",
    "print(f\"\\nüì¶ Available stages:\")\n",
    "for stage in stages:\n",
    "    print(f\"   - {stage['name']}\")\n",
    "\n",
    "print(f\"\\nüí° Upload PDFs using:\")\n",
    "print(f\"   PUT file:///path/to/file.pdf @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\")\n",
    "\n",
    "print(\"\\n‚úÖ Stage ready!\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## üìä Step 3: Create Tables\n",
    "\n",
    "Create all tables needed for the GWAS extraction pipeline.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_Parsed_Docs",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create PARSED_DOCUMENTS table in PDF_RAW schema\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS PARSED_DOCUMENTS (\n",
    "        document_id VARCHAR PRIMARY KEY,\n",
    "        file_path VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        parsed_content VARIANT NOT NULL,\n",
    "        total_pages INTEGER,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "    COMMENT = 'Raw PDF data from Cortex AI_PARSE_DOCUMENT'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table PARSED_DOCUMENTS created in {DATABASE_NAME}.{SCHEMA_RAW}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_PDF_Processing",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create TEXT_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_PROCESSING}\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS TEXT_PAGES (\n",
    "        page_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        page_text TEXT,\n",
    "        word_count INTEGER,\n",
    "        text_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Page text with embeddings for semantic search'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table TEXT_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_Image_pages",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create IMAGE_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS IMAGE_PAGES (\n",
    "        image_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        image_file_path VARCHAR NOT NULL,\n",
    "        image_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        dpi INTEGER DEFAULT 300,\n",
    "        image_format VARCHAR(10) DEFAULT 'PNG',\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Page images metadata for multimodal processing'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table IMAGE_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_Multmodal_Joins",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create MULTIMODAL_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS MULTIMODAL_PAGES (\n",
    "        page_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        image_id VARCHAR,\n",
    "        page_text TEXT,\n",
    "        image_path VARCHAR,\n",
    "        text_embedding VECTOR(FLOAT, 1024),\n",
    "        image_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        has_text BOOLEAN DEFAULT FALSE,\n",
    "        has_image BOOLEAN DEFAULT FALSE,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Combined text + image embeddings for multimodal RAG'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table MULTIMODAL_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_Table_PDF_Processing",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create GWAS_TRAIT_ANALYTICS table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS GWAS_TRAIT_ANALYTICS (\n",
    "        analytics_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        extraction_version VARCHAR(50),\n",
    "        finding_number INTEGER DEFAULT 1,\n",
    "        \n",
    "        -- Genomic traits\n",
    "        trait VARCHAR(500),\n",
    "        germplasm_name VARCHAR(500),\n",
    "        genome_version VARCHAR(100),\n",
    "        chromosome VARCHAR(50),\n",
    "        physical_position VARCHAR(200),\n",
    "        gene VARCHAR(500),\n",
    "        snp_name VARCHAR(200),\n",
    "        variant_id VARCHAR(200),\n",
    "        variant_type VARCHAR(100),\n",
    "        effect_size VARCHAR(200),\n",
    "        gwas_model VARCHAR(200),\n",
    "        evidence_type VARCHAR(100),\n",
    "        allele VARCHAR(100),\n",
    "        annotation TEXT,\n",
    "        candidate_region VARCHAR(500),\n",
    "        \n",
    "        -- Metadata\n",
    "        extraction_source VARCHAR(50),\n",
    "        field_citations VARIANT,\n",
    "        field_confidence VARIANT,\n",
    "        field_raw_values VARIANT,\n",
    "        traits_extracted INTEGER,\n",
    "        traits_not_reported INTEGER,\n",
    "        extraction_accuracy_pct FLOAT,\n",
    "        \n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, extraction_version, finding_number)\n",
    "    )\n",
    "    COMMENT = 'Extracted GWAS trait data from research papers'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table GWAS_TRAIT_ANALYTICS created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "id": "42413419-25bc-44eb-b888-35bb39d93a8b",
   "metadata": {
    "name": "cell16"
   },
   "source": [
    "## üßπ Step 3b: Cleanup & Reset Utilities (Optional)\n",
    "\n",
    "**Use these commands to reset your environment:**\n",
    "\n",
    "This section provides utilities to:\n",
    "- **Truncate all PDF tables** - Clear all processed data\n",
    "- **Delete files from stage** - Remove PDFs from stage\n",
    "- **Full reset** - Start completely fresh\n",
    "\n",
    "‚ö†Ô∏è **Warning**: These operations are destructive and cannot be undone!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ac742-4207-42bd-8fee-1aa1baa87730",
   "metadata": {
    "name": "Rollback_Reference"
   },
   "source": [
    "### Quick Reference - Common Cleanup Scenarios\n",
    "\n",
    "**Scenario 1: Start completely fresh**\n",
    "```python\n",
    "TRUNCATE_TABLES = True\n",
    "DELETE_STAGE_FILES = True\n",
    "SHOW_STATUS_ONLY = False\n",
    "# Then run the cell above\n",
    "```\n",
    "\n",
    "**Scenario 2: Re-process existing PDFs**\n",
    "```python\n",
    "TRUNCATE_TABLES = True      # Clear tables\n",
    "DELETE_STAGE_FILES = False  # Keep PDFs in stage\n",
    "SHOW_STATUS_ONLY = False\n",
    "# Then run the cell above, then re-run batch processing\n",
    "```\n",
    "\n",
    "**Scenario 3: Remove old PDFs, keep processed data**\n",
    "```python\n",
    "TRUNCATE_TABLES = False     # Keep data\n",
    "DELETE_STAGE_FILES = True   # Remove PDFs (saves storage)\n",
    "SHOW_STATUS_ONLY = False\n",
    "# Then run the cell above\n",
    "```\n",
    "\n",
    "**Scenario 4: Just check status (no changes)**\n",
    "```python\n",
    "SHOW_STATUS_ONLY = True  # Safe - no changes\n",
    "# Then run the cell above\n",
    "```\n",
    "\n",
    "### Alternative: SQL Commands\n",
    "\n",
    "You can also run these SQL commands directly in Snowsight:\n",
    "\n",
    "```sql\n",
    "-- Truncate all tables\n",
    "TRUNCATE TABLE GWAS.PDF_RAW.PARSED_DOCUMENTS;\n",
    "TRUNCATE TABLE GWAS.PDF_PROCESSING.TEXT_PAGES;\n",
    "TRUNCATE TABLE GWAS.PDF_PROCESSING.IMAGE_PAGES;\n",
    "TRUNCATE TABLE GWAS.PDF_PROCESSING.MULTIMODAL_PAGES;\n",
    "TRUNCATE TABLE GWAS.PDF_PROCESSING.GWAS_TRAIT_ANALYTICS;\n",
    "\n",
    "-- Remove all PDFs from stage (be careful!)\n",
    "REMOVE @GWAS.PDF_RAW.PDF_STAGE PATTERN='.*\\.pdf';\n",
    "\n",
    "-- Or remove specific file\n",
    "REMOVE @GWAS.PDF_RAW.PDF_STAGE/your-file.pdf;\n",
    "\n",
    "-- List files in stage\n",
    "LIST @GWAS.PDF_RAW.PDF_STAGE;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Upload_PDF_Stage"
   },
   "source": [
    "## üì§ Step 4: Upload PDF to Stage\n",
    "\n",
    "**‚ö†Ô∏è MANUAL UPLOAD REQUIRED**\n",
    "\n",
    "Please upload your PDF files manually to the stage using one of these methods:\n",
    "\n",
    "<!-- COMMENTED OUT - Use manual upload instead\n",
    "### Option 1: Using SnowSQL (Command Line)\n",
    "```bash\n",
    "# From terminal\n",
    "snowsql -a YOUR_ACCOUNT -u YOUR_USER\n",
    "PUT file:///Users/jholt/Downloads/fpls-15-1373081.pdf @GWAS.PDF_RAW.PDF_STAGE/;\n",
    "```\n",
    "\n",
    "### Option 2: Using Python (Below)\n",
    "Run the cell below to upload from your local system.\n",
    "-->"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Manual_Upload",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # PYTHON UPLOAD - COMMENTED OUT (Use manual upload instead)\n",
    "# # ============================================================================\n",
    "# # Upload files manually to @GWAS.PDF_RAW.PDF_STAGE/\n",
    "# # Then use the batch processing in Section 4a to process all files\n",
    "\n",
    "# print(\"‚ö†Ô∏è Python upload is disabled - please upload PDFs manually\")\n",
    "# print(f\"   Target stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\")\n",
    "# print(\"\\nüí° After manual upload, proceed to Section 4a for batch processing\")\n",
    "\n",
    "# # COMMENTED OUT - Automated upload code\n",
    "# \"\"\"\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Path to your PDF file\n",
    "# PDF_LOCAL_PATH = \"/Users/jholt/Downloads/fpls-15-1373081.pdf\"\n",
    "\n",
    "# # Verify file exists\n",
    "# pdf_path = Path(PDF_LOCAL_PATH)\n",
    "# if not pdf_path.exists():\n",
    "#     print(f\"‚ùå File not found: {PDF_LOCAL_PATH}\")\n",
    "#     print(\"   Update PDF_LOCAL_PATH to point to your PDF file\")\n",
    "# else:\n",
    "#     print(f\"üìÑ Found PDF: {pdf_path.name} ({pdf_path.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "    \n",
    "#     # Upload to stage\n",
    "#     print(f\"\\nüì§ Uploading to stage...\")\n",
    "#     session.file.put(\n",
    "#         str(pdf_path),\n",
    "#         f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\",\n",
    "#         auto_compress=False,\n",
    "#         overwrite=True\n",
    "#     )\n",
    "    \n",
    "#     print(f\"‚úÖ PDF uploaded to @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{pdf_path.name}\")\n",
    "    \n",
    "#     # List files in stage to verify\n",
    "#     print(f\"\\nüìÇ Files in stage:\")\n",
    "#     files = session.sql(f\"LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\").collect()\n",
    "#     for file in files:\n",
    "#         print(f\"   - {file[0]}\")\n",
    "# \"\"\""
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "id": "4664ff11-940a-40fd-96b6-bc0f8ae9a1e8",
   "metadata": {
    "name": "cell24"
   },
   "source": [
    "## üîÑ Section 4a - Batch Processing Mode (NEW!)\n",
    "\n",
    "**Choose your processing mode:**\n",
    "- **SINGLE FILE MODE** (below): Process one specific PDF\n",
    "- **BATCH MODE** (this section): Process multiple PDFs at once\n",
    "\n",
    "**Batch Processing Benefits:**\n",
    "- ‚úÖ Process multiple PDFs in one run\n",
    "- ‚úÖ Automatic file discovery from stage\n",
    "- ‚úÖ Progress tracking with tqdm\n",
    "- ‚úÖ Error handling per file (one failure doesn't stop the batch)\n",
    "- ‚úÖ Comprehensive batch statistics\n",
    "- ‚úÖ Skip already-processed files\n",
    "\n",
    "**To use batch mode:**\n",
    "1. Run cells in this section (4a)\n",
    "2. Skip single-file cells (4b)\n",
    "\n",
    "**To use single-file mode:**\n",
    "1. Skip this section (4a)\n",
    "2. Run single-file cells (4b) as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell23"
   },
   "source": [
    "## üìÑ CELL 5-6: Section 3 - List PDFs in Snowflake Stage\n",
    "\n",
    "- **Cell 5**: List available PDFs\n",
    "- **Cell 6**: Configure which PDF to process"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e5dd7-8e61-4263-932d-0947668ec00c",
   "metadata": {
    "name": "List_PDF_In_Stage",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH MODE: Discover all PDFs in stage\n",
    "# ============================================================================\n",
    "print(\"üîç Discovering PDF files in stage...\\n\")\n",
    "\n",
    "# List all files in stage\n",
    "list_query = f\"\"\"\n",
    "LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    stage_files = session.sql(list_query).collect()\n",
    "    \n",
    "    # Filter for PDF files (at root level, not in subdirectories)\n",
    "    pdf_files = []\n",
    "    for file in stage_files:\n",
    "        file_name = file['name']\n",
    "        # Extract filename from full path: PDF_STAGE/filename.pdf\n",
    "        if file_name.endswith('.pdf') and '/' not in file_name.split('/')[-1]:\n",
    "            # Get just the filename\n",
    "            filename = file_name.split('/')[-1]\n",
    "            if filename:  # Not empty\n",
    "                pdf_files.append({\n",
    "                    'filename': filename,\n",
    "                    'size': file['size'],\n",
    "                    'last_modified': file['last_modified']\n",
    "                })\n",
    "    \n",
    "    if pdf_files:\n",
    "        print(f\"‚úÖ Found {len(pdf_files)} PDF file(s) in stage:\\n\")\n",
    "        for i, pdf in enumerate(pdf_files, 1):\n",
    "            size_mb = pdf['size'] / (1024 * 1024)\n",
    "            print(f\"   {i}. {pdf['filename']}\")\n",
    "            print(f\"      Size: {size_mb:.2f} MB\")\n",
    "            print(f\"      Modified: {pdf['last_modified']}\")\n",
    "            print()\n",
    "        \n",
    "        # Check which are already processed\n",
    "        if pdf_files:\n",
    "            filenames_str = \"', '\".join([pdf['filename'] for pdf in pdf_files])\n",
    "            check_query = f\"\"\"\n",
    "            SELECT document_id, total_pages, created_at\n",
    "            FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "            WHERE document_id IN ('{filenames_str}')\n",
    "            \"\"\"\n",
    "            processed = session.sql(check_query).collect()\n",
    "            \n",
    "            processed_ids = {row[0] for row in processed}\n",
    "            \n",
    "            print(f\"üìä Processing Status:\")\n",
    "            print(f\"   Total PDFs in stage: {len(pdf_files)}\")\n",
    "            print(f\"   Already processed: {len(processed_ids)}\")\n",
    "            print(f\"   Ready to process: {len(pdf_files) - len(processed_ids)}\")\n",
    "            \n",
    "            if processed_ids:\n",
    "                print(f\"\\n   Already processed:\")\n",
    "                for doc_id in processed_ids:\n",
    "                    print(f\"      ‚úì {doc_id}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No PDF files found in stage root\")\n",
    "        print(\"\\nüí° Upload PDFs using:\")\n",
    "        print(f\"   PUT file:///path/to/file.pdf @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\")\n",
    "        pdf_files = []\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error listing stage: {e}\")\n",
    "    pdf_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89640147-71c3-47d7-863e-4d0b3a76d31f",
   "metadata": {
    "name": "Configure_Batch_Run",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH MODE: Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Choose processing mode\n",
    "PROCESS_ALL = True  # True: process all files, False: process only selected files\n",
    "SKIP_EXISTING = True  # True: skip already processed files, False: reprocess all\n",
    "MAX_FILES = None  # None: no limit, or set to a number (e.g., 5)\n",
    "\n",
    "# If PROCESS_ALL is False, specify which files to process\n",
    "SELECTED_FILES = [\n",
    "    # \"fpls-15-1373081.pdf\",\n",
    "    # \"another-paper.pdf\",\n",
    "]\n",
    "\n",
    "# Filter PDFs based on configuration\n",
    "if pdf_files:\n",
    "    files_to_process = []\n",
    "    \n",
    "    if PROCESS_ALL:\n",
    "        files_to_process = [pdf['filename'] for pdf in pdf_files]\n",
    "        print(f\"üìã Mode: Process ALL files in stage\")\n",
    "    else:\n",
    "        files_to_process = SELECTED_FILES\n",
    "        print(f\"üìã Mode: Process SELECTED files only\")\n",
    "    \n",
    "    # Filter out already processed files if SKIP_EXISTING is True\n",
    "    if SKIP_EXISTING and files_to_process:\n",
    "        filenames_str = \"', '\".join(files_to_process)\n",
    "        check_query = f\"\"\"\n",
    "        SELECT document_id\n",
    "        FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "        WHERE document_id IN ('{filenames_str}')\n",
    "        \"\"\"\n",
    "        processed = session.sql(check_query).collect()\n",
    "        processed_ids = {row[0] for row in processed}\n",
    "        \n",
    "        original_count = len(files_to_process)\n",
    "        files_to_process = [f for f in files_to_process if f not in processed_ids]\n",
    "        skipped_count = original_count - len(files_to_process)\n",
    "        \n",
    "        if skipped_count > 0:\n",
    "            print(f\"‚è≠Ô∏è  Skipping {skipped_count} already-processed file(s)\")\n",
    "    \n",
    "    # Apply MAX_FILES limit\n",
    "    if MAX_FILES and len(files_to_process) > MAX_FILES:\n",
    "        files_to_process = files_to_process[:MAX_FILES]\n",
    "        print(f\"‚ö†Ô∏è  Limited to first {MAX_FILES} files\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ready to process {len(files_to_process)} file(s):\")\n",
    "    for i, filename in enumerate(files_to_process, 1):\n",
    "        print(f\"   {i}. {filename}\")\n",
    "    \n",
    "    if len(files_to_process) == 0:\n",
    "        print(\"\\nüí° No files to process!\")\n",
    "else:\n",
    "    files_to_process = []\n",
    "    print(\"‚ùå No PDF files available for processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf123d91-d5d0-4f97-84f3-2258c3dbb3bf",
   "metadata": {
    "name": "Process_PDF",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH MODE: Process Multiple PDFs with AI_PARSE_DOCUMENT\n",
    "# ============================================================================\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "if not files_to_process:\n",
    "    print(\"‚ö†Ô∏è  No files to process. Configure files in the cell above.\")\n",
    "else:\n",
    "    print(f\"üöÄ Starting batch processing of {len(files_to_process)} PDF(s)\")\n",
    "    print(f\"   Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Track batch statistics\n",
    "    batch_stats = {\n",
    "        'total': len(files_to_process),\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'skipped': 0,\n",
    "        'total_time': 0,\n",
    "        'results': []\n",
    "    }\n",
    "    \n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    # Process each PDF\n",
    "    for idx, filename in enumerate(files_to_process, 1):\n",
    "        print(f\"\\nüìÑ [{idx}/{len(files_to_process)}] Processing: {filename}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        file_start_time = time.time()\n",
    "        document_id = filename\n",
    "        stage_file_path = filename\n",
    "        \n",
    "        try:\n",
    "            # Check if already processed (double-check)\n",
    "            check_query = f\"\"\"\n",
    "            SELECT document_id, total_pages, created_at\n",
    "            FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "            WHERE document_id = '{document_id}'\n",
    "            \"\"\"\n",
    "            \n",
    "            existing = session.sql(check_query).collect()\n",
    "            \n",
    "            if existing:\n",
    "                print(f\"   ‚è≠Ô∏è  Already processed (skipping)\")\n",
    "                print(f\"      Parsed at: {existing[0][2]}\")\n",
    "                print(f\"      Total pages: {existing[0][1]}\")\n",
    "                batch_stats['skipped'] += 1\n",
    "                batch_stats['results'].append({\n",
    "                    'filename': filename,\n",
    "                    'status': 'skipped',\n",
    "                    'pages': existing[0][1],\n",
    "                    'time': 0\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Parse PDF with AI_PARSE_DOCUMENT\n",
    "            print(f\"   ü§ñ Calling AI_PARSE_DOCUMENT...\")\n",
    "            \n",
    "            parse_query = f\"\"\"\n",
    "            INSERT INTO {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS \n",
    "                (document_id, file_path, file_name, parsed_content, total_pages)\n",
    "            SELECT\n",
    "                '{document_id}' AS document_id,\n",
    "                '@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{stage_file_path}' AS file_path,\n",
    "                '{filename}' AS file_name,\n",
    "                parsed_data AS parsed_content,\n",
    "                ARRAY_SIZE(parsed_data:pages) AS total_pages\n",
    "            FROM (\n",
    "                SELECT SNOWFLAKE.CORTEX.AI_PARSE_DOCUMENT(\n",
    "                    TO_FILE('@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE', '{stage_file_path}'),\n",
    "                    {{'mode': 'LAYOUT', 'page_split': true}}\n",
    "                ) AS parsed_data\n",
    "            )\n",
    "            \"\"\"\n",
    "            \n",
    "            session.sql(parse_query).collect()\n",
    "            \n",
    "            # Verify and get page count\n",
    "            result = session.sql(check_query).collect()\n",
    "            if result:\n",
    "                pages = result[0][1]\n",
    "                elapsed = time.time() - file_start_time\n",
    "                \n",
    "                print(f\"   ‚úÖ Success!\")\n",
    "                print(f\"      Pages: {pages}\")\n",
    "                print(f\"      Time: {elapsed:.1f}s\")\n",
    "                \n",
    "                batch_stats['successful'] += 1\n",
    "                batch_stats['total_time'] += elapsed\n",
    "                batch_stats['results'].append({\n",
    "                    'filename': filename,\n",
    "                    'status': 'success',\n",
    "                    'pages': pages,\n",
    "                    'time': elapsed\n",
    "                })\n",
    "            else:\n",
    "                raise Exception(\"Parsing succeeded but no record found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - file_start_time\n",
    "            error_msg = str(e)[:200]\n",
    "            print(f\"   ‚ùå Failed!\")\n",
    "            print(f\"      Error: {error_msg}\")\n",
    "            print(f\"      Time: {elapsed:.1f}s\")\n",
    "            \n",
    "            batch_stats['failed'] += 1\n",
    "            batch_stats['results'].append({\n",
    "                'filename': filename,\n",
    "                'status': 'failed',\n",
    "                'error': error_msg,\n",
    "                'time': elapsed\n",
    "            })\n",
    "    \n",
    "    # Print batch summary\n",
    "    total_elapsed = time.time() - batch_start_time\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä BATCH PROCESSING SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n‚è±Ô∏è  Total Time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} minutes)\")\n",
    "    print(f\"\\nüìà Results:\")\n",
    "    print(f\"   Total files: {batch_stats['total']}\")\n",
    "    print(f\"   ‚úÖ Successful: {batch_stats['successful']}\")\n",
    "    print(f\"   ‚è≠Ô∏è  Skipped: {batch_stats['skipped']}\")\n",
    "    print(f\"   ‚ùå Failed: {batch_stats['failed']}\")\n",
    "    \n",
    "    if batch_stats['successful'] > 0:\n",
    "        avg_time = batch_stats['total_time'] / batch_stats['successful']\n",
    "        total_pages = sum(r['pages'] for r in batch_stats['results'] if r['status'] == 'success')\n",
    "        print(f\"\\nüìÑ Processing Stats:\")\n",
    "        print(f\"   Total pages processed: {total_pages}\")\n",
    "        print(f\"   Average time per file: {avg_time:.1f}s\")\n",
    "        print(f\"   Average pages per file: {total_pages / batch_stats['successful']:.1f}\")\n",
    "    \n",
    "    # Show detailed results\n",
    "    if batch_stats['failed'] > 0:\n",
    "        print(f\"\\n‚ùå Failed Files:\")\n",
    "        for result in batch_stats['results']:\n",
    "            if result['status'] == 'failed':\n",
    "                print(f\"   ‚Ä¢ {result['filename']}\")\n",
    "                print(f\"     Error: {result.get('error', 'Unknown error')[:100]}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Batch processing complete!\")\n",
    "    print(f\"   Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758f1b2-8853-4f54-a8e0-04a5bdc991ed",
   "metadata": {
    "name": "View_Processed_DOCS",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH MODE: View All Processed Documents\n",
    "# ============================================================================\n",
    "print(\"üìä All Processed Documents in Database\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    document_id,\n",
    "    file_name,\n",
    "    total_pages,\n",
    "    created_at,\n",
    "    ROUND(LENGTH(parsed_content) / 1024.0 / 1024.0, 2) as content_size_mb\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "ORDER BY created_at DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    results = session.sql(query).collect()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"Total documents processed: {len(results)}\\n\")\n",
    "        \n",
    "        for i, row in enumerate(results, 1):\n",
    "            print(f\"{i}. {row['FILE_NAME']}\")\n",
    "            print(f\"   Document ID: {row['DOCUMENT_ID']}\")\n",
    "            print(f\"   Pages: {row['TOTAL_PAGES']}\")\n",
    "            print(f\"   Content Size: {row['CONTENT_SIZE_MB']} MB\")\n",
    "            print(f\"   Processed: {row['CREATED_AT']}\")\n",
    "            print()\n",
    "        \n",
    "        # Summary statistics\n",
    "        total_pages = sum(row['TOTAL_PAGES'] for row in results)\n",
    "        total_size_mb = sum(row['CONTENT_SIZE_MB'] for row in results)\n",
    "        avg_pages = total_pages / len(results) if results else 0\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"üìà Summary Statistics\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total documents: {len(results)}\")\n",
    "        print(f\"Total pages: {total_pages:,}\")\n",
    "        print(f\"Total content: {total_size_mb:.2f} MB\")\n",
    "        print(f\"Average pages per document: {avg_pages:.1f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No documents have been processed yet.\")\n",
    "        print(\"\\nüí° Run the batch processing cell above to process PDFs!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error querying database: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell29"
   },
   "source": [
    "## üìù CELL 11: Section 5 - Extract Text Pages & Generate Embeddings\n",
    "\n",
    "Uses `snowflake-arctic-embed-l-v2.0-8k` model for text embeddings"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Extract_Text",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH: Extract text pages with embeddings for ALL documents\n",
    "# ============================================================================\n",
    "# Uses snowflake-arctic-embed-l-v2.0-8k (1024D, 8K tokens)\n",
    "print(\"üîÑ Extracting text pages and generating embeddings for ALL documents...\\n\")\n",
    "print(\"üìã Text Embedding Model: snowflake-arctic-embed-l-v2.0-8k\")\n",
    "print(\"   - Dimensions: 1024\")\n",
    "print(\"   - Context length: 8K tokens\")\n",
    "print(\"   - Optimized for: Long-form documents\\n\")\n",
    "\n",
    "# Insert text pages with embeddings for ALL documents in PARSED_DOCUMENTS\n",
    "text_extract_query = f\"\"\"\n",
    "INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES \n",
    "    (document_id, file_name, page_number, page_text, word_count, \n",
    "     text_embedding, embedding_model)\n",
    "SELECT\n",
    "    pd.document_id,\n",
    "    pd.file_name,\n",
    "    page.index AS page_number,\n",
    "    page.value:content::STRING AS page_text,\n",
    "    ARRAY_SIZE(SPLIT(page.value:content::STRING, ' ')) AS word_count,\n",
    "    SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0-8k',\n",
    "        page.value:content::STRING\n",
    "    ) AS text_embedding,\n",
    "    'snowflake-arctic-embed-l-v2.0-8k' AS embedding_model\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS pd,\n",
    "LATERAL FLATTEN(input => pd.parsed_content:pages) page\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES tp\n",
    "    WHERE tp.document_id = pd.document_id \n",
    "    AND tp.page_number = page.index\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(text_extract_query).collect()\n",
    "    print(\"‚úÖ Text pages extracted with embeddings!\\n\")\n",
    "    \n",
    "    # Get statistics for ALL documents\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT document_id) as doc_count,\n",
    "        COUNT(*) as total_pages,\n",
    "        AVG(word_count) as avg_words,\n",
    "        MIN(word_count) as min_words,\n",
    "        MAX(word_count) as max_words\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = session.sql(stats_query).collect()\n",
    "    if stats and stats[0][1] > 0:\n",
    "        print(f\"üìä Text Extraction Statistics (ALL DOCUMENTS):\")\n",
    "        print(f\"   Total documents: {stats[0][0]}\")\n",
    "        print(f\"   Total pages: {stats[0][1]}\")\n",
    "        print(f\"   Avg words/page: {stats[0][2]:.0f}\")\n",
    "        print(f\"   Min words: {stats[0][3]}\")\n",
    "        print(f\"   Max words: {stats[0][4]}\")\n",
    "        \n",
    "        # Verify embeddings\n",
    "        embed_check = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES \n",
    "            WHERE text_embedding IS NOT NULL\n",
    "        \"\"\").collect()\n",
    "        print(f\"   Pages with embeddings: {embed_check[0][0]}\")\n",
    "        \n",
    "        # Show per-document summary\n",
    "        doc_summary_query = f\"\"\"\n",
    "        SELECT \n",
    "            document_id,\n",
    "            COUNT(*) as pages,\n",
    "            AVG(word_count) as avg_words\n",
    "        FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "        GROUP BY document_id\n",
    "        ORDER BY document_id\n",
    "        \"\"\"\n",
    "        \n",
    "        doc_summary = session.sql(doc_summary_query).collect()\n",
    "        if doc_summary:\n",
    "            print(f\"\\nüìÑ Per-Document Summary:\")\n",
    "            df_summary = pd.DataFrame(doc_summary, columns=['Document ID', 'Pages', 'Avg Words'])\n",
    "            display(df_summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell31"
   },
   "source": [
    "## üñºÔ∏è CELL 13-14: Section 6 - Create Image Pages\n",
    "\n",
    "- **Cell 13**: Debug - List files in stage\n",
    "- **Cell 14**: Generate image embeddings using `voyage-multimodal-3`\n",
    "\n",
    "**Purpose:** Create embeddings for PNG images to enable multimodal search (text + images).\n",
    "Images capture tables, charts, and figures that may contain GWAS data not easily extracted from text."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_images",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH: Create PNG Images from All PDFs and Insert IMAGE_PAGES Records\n",
    "# ============================================================================\n",
    "# This cell:\n",
    "# 1. Gets all parsed PDFs from database\n",
    "# 2. Downloads each PDF from stage\n",
    "# 3. Converts pages to PNG using PyMuPDF\n",
    "# 4. Uploads PNGs to stage in proper structure\n",
    "# 5. Inserts IMAGE_PAGES records\n",
    "# NOTE: Requires PyMuPDF (pip install PyMuPDF)\n",
    "\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: PyMuPDF not installed!\")\n",
    "    print(\"   Install: pip install PyMuPDF\")\n",
    "    print(\"   Then restart kernel and re-run this cell\")\n",
    "    raise\n",
    "\n",
    "print(\"üñºÔ∏è  BATCH PNG IMAGE CREATION FROM PDFs\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Configuration\n",
    "SKIP_EXISTING_IMAGES = True  # Skip documents that already have images\n",
    "DPI = 150  # Image quality (150 is good balance of quality/size)\n",
    "\n",
    "# Get all parsed documents\n",
    "docs_query = f\"\"\"\n",
    "SELECT \n",
    "    pd.document_id,\n",
    "    pd.file_name,\n",
    "    pd.file_path,\n",
    "    pd.total_pages,\n",
    "    pd.created_at,\n",
    "    COUNT(ip.page_number) as existing_images\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS pd\n",
    "LEFT JOIN {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES ip\n",
    "    ON pd.document_id = ip.document_id\n",
    "GROUP BY pd.document_id, pd.file_name, pd.file_path, pd.total_pages, pd.created_at\n",
    "ORDER BY pd.created_at DESC\n",
    "\"\"\"\n",
    "\n",
    "documents = session.sql(docs_query).collect()\n",
    "\n",
    "if not documents:\n",
    "    print(\"‚ùå No parsed documents found\")\n",
    "else:\n",
    "    # Filter documents to process\n",
    "    docs_to_process = []\n",
    "    docs_skipped = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc['DOCUMENT_ID']\n",
    "        total_pages = doc['TOTAL_PAGES']\n",
    "        existing_images = doc['EXISTING_IMAGES']\n",
    "        \n",
    "        if existing_images >= total_pages and SKIP_EXISTING_IMAGES:\n",
    "            docs_skipped.append(doc)\n",
    "        else:\n",
    "            docs_to_process.append(doc)\n",
    "    \n",
    "    print(f\"üìä Document Summary:\")\n",
    "    print(f\"   Total documents: {len(documents)}\")\n",
    "    print(f\"   Already have images: {len(docs_skipped)}\")\n",
    "    print(f\"   Need images: {len(docs_to_process)}\")\n",
    "    \n",
    "    if docs_skipped:\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipping {len(docs_skipped)} documents with existing images:\")\n",
    "        for doc in docs_skipped[:5]:\n",
    "            print(f\"   - {doc['DOCUMENT_ID'][:30]}... ({doc['EXISTING_IMAGES']} images)\")\n",
    "        if len(docs_skipped) > 5:\n",
    "            print(f\"   ... and {len(docs_skipped) - 5} more\")\n",
    "    \n",
    "    if not docs_to_process:\n",
    "        print(\"\\n‚úÖ All documents already have images!\")\n",
    "        print(\"üí° To regenerate, set SKIP_EXISTING_IMAGES = False\")\n",
    "    else:\n",
    "        print(f\"\\nüîÑ Processing {len(docs_to_process)} document(s)...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Track batch statistics\n",
    "        batch_stats = {\n",
    "            'total': len(docs_to_process),\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'total_pages_processed': 0,\n",
    "            'start_time': time.time(),\n",
    "            'results': []\n",
    "        }\n",
    "        \n",
    "        # Process each document\n",
    "        for idx, doc in enumerate(docs_to_process, 1):\n",
    "            doc_id = doc['DOCUMENT_ID']\n",
    "            file_name = doc['FILE_NAME']\n",
    "            file_path = doc['FILE_PATH']\n",
    "            total_pages = doc['TOTAL_PAGES']\n",
    "            \n",
    "            print(f\"\\nüìÑ [{idx}/{len(docs_to_process)}] Processing: {doc_id}\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"   Pages: {total_pages}\")\n",
    "            \n",
    "            doc_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Create temp directory for processing\n",
    "                with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                    temp_path = Path(temp_dir)\n",
    "                    local_pdf_path = temp_path / file_name\n",
    "                    \n",
    "                    # Download PDF from stage\n",
    "                    print(f\"   üì• Downloading PDF from stage...\")\n",
    "                    session.file.get(\n",
    "                        f\"{file_path}\",\n",
    "                        str(temp_path),\n",
    "                    )\n",
    "                    \n",
    "                    # Find the downloaded PDF (may have timestamp prefix)\n",
    "                    pdf_files = list(temp_path.glob(\"*.pdf\"))\n",
    "                    if pdf_files:\n",
    "                        local_pdf_path = pdf_files[0]\n",
    "                    \n",
    "                    if not local_pdf_path.exists():\n",
    "                        raise FileNotFoundError(f\"PDF not found after download: {local_pdf_path}\")\n",
    "                    \n",
    "                    print(f\"      ‚úì Downloaded: {local_pdf_path.name}\")\n",
    "                    \n",
    "                    # Open PDF with PyMuPDF\n",
    "                    print(f\"   üîÑ Converting pages to PNG...\")\n",
    "                    pdf_doc = fitz.open(local_pdf_path)\n",
    "                    \n",
    "                    # Save page count before closing (will need it later)\n",
    "                    actual_page_count = pdf_doc.page_count\n",
    "                    \n",
    "                    if actual_page_count != total_pages:\n",
    "                        print(f\"      ‚ö†Ô∏è  Page count mismatch: PDF has {actual_page_count}, expected {total_pages}\")\n",
    "                    \n",
    "                    # Create images directory\n",
    "                    images_dir = temp_path / \"images\"\n",
    "                    images_dir.mkdir(exist_ok=True)\n",
    "                    \n",
    "                    # Convert each page to PNG\n",
    "                    for page_num in range(actual_page_count):\n",
    "                        page = pdf_doc[page_num]\n",
    "                        pix = page.get_pixmap(dpi=DPI)\n",
    "                        \n",
    "                        # Save PNG with zero-padded numbering\n",
    "                        png_filename = f\"page_{page_num:04d}.png\"\n",
    "                        png_path = images_dir / png_filename\n",
    "                        pix.save(png_path)\n",
    "                    \n",
    "                    pdf_doc.close()\n",
    "                    \n",
    "                    print(f\"      ‚úì Created {actual_page_count} PNG images\")\n",
    "                    \n",
    "                    # Upload images to stage\n",
    "                    print(f\"   üì§ Uploading images to stage...\")\n",
    "                    stage_target = f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{doc_id}/pages_images\"\n",
    "                    \n",
    "                    session.file.put(\n",
    "                        str(images_dir / \"*.png\"),\n",
    "                        stage_target,\n",
    "                        auto_compress=False,\n",
    "                        overwrite=True\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"      ‚úì Uploaded to: {stage_target}\")\n",
    "                    \n",
    "                    # Insert IMAGE_PAGES records\n",
    "                    print(f\"   üíæ Inserting IMAGE_PAGES records...\")\n",
    "                    \n",
    "                    for page_num in range(actual_page_count):\n",
    "                        png_filename = f\"page_{page_num:04d}.png\"\n",
    "                        image_path = f\"{doc_id}/pages_images/{png_filename}\"\n",
    "                        \n",
    "                        insert_query = f\"\"\"\n",
    "                        INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "                        (document_id, file_name, page_number, image_file_path)\n",
    "                        SELECT \n",
    "                            '{doc_id}',\n",
    "                            '{file_name}',\n",
    "                            {page_num},\n",
    "                            '{image_path}'\n",
    "                        WHERE NOT EXISTS (\n",
    "                            SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "                            WHERE document_id = '{doc_id}' AND page_number = {page_num}\n",
    "                        )\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        session.sql(insert_query).collect()\n",
    "                    \n",
    "                    print(f\"      ‚úì Inserted {actual_page_count} records\")\n",
    "                    \n",
    "                    elapsed = time.time() - doc_start_time\n",
    "                    print(f\"   ‚úÖ Completed in {elapsed:.1f}s\")\n",
    "                    \n",
    "                    batch_stats['successful'] += 1\n",
    "                    batch_stats['total_pages_processed'] += actual_page_count\n",
    "                    batch_stats['results'].append({\n",
    "                        'document_id': doc_id,\n",
    "                        'status': 'success',\n",
    "                        'pages': actual_page_count,\n",
    "                        'time': elapsed\n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - doc_start_time\n",
    "                error_msg = str(e)[:200]\n",
    "                print(f\"   ‚ùå Failed after {elapsed:.1f}s\")\n",
    "                print(f\"      Error: {error_msg}\")\n",
    "                \n",
    "                batch_stats['failed'] += 1\n",
    "                batch_stats['results'].append({\n",
    "                    'document_id': doc_id,\n",
    "                    'status': 'failed',\n",
    "                    'error': error_msg,\n",
    "                    'time': elapsed\n",
    "                })\n",
    "        \n",
    "        # Print batch summary\n",
    "        total_elapsed = time.time() - batch_stats['start_time']\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä BATCH PNG CREATION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n‚è±Ô∏è  Total Time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} minutes)\")\n",
    "        print(f\"\\nüìà Results:\")\n",
    "        print(f\"   Documents processed: {batch_stats['total']}\")\n",
    "        print(f\"   ‚úÖ Successful: {batch_stats['successful']}\")\n",
    "        print(f\"   ‚ùå Failed: {batch_stats['failed']}\")\n",
    "        print(f\"   üìÑ Total pages: {batch_stats['total_pages_processed']}\")\n",
    "        \n",
    "        if batch_stats['successful'] > 0:\n",
    "            avg_time = total_elapsed / batch_stats['successful']\n",
    "            print(f\"   ‚è±Ô∏è  Average time/doc: {avg_time:.1f}s\")\n",
    "        \n",
    "        if batch_stats['failed'] > 0:\n",
    "            print(f\"\\n‚ùå Failed Documents:\")\n",
    "            for result in batch_stats['results']:\n",
    "                if result['status'] == 'failed':\n",
    "                    print(f\"   ‚Ä¢ {result['document_id'][:30]}...\")\n",
    "                    print(f\"     Error: {result.get('error', 'Unknown')}\")\n",
    "        \n",
    "        # Verify final counts\n",
    "        verify_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT document_id) as total_docs,\n",
    "            COUNT(*) as total_images\n",
    "        FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "        \"\"\"\n",
    "        \n",
    "        result = session.sql(verify_query).collect()\n",
    "        if result:\n",
    "            print(f\"\\nüìä Final Status:\")\n",
    "            print(f\"   Documents with images: {result[0][0]}\")\n",
    "            print(f\"   Total IMAGE_PAGES records: {result[0][1]}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Batch PNG creation complete!\")\n",
    "        print(f\"   Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"\\nüí° Next step: Run Section 6 to generate image embeddings\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Embed_Images",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH: Generate Image Embeddings for ALL IMAGE_PAGES Records\n",
    "# ============================================================================\n",
    "# Uses voyage-multimodal-3 to create embeddings from PNGs in stage\n",
    "print(\"üîÑ Generating image embeddings for ALL documents...\\n\")\n",
    "print(\"üìã Image Embedding Model: voyage-multimodal-3 via AI_EMBED\")\n",
    "print(\"   - Dimensions: 1024\")\n",
    "print(\"   - Supports: Images + Text\")\n",
    "print(\"   - Use case: Visual understanding of tables, charts, figures\\n\")\n",
    "\n",
    "try:\n",
    "    # Get existing IMAGE_PAGES records without embeddings (ALL DOCUMENTS)\n",
    "    check_query = f\"\"\"\n",
    "    SELECT \n",
    "        document_id,\n",
    "        page_number,\n",
    "        image_file_path,\n",
    "        COUNT(*) OVER() as total_records\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    WHERE image_embedding IS NULL\n",
    "    ORDER BY document_id, page_number\n",
    "    \"\"\"\n",
    "    \n",
    "    records = session.sql(check_query).collect()\n",
    "    \n",
    "    if not records:\n",
    "        print(\"‚ÑπÔ∏è  No records found without embeddings\")\n",
    "        \n",
    "        # Check if embeddings already exist\n",
    "        existing = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "            WHERE image_embedding IS NOT NULL\n",
    "        \"\"\").collect()\n",
    "        if existing and existing[0][0] > 0:\n",
    "            print(f\"   ‚úÖ {existing[0][0]} records already have embeddings!\\n\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  No IMAGE_PAGES records found - run Cell 34 to create images first\\n\")\n",
    "    else:\n",
    "        total_records = records[0][3]\n",
    "        print(f\"üìä Found {total_records} IMAGE_PAGES records without embeddings\")\n",
    "        print(f\"   Processing {len(records)} pages across multiple documents...\\n\")\n",
    "        \n",
    "        # Update each record with embedding\n",
    "        failed_count = 0\n",
    "        success_count = 0\n",
    "        \n",
    "        for idx, record in enumerate(records, 1):\n",
    "            doc_id = record[0]\n",
    "            page_num = record[1]\n",
    "            image_path = record[2]\n",
    "            \n",
    "            # Parse the stored path - already in format: document.pdf/pages_images/page_0000.png\n",
    "            relative_path = image_path\n",
    "            \n",
    "            # Always use full stage name for TO_FILE\n",
    "            full_stage_name = f'@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE'\n",
    "            \n",
    "            if idx <= 3 or idx % 10 == 0:  # Show first 3 and every 10th\n",
    "                print(f\"   [{idx}/{len(records)}] {doc_id[:30]}... page {page_num}\")\n",
    "            \n",
    "            # Generate embedding and update record\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "            SET \n",
    "                image_embedding = AI_EMBED(\n",
    "                    'voyage-multimodal-3',\n",
    "                    TO_FILE('{full_stage_name}', '{relative_path}')\n",
    "                ),\n",
    "                embedding_model = 'voyage-multimodal-3'\n",
    "            WHERE document_id = '{doc_id}'\n",
    "            AND page_number = {page_num}\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                session.sql(update_query).collect()\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                error_msg = str(e)\n",
    "                if failed_count <= 3:  # Show first 3 failures in detail\n",
    "                    print(f\"   ‚úó Failed on {doc_id[:30]}... page {page_num}: {error_msg[:200]}\\n\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Embedding generation complete!\")\n",
    "        print(f\"   ‚úÖ Success: {success_count}\")\n",
    "        print(f\"   ‚úó Failed: {failed_count}\\n\")\n",
    "    \n",
    "    # Verify final counts\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT document_id) as total_docs,\n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(image_embedding) as with_embeddings,\n",
    "        COUNT(CASE WHEN image_embedding IS NULL THEN 1 END) as without_embeddings\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    \"\"\"\n",
    "    \n",
    "    result = session.sql(verify_query).collect()\n",
    "    if result:\n",
    "        total_docs, total, with_emb, without_emb = result[0]\n",
    "        print(f\"üìä Final Status (ALL DOCUMENTS):\")\n",
    "        print(f\"   Documents: {total_docs}\")\n",
    "        print(f\"   Total records: {total}\")\n",
    "        print(f\"   ‚úÖ With embeddings: {with_emb}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Without embeddings: {without_emb}\")\n",
    "        print(f\"   üìà Ready for multimodal search: {with_emb}/{total}\")\n",
    "        \n",
    "        if with_emb == total and total > 0:\n",
    "            print(f\"\\nüéâ All image embeddings generated successfully!\")\n",
    "        elif without_emb > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  {without_emb} pages still need embeddings\")\n",
    "            print(f\"   Re-run this cell to retry failed embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell34"
   },
   "source": [
    "## üîó CELL 16: Section 7 - Create Multimodal Pages\n",
    "\n",
    "Join text and image embeddings into a unified multimodal table for search"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Join_Text_and_Images",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC: Check IMAGE_PAGES status before creating multimodal pages\n",
    "# ============================================================================\n",
    "print(\"üîç Diagnosing IMAGE_PAGES table status...\\n\")\n",
    "\n",
    "# Check if IMAGE_PAGES has records\n",
    "check_query = f\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_records,\n",
    "    COUNT(DISTINCT document_id) as total_docs,\n",
    "    COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as with_embeddings,\n",
    "    COUNT(CASE WHEN image_embedding IS NULL THEN 1 END) as without_embeddings\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "\"\"\"\n",
    "\n",
    "result = session.sql(check_query).collect()\n",
    "if result:\n",
    "    total, docs, with_emb, without_emb = result[0]\n",
    "    \n",
    "    print(f\"üìä IMAGE_PAGES Table Status:\")\n",
    "    print(f\"   Total records: {total}\")\n",
    "    print(f\"   Documents: {docs}\")\n",
    "    print(f\"   With embeddings: {with_emb}\")\n",
    "    print(f\"   Without embeddings: {without_emb}\")\n",
    "    \n",
    "    if total == 0:\n",
    "        print(f\"\\n‚ùå ERROR: IMAGE_PAGES table is EMPTY!\")\n",
    "        print(f\"   üí° You need to run Section 6 first:\")\n",
    "        print(f\"      1. Cell 34: Create PNG images from PDFs\")\n",
    "        print(f\"      2. Cell 35: Generate image embeddings\")\n",
    "        print(f\"\\n‚èπÔ∏è  Stopping here - fix IMAGE_PAGES first\")\n",
    "    elif without_emb > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {without_emb} image records don't have embeddings yet!\")\n",
    "        print(f\"   üí° Run Cell 35 (Generate Image Embeddings) before continuing\")\n",
    "        print(f\"\\n‚è∏Ô∏è  You can continue, but images won't be searchable without embeddings\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ IMAGE_PAGES looks good - ready to create multimodal pages!\")\n",
    "\n",
    "# Show sample records\n",
    "if result and result[0][0] > 0:\n",
    "    sample_query = f\"\"\"\n",
    "    SELECT \n",
    "        document_id,\n",
    "        page_number,\n",
    "        image_file_path,\n",
    "        CASE WHEN image_embedding IS NOT NULL THEN 'Yes' ELSE 'No' END as has_embedding\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüìÑ Sample IMAGE_PAGES records:\")\n",
    "    samples = session.sql(sample_query).collect()\n",
    "    for s in samples:\n",
    "        emb_status = \"‚úÖ\" if s[3] == \"Yes\" else \"‚ùå\"\n",
    "        print(f\"   {emb_status} {s[0][:30]}... page {s[1]}: {s[2]}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Finalize_Joins",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH: Create multimodal pages for ALL documents\n",
    "# ============================================================================\n",
    "# Join text and image embeddings by page_number\n",
    "print(\"üîÑ Creating multimodal pages for ALL documents...\\n\")\n",
    "print(\"üîó Joining text and image data by page_number\")\n",
    "print(\"   - Copies both text and image embeddings\")\n",
    "print(\"   - Enables unified multi-modal search\\n\")\n",
    "\n",
    "# First, check if MULTIMODAL_PAGES has old data\n",
    "check_old_query = f\"\"\"\n",
    "SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "\"\"\"\n",
    "old_count = session.sql(check_old_query).collect()[0][0]\n",
    "\n",
    "if old_count > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {old_count} existing records in MULTIMODAL_PAGES\")\n",
    "    print(f\"   Clearing table to regenerate with latest image embeddings...\")\n",
    "    session.sql(f\"TRUNCATE TABLE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\").collect()\n",
    "    print(f\"   ‚úÖ Table cleared\\n\")\n",
    "\n",
    "multimodal_insert_query = f\"\"\"\n",
    "INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    (document_id, file_name, page_number, page_id, image_id,\n",
    "     page_text, image_path, text_embedding, image_embedding, \n",
    "     has_text, has_image)\n",
    "SELECT\n",
    "    COALESCE(tp.document_id, ip.document_id) AS document_id,\n",
    "    COALESCE(tp.file_name, ip.file_name) AS file_name,\n",
    "    COALESCE(tp.page_number, ip.page_number) AS page_number,\n",
    "    tp.page_id,\n",
    "    ip.image_id,\n",
    "    tp.page_text,\n",
    "    ip.image_file_path AS image_path,\n",
    "    tp.text_embedding,\n",
    "    ip.image_embedding,\n",
    "    tp.page_id IS NOT NULL AS has_text,\n",
    "    ip.image_id IS NOT NULL AS has_image\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES tp\n",
    "FULL OUTER JOIN {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES ip\n",
    "    ON tp.document_id = ip.document_id\n",
    "    AND tp.page_number = ip.page_number\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(multimodal_insert_query).collect()\n",
    "    print(\"‚úÖ Multimodal pages created!\\n\")\n",
    "    \n",
    "    # Get statistics for ALL documents\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT document_id) as total_docs,\n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN has_text THEN 1 END) as pages_with_text,\n",
    "        COUNT(CASE WHEN has_image THEN 1 END) as pages_with_images,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL THEN 1 END) as text_embeddings,\n",
    "        COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as image_embeddings\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = session.sql(stats_query).collect()\n",
    "    if stats:\n",
    "        print(f\"üìä Multimodal Pages Statistics (ALL DOCUMENTS):\")\n",
    "        print(f\"   Documents: {stats[0][0]}\")\n",
    "        print(f\"   Total pages: {stats[0][1]}\")\n",
    "        print(f\"   Pages with text: {stats[0][2]}\")\n",
    "        print(f\"   Pages with images: {stats[0][3]}\")\n",
    "        print(f\"   Text embeddings: {stats[0][4]}\")\n",
    "        print(f\"   Image embeddings: {stats[0][5]}\")\n",
    "    \n",
    "    # Show per-document summary\n",
    "    doc_summary_query = f\"\"\"\n",
    "    SELECT \n",
    "        document_id,\n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN has_text THEN 1 END) as text_pages,\n",
    "        COUNT(CASE WHEN has_image THEN 1 END) as image_pages\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    GROUP BY document_id\n",
    "    ORDER BY document_id\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_summary = session.sql(doc_summary_query).collect()\n",
    "    if doc_summary:\n",
    "        print(f\"\\nüìÑ Per-Document Summary:\")\n",
    "        df = pd.DataFrame(doc_summary, \n",
    "                          columns=['Document ID', 'Total Pages', 'Text Pages', 'Image Pages'])\n",
    "        display(df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell37"
   },
   "source": [
    "## üîç Section 8: Create Multi-Index Cortex Search Service\n",
    "\n",
    "Create a Cortex Search service that indexes:\n",
    "- **Text content** (keyword search)\n",
    "- **Text embeddings** (semantic search with Arctic-8k)\n",
    "- **Image embeddings** (visual search with voyage-multimodal-3)\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Create_search_service",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create multi-index Cortex Search Service\n",
    "print(\"üîÑ Creating Cortex Search Service...\\n\")\n",
    "print(\"üìã Service Configuration:\")\n",
    "print(\"   ‚Ä¢ Name: MULTIMODAL_SEARCH_SERVICE\")\n",
    "print(\"   ‚Ä¢ Text Index: page_text (keyword search)\")\n",
    "print(\"   ‚Ä¢ Vector Index 1: text_embedding (1024D - Arctic-8k)\")\n",
    "print(\"   ‚Ä¢ Vector Index 2: image_embedding (1024D - voyage-multimodal-3)\")\n",
    "print(\"   ‚Ä¢ Target Lag: 1 minute\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if service already exists\n",
    "    check_sql = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    \n",
    "    service_exists = False\n",
    "    try:\n",
    "        result = session.sql(check_sql).collect()\n",
    "        service_exists = len(result) > 0\n",
    "    except:\n",
    "        service_exists = False\n",
    "    \n",
    "    if service_exists:\n",
    "        print(\"‚úÖ Service already exists, skipping creation (will refresh at end)\\n\")\n",
    "        # Skip to refresh section\n",
    "    else:\n",
    "        print(\"üÜï Creating new search service...\\n\")\n",
    "        \n",
    "        # Create multi-index search service (Limited Private Preview feature)\n",
    "        # Docs: https://docs.snowflake.com/LIMITEDACCESS/cortex-search/multi-index-service\n",
    "        create_sql = f\"\"\"\n",
    "CREATE CORTEX SEARCH SERVICE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE\n",
    "  TEXT INDEXES page_text\n",
    "  VECTOR INDEXES (\n",
    "    text_embedding,\n",
    "    image_embedding\n",
    "  )\n",
    "  ATTRIBUTES (\n",
    "    page_id,\n",
    "    document_id,\n",
    "    file_name,\n",
    "    page_number,\n",
    "    image_path\n",
    "  )\n",
    "  WAREHOUSE = {WAREHOUSE_NAME}\n",
    "  TARGET_LAG = '1 minute'\n",
    "AS \n",
    "  SELECT \n",
    "    page_id,\n",
    "    document_id,\n",
    "    file_name,\n",
    "    page_number,\n",
    "    page_text,\n",
    "    text_embedding,\n",
    "    image_embedding,\n",
    "    image_path\n",
    "  FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "  WHERE has_text = TRUE AND has_image = TRUE\n",
    "\"\"\"\n",
    "    \n",
    "        session.sql(create_sql).collect()\n",
    "        print(\"‚úÖ Cortex Search Service created!\\n\")\n",
    "    \n",
    "    # Regardless of create or skip, check service status\n",
    "    status_sql = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    status = session.sql(status_sql).collect()\n",
    "    if status:\n",
    "        print(\"üìä Service Status:\")\n",
    "        print(f\"   Name: {status[0][1]}\")  # name column\n",
    "        print(f\"   Database: {status[0][2]}\")  # database_name\n",
    "        print(f\"   Schema: {status[0][3]}\")  # schema_name\n",
    "        print(\"\\n‚ö†Ô∏è  Note: Service may take ~1 minute to build indexes\")\n",
    "        print(\"   Wait before running search queries if you get errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating search service: {e}\")\n",
    "    print(\"\\n   If you see 'already exists', that's OK - service is ready\")\n",
    "    print(\"   If you see 'insufficient privileges', contact your Snowflake admin\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Refresch_Search_Service",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Refresh the search service to pick up any new data\n",
    "# This is fast and updates indexes without recreating the service\n",
    "print(\"üîÑ Refreshing Search Service...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check current refresh status\n",
    "    status_query = \"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        database_name,\n",
    "        schema_name,\n",
    "        created_on,\n",
    "        refresh_on\n",
    "    FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n",
    "    WHERE name = 'MULTIMODAL_SEARCH_SERVICE'\n",
    "    \"\"\"\n",
    "    \n",
    "    # First get the service info\n",
    "    show_query = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    session.sql(show_query)\n",
    "    \n",
    "    # Force a refresh\n",
    "    print(\"‚è±Ô∏è  Initiating service refresh...\")\n",
    "    refresh_query = f\"\"\"\n",
    "    ALTER CORTEX SEARCH SERVICE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE REFRESH\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        session.sql(refresh_query).collect()\n",
    "        print(\"‚úÖ Service refresh initiated\\n\")\n",
    "    except Exception as refresh_error:\n",
    "        if \"does not support manual refresh\" in str(refresh_error):\n",
    "            print(\"‚ÑπÔ∏è  Service auto-refreshes based on TARGET_LAG setting\\n\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Refresh note: {refresh_error}\\n\")\n",
    "    \n",
    "    # Wait a moment for refresh\n",
    "    import time\n",
    "    print(\"‚è≥ Waiting 5 seconds for service to sync...\")\n",
    "    time.sleep(5)\n",
    "    print(\"‚úÖ Ready to query\\n\")\n",
    "    \n",
    "    # Verify data one more time\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT COUNT(*) as ready_pages\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{SELECTED_DOCUMENT_ID}'\n",
    "      AND text_embedding IS NOT NULL\n",
    "      AND image_embedding IS NOT NULL\n",
    "      AND has_text = TRUE\n",
    "      AND has_image = TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "    result = session.sql(verify_query).collect()\n",
    "    if result and result[0][0] > 0:\n",
    "        print(f\"‚úÖ {result[0][0]} pages are indexed and ready for search\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No pages found matching service criteria\")\n",
    "        print(\"   Service filters: has_text = TRUE AND has_image = TRUE\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  {e}\")\n",
    "    print(\"\\n‚ÑπÔ∏è  This is OK - service should still work if it was created\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Verify_Search_service",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Verify search service and data readiness\n",
    "print(\"üîç Verifying Search Service Status...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if service exists\n",
    "    check_service = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    service_info = session.sql(check_service).collect()\n",
    "    \n",
    "    if service_info:\n",
    "        print(\"‚úÖ Search service exists\")\n",
    "        print(f\"   Name: {service_info[0][1]}\")\n",
    "        print(f\"   Created: {service_info[0][4]}\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Search service NOT found!\")\n",
    "        print(\"   Run the previous cell to create it\\n\")\n",
    "    \n",
    "    # Check data in multimodal pages\n",
    "    data_check = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL THEN 1 END) as with_text_emb,\n",
    "        COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as with_image_emb,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL AND image_embedding IS NOT NULL THEN 1 END) as with_both\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{SELECTED_DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    data_stats = session.sql(data_check).collect()\n",
    "    if data_stats:\n",
    "        total, text_emb, image_emb, both = data_stats[0]\n",
    "        print(f\"üìä Data Readiness:\")\n",
    "        print(f\"   Total pages: {total}\")\n",
    "        print(f\"   With text embeddings: {text_emb}\")\n",
    "        print(f\"   With image embeddings: {image_emb}\")\n",
    "        print(f\"   With BOTH embeddings: {both}\")\n",
    "        \n",
    "        if both == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: No pages have both embeddings!\")\n",
    "            print(\"   Search service filters for: has_text = TRUE AND has_image = TRUE\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Ready to search {both} pages\")\n",
    "    \n",
    "    # Give service time to build indexes\n",
    "    print(\"\\nüí° If you just created the service, wait ~60 seconds for indexes to build\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking service: {e}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell41"
   },
   "source": [
    "## üéØ Section 9: Test Multimodal Search\n",
    "\n",
    "Query the multi-index Cortex Search service with:\n",
    "- **Text keyword search** (exact/fuzzy matching on page_text)\n",
    "- **Text embedding search** (semantic similarity with Arctic-8k)\n",
    "- **Image embedding search** (visual similarity with voyage-multimodal-3)\n",
    "\n",
    "The search uses weighted scoring to balance text and visual results.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5bde1-f130-4e36-b393-8cc348b7bd34",
   "metadata": {
    "name": "Test_Search_Service",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTION: Safely convert embeddings to proper list format\n",
    "def safe_vector_conversion(vector_data):\n",
    "    \"\"\"\n",
    "    Safely convert Snowflake embedding results to Python lists.\n",
    "    Handles various formats that Snowflake might return.\n",
    "    \"\"\"\n",
    "    if vector_data is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's already a list, return it\n",
    "    if isinstance(vector_data, list) and len(vector_data) > 0 and isinstance(vector_data[0], (int, float)):\n",
    "        return vector_data\n",
    "    \n",
    "    # If it's a string representation of a list\n",
    "    if isinstance(vector_data, str):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(vector_data)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except:\n",
    "            # If ast.literal_eval fails, try json\n",
    "            try:\n",
    "                import json\n",
    "                parsed = json.loads(vector_data)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # If it has a tolist method (numpy array or similar)\n",
    "    if hasattr(vector_data, 'tolist'):\n",
    "        return vector_data.tolist()\n",
    "    \n",
    "    # If it's an array-like object that can be converted to list\n",
    "    try:\n",
    "        result = list(vector_data)\n",
    "        # Check if we got a proper numeric list\n",
    "        if result and isinstance(result[0], (int, float)):\n",
    "            return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # If all else fails, raise an error\n",
    "    raise ValueError(f\"Could not convert vector data of type {type(vector_data)} to list\")\n",
    "\n",
    "# Test the function\n",
    "print(\"‚úÖ Vector conversion helper function defined!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"text_vector = safe_vector_conversion(embeddings[0][0])\")\n",
    "print(\"image_vector = safe_vector_conversion(embeddings[0][1])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "GWAS_Traits"
   },
   "source": [
    "## üß¨ Section 10 - Extract GWAS Traits (Optimized AI Pipeline)\n",
    "\n",
    "**Overview of extraction phases:**\n",
    "- **Cell 40**: Define 15 GWAS traits with complex extraction prompts\n",
    "- **Cell 42**: Phase 1 - AI_EXTRACT from full document text\n",
    "- **Cell 44**: Phase 2 - Multimodal search + AI_EXTRACT validation\n",
    "- **Cell 45**: Phase 3 - Final merge of Phase 1 & Phase 2 results\n",
    "- **Cell 46**: Display final results\n",
    "\n",
    "This optimized approach uses AI_EXTRACT exclusively for consistent, fast, and accurate GWAS trait extraction from scientific papers."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "markdown",
   "id": "2e0cf440-0ffb-4c13-b155-fdcaf2ab797f",
   "metadata": {
    "name": "cell44"
   },
   "source": [
    "### üìÑ Document Selection for GWAS Trait Extraction\n",
    "\n",
    "**Important:** Section 10 processes ONE document at a time for GWAS trait extraction.\n",
    "\n",
    "To process multiple documents:\n",
    "1. Run the configuration cell below to select a document\n",
    "2. Run all Phase 1-3 extraction cells\n",
    "3. Save results to GWAS_TRAIT_ANALYTICS\n",
    "4. Return to step 1 and select the next document\n",
    "\n",
    "This approach allows you to review and verify extraction results for each paper individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb131ac-bfae-441c-9673-146984cc5a5c",
   "metadata": {
    "name": "Select_Docs",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SELECT DOCUMENT FOR GWAS TRAIT EXTRACTION\n",
    "# ============================================================================\n",
    "# Choose which document to process for GWAS trait extraction\n",
    "\n",
    "# Get list of available documents\n",
    "available_docs_query = f\"\"\"\n",
    "SELECT document_id, file_name, total_pages\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "ORDER BY created_at DESC\n",
    "\"\"\"\n",
    "\n",
    "available_docs = session.sql(available_docs_query).collect()\n",
    "\n",
    "if not available_docs:\n",
    "    print(\"‚ùå No documents found in PARSED_DOCUMENTS\")\n",
    "    print(\"   Please process PDFs using Section 4a (Batch Mode) first\")\n",
    "else:\n",
    "    print(f\"üìã Available Documents for GWAS Extraction:\\n\")\n",
    "    for idx, doc in enumerate(available_docs, 1):\n",
    "        print(f\"   {idx}. {doc[0]} ({doc[2]} pages)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CONFIGURATION: Select which document to process\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Option 1: Auto-select the first document\n",
    "    SELECTED_DOCUMENT_ID = available_docs[0][0]\n",
    "    \n",
    "    # Option 2: Manually specify a document ID\n",
    "    # SELECTED_DOCUMENT_ID = \"fpls-15-1373081.pdf\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Selected for extraction: {SELECTED_DOCUMENT_ID}\")\n",
    "    print(f\"\\nüí° To process a different document:\")\n",
    "    print(f\"   1. Update SELECTED_DOCUMENT_ID in this cell\")\n",
    "    print(f\"   2. Re-run this cell and all Phase 1-3 cells below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Define_trait_Prompts",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Define 15 GWAS traits with refined, context-aware extraction prompts\n",
    "# Based on GWAS paper structure: Abstract ‚Üí Intro ‚Üí Methods ‚Üí Results ‚Üí Discussion\n",
    "# ‚ú® IMPROVED: Fixed for multi-species plant genomics coverage\n",
    "# ‚ú® NEW: Support for multiple findings extraction (10-20 SNPs per paper)\n",
    "\n",
    "traits_config_improved = {\n",
    "    # ========================================\n",
    "    # DOCUMENT-LEVEL TRAITS (Extract once per paper)\n",
    "    # ========================================\n",
    "    \n",
    "    \"Trait\": {\n",
    "        \"search_query\": \"trait phenotype disease resistance agronomic character quality stress tolerance\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the MAIN phenotypic trait studied in this GWAS paper.\n",
    "\n",
    "Look in: Title, Abstract (first paragraph), Introduction (study objective).\n",
    "\n",
    "Format: Descriptive name of the trait being studied.\n",
    "Examples: 'Disease resistance' (generic), 'Plant height', 'Flowering time', 'Grain yield', 'Drought tolerance'\n",
    "\n",
    "Return the primary trait name ONLY, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Germplasm_Name\": {\n",
    "        \"search_query\": \"germplasm variety line population inbred diversity panel genetic background subpopulation\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the germplasm/population used in this GWAS study.\n",
    "\n",
    "Look in: Methods ‚Üí Plant Materials/Germplasm, Introduction ‚Üí Study population.\n",
    "\n",
    "Common formats across crops:\n",
    "- Inbred lines: 'B73' (maize), 'Nipponbare' (rice), 'Col-0' (Arabidopsis), 'Chinese Spring' (wheat)\n",
    "- Diversity panels: '282 association panel', '3K rice genome panel', 'SoyNAM', 'UK wheat diversity panel'\n",
    "- Population codes: 'DH population', 'RIL population', 'F2:3 families', 'BC1F2'\n",
    "- Specific varieties: 'Williams 82' (soybean), 'Kitaake' (rice)\n",
    "\n",
    "Return the most specific germplasm name, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Genome_Version\": {\n",
    "        \"search_query\": \"genome version reference assembly RefGen annotation build\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the reference genome assembly version used.\n",
    "\n",
    "Look in: Methods ‚Üí Genotyping/Variant Calling, Supplementary Methods.\n",
    "\n",
    "Common formats by crop:\n",
    "- Maize: 'B73 RefGen_v4', 'AGPv4', 'Zm00001e'\n",
    "- Rice: 'IRGSP-1.0', 'MSU7', 'Nipponbare-v7.0'\n",
    "- Wheat: 'IWGSC RefSeq v2.1', 'CS42'\n",
    "- Arabidopsis: 'TAIR10', 'Col-0'\n",
    "- Soybean: 'Glycine_max_v4.0', 'Williams 82 v2.0'\n",
    "- Tomato: 'SL4.0', 'Heinz 1706'\n",
    "\n",
    "Return the version identifier, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"GWAS_Model\": {\n",
    "        \"search_query\": \"GWAS model GLM MLM statistical method population structure kinship software\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the statistical model/software used for GWAS.\n",
    "\n",
    "Look in: Methods ‚Üí Statistical analysis/GWAS analysis section.\n",
    "\n",
    "Common models: MLM (mixed linear model), GLM, CMLM, FarmCPU, BLINK, SUPER,\n",
    "               EMMAX, FastGWA, rrBLUP, BOLT-LMM\n",
    "\n",
    "Common software: TASSEL, GAPIT, GEMMA, PLINK, regenie, GCTA, rMVP, GENESIS\n",
    "\n",
    "Return model name OR software, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Evidence_Type\": {\n",
    "        \"search_query\": \"GWAS QTL linkage association mapping study type genetic analysis\",\n",
    "        \"extraction_prompt\": \"\"\"Identify the genetic mapping approach used.\n",
    "\n",
    "Look in: Title, Abstract, Methods ‚Üí Study design.\n",
    "\n",
    "Types: \n",
    "- 'GWAS' (genome-wide association study) - most common\n",
    "- 'QTL' (quantitative trait loci mapping) - biparental populations\n",
    "- 'Linkage' (family-based mapping)\n",
    "- 'Fine_Mapping' (high-resolution narrowing of QTL)\n",
    "\n",
    "Return ONE type: 'GWAS', 'QTL', 'Linkage', 'Fine_Mapping', or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    # ========================================\n",
    "    # FINDING-LEVEL TRAITS (Extract multiple per paper)\n",
    "    # ========================================\n",
    "    # ‚ú® NEW: These can now extract arrays of findings\n",
    "    \n",
    "    \"Chromosome\": {\n",
    "        \"search_query\": \"chromosome chr number genomic location linkage group significant hits\",\n",
    "        \"extraction_prompt\": \"\"\"Extract ALL chromosomes with significant associations (p < 0.001 or genome-wide significant).\n",
    "\n",
    "Look in: Results ‚Üí GWAS hits, Manhattan plot peaks, Tables of significant SNPs.\n",
    "\n",
    "Format: Return comma-separated list of chromosome identifiers, ranked by significance (lowest p-value first).\n",
    "Examples: '5, 3, 10, 1' or '3A, 5B, 2D' (wheat) or 'X, 3, 5' or 'LG1, LG3, LG5' (linkage groups)\n",
    "\n",
    "If only 1 significant hit: Return that chromosome.\n",
    "If 10+ hits: Return top 10 most significant.\n",
    "\n",
    "Return chromosome identifiers (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Physical_Position\": {\n",
    "        \"search_query\": \"physical position locus base pairs bp genomic coordinate marker location\",\n",
    "        \"extraction_prompt\": \"\"\"Extract physical positions of SIGNIFICANT SNPs (top 10 by p-value).\n",
    "\n",
    "Look in: Results ‚Üí Significant associations, Tables with 'Position' or 'bp' columns.\n",
    "\n",
    "Format: Return comma-separated positions with chromosome context.\n",
    "Examples: \n",
    "- Single: '145.6 Mb'\n",
    "- Multiple: 'Chr5:145.6Mb, Chr3:198.2Mb, Chr10:78.9Mb'\n",
    "- Alt format: '145678901 (Chr5), 198234567 (Chr3)'\n",
    "\n",
    "If positions are in a table: Extract top 10 rows.\n",
    "Include chromosome reference for clarity.\n",
    "\n",
    "Return positions (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Gene\": {\n",
    "        \"search_query\": \"candidate gene causal gene functional gene locus gene model annotation\",\n",
    "        \"extraction_prompt\": \"\"\"Extract ALL candidate genes mentioned for significant associations.\n",
    "\n",
    "Look in: Results ‚Üí Candidate genes, Tables ‚Üí Gene columns, Discussion ‚Üí Gene function.\n",
    "\n",
    "Common formats across crops:\n",
    "- Maize: 'Zm00001d027230', 'GRMZM2G123456', 'tb1', 'dwarf8'\n",
    "- Rice: 'LOC_Os03g01234', 'OsMADS1', 'SD1'\n",
    "- Arabidopsis: 'AT1G12345', 'FLC', 'CO'\n",
    "- Wheat: 'TraesCS3A02G123456', 'Rht-D1'\n",
    "- Soybean: 'Glyma.01G000100', 'E1', 'Dt1'\n",
    "\n",
    "Return comma-separated list if multiple genes.\n",
    "Examples: 'Zm00001d027230, Zm00001d042156, Zm00001d013894'\n",
    "\n",
    "Return candidate genes (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"SNP_Name\": {\n",
    "        \"search_query\": \"SNP marker name identifier genotyping array lead markers\",\n",
    "        \"extraction_prompt\": \"\"\"Extract SNP/marker names for SIGNIFICANT associations (top 10).\n",
    "\n",
    "Look in: Results ‚Üí Significant markers, Tables ‚Üí Marker ID column.\n",
    "\n",
    "Common prefixes vary by genotyping platform:\n",
    "- Array-based: 'PZE-', 'AX-', 'Affx-'\n",
    "- Sequence-based: 'S1_', 'Chr1_', 'ss', 'rs' (if dbSNP)\n",
    "- Custom: May be position-based or study-specific\n",
    "\n",
    "Return comma-separated list if multiple SNPs.\n",
    "Examples: 'PZE-101234567, AX-90812345, S1_145678901'\n",
    "\n",
    "Return marker identifiers (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Variant_ID\": {\n",
    "        \"search_query\": \"variant ID SNP ID rs number dbSNP database identifier\",\n",
    "        \"extraction_prompt\": \"\"\"Extract dbSNP variant IDs if referenced for significant associations.\n",
    "\n",
    "Look in: Methods ‚Üí Variant annotation, Supplementary tables.\n",
    "\n",
    "Format: 'rs' or 'ss' prefixes (human/model organism databases)\n",
    "Examples: 'rs123456789, rs987654321, rs111222333'\n",
    "\n",
    "NOTE: Most plant studies don't use dbSNP IDs (common in human/model organisms).\n",
    "\n",
    "Return dbSNP IDs (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Variant_Type\": {\n",
    "        \"search_query\": \"variant type SNP InDel polymorphism haplotype marker genotyping\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the predominant variant/marker type analyzed.\n",
    "\n",
    "Look in: Methods ‚Üí Variant calling/Genotyping, Results ‚Üí Association type.\n",
    "\n",
    "Common types:\n",
    "- SNP (single nucleotide polymorphism) - most common\n",
    "- InDel (insertion/deletion)\n",
    "- CNV (copy number variant)\n",
    "- SV (structural variant)\n",
    "- PAV (presence/absence variant) - plant pangenomes\n",
    "- Haplotype (multi-marker block)\n",
    "- SSR/Microsatellite (older studies)\n",
    "\n",
    "Return ONE primary type (this is usually uniform across findings), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Effect_Size\": {\n",
    "        \"search_query\": \"effect size R-squared R2 variance explained phenotypic variation proportion\",\n",
    "        \"extraction_prompt\": \"\"\"Extract effect sizes for SIGNIFICANT QTLs (top 10).\n",
    "\n",
    "Look in: Results ‚Üí QTL effect, Tables ‚Üí R¬≤ or 'Variance explained' columns.\n",
    "\n",
    "Format: Return comma-separated if multiple, with chromosome context if helpful.\n",
    "Examples:\n",
    "- Single: 'R¬≤=0.23'\n",
    "- Multiple: '0.31 (Chr10), 0.23 (Chr5), 0.19 (Chr3)'\n",
    "- Alt format: '23%, 19%, 15%'\n",
    "\n",
    "Return effect sizes (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Allele\": {\n",
    "        \"search_query\": \"allele REF ALT haplotype genotype reference alternate favorable effect\",\n",
    "        \"extraction_prompt\": \"\"\"Extract allele information for SIGNIFICANT SNPs.\n",
    "\n",
    "Look in: Results tables (REF, ALT, Allele columns), figures, supplementary data.\n",
    "\n",
    "Common formats:\n",
    "- Slash: 'A/G', 'T/C', 'G/T'\n",
    "- Arrow: 'A>G', 'T>C'\n",
    "- Explicit: 'REF: A ALT: G'\n",
    "- Effect notation: 'favorable: T'\n",
    "\n",
    "If multiple SNPs: Return comma-separated alleles.\n",
    "Examples: 'A/G, T/C, G/A'\n",
    "\n",
    "NOTE: Allele data is typically in tables/charts, not body text.\n",
    "\n",
    "Return allele notations (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Annotation\": {\n",
    "        \"search_query\": \"functional annotation missense synonymous intergenic gene ontology regulatory\",\n",
    "        \"extraction_prompt\": \"\"\"Extract functional annotations for SIGNIFICANT variants.\n",
    "\n",
    "Look in: Results ‚Üí Variant annotation, Discussion ‚Üí Functional impact.\n",
    "\n",
    "Categories: \n",
    "- 'missense_variant', 'synonymous', 'intergenic_region'\n",
    "- 'upstream_gene', '5_prime_UTR', '3_prime_UTR'\n",
    "- 'intronic', 'regulatory_region'\n",
    "\n",
    "If multiple variants: Return comma-separated annotations.\n",
    "Examples: 'missense_variant, intergenic_region, missense_variant'\n",
    "\n",
    "Return annotations (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Candidate_Region\": {\n",
    "        \"search_query\": \"QTL region confidence interval linkage disequilibrium block bin locus interval\",\n",
    "        \"extraction_prompt\": \"\"\"Extract QTL regions or confidence intervals for SIGNIFICANT associations.\n",
    "\n",
    "Look in: Results ‚Üí QTL mapping, Tables ‚Üí QTL interval/region columns.\n",
    "\n",
    "Format: Genomic intervals with units\n",
    "Examples: \n",
    "- Single: 'chr1:145.6-146.1 Mb'\n",
    "- Multiple: 'chr5:145.6-146.1Mb, chr3:198-199Mb, chr10:78-79Mb'\n",
    "- Alt: 'bin 1.04, bin 3.05, bin 10.02'\n",
    "- cM: '10-12 cM (Chr5), 45-47 cM (Chr3)'\n",
    "\n",
    "Return genomic regions (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Defined 15 GWAS Traits for Targeted Extraction\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚ú® IMPROVEMENTS APPLIED:\")\n",
    "print(\"   ‚úÖ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean, tomato)\")\n",
    "print(\"   ‚úÖ Germplasm_Name: Added rice, wheat, Arabidopsis, soybean examples\")\n",
    "print(\"   ‚úÖ Genome_Version: Added 6 crop genome formats\")\n",
    "print(\"   ‚úÖ Gene: Added 5 crop gene ID patterns\")\n",
    "print(\"   ‚úÖ Allele: Shortened from 15 lines to 8 lines (50% reduction)\")\n",
    "print(\"   ‚úÖ Chromosome: Now accepts numbers, letters (3A, X, Y, MT), linkage groups\")\n",
    "print(\"   ‚úÖ Enhanced search queries with GWAS terminology\")\n",
    "print(\"   ‚úÖ NEW: Multi-finding support (extract ALL significant associations, not just strongest)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for idx, (trait_name, trait_info) in enumerate(traits_config_improved.items(), 1):\n",
    "    print(f\"{idx:2d}. {trait_name:20s} ‚Üí Search: '{trait_info['search_query'][:50]}...'\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ Ready to extract {len(traits_config_improved)} traits using multi-phase approach\")\n",
    "print(\"üåæ Now supports: Maize, Rice, Wheat, Arabidopsis, Soybean, Tomato, and more!\")\n",
    "print(\"üéØ NEW: Can extract 10-20 findings per paper (not just strongest SNP)\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell47"
   },
   "source": [
    "### üìä Phase 1 - Optimized Text Extraction with AI_EXTRACT\n",
    "\n",
    "**What this does:** Extracts GWAS traits from the full document text using AI_EXTRACT:\n",
    "- **Single API call**: Batch processing all 15 traits at once\n",
    "- **Enhanced prompts**: Full complex prompts with multi-species examples\n",
    "- **Smart context**: 25K character window for comprehensive coverage\n",
    "- **Output**: Extracted traits with HIGH confidence when found\n",
    "\n",
    "Processes the complete document text to extract all genomic trait information."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Extract_Traits",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Phase 1: OPTIMIZED AI_EXTRACT - Single Method Extraction\n",
    "print(\"üìù Phase 1: Text-Based Extraction (Optimized Single Method)\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Strategy: Use AI_EXTRACT with enhanced prompts\")\n",
    "print(\"   ‚Ä¢ Batch processing all 15 traits in one call\")\n",
    "print(\"   ‚Ä¢ Full complex prompts (no truncation)\")\n",
    "print(\"   ‚Ä¢ 25K context window for better coverage\")\n",
    "print(\"   ‚Ä¢ Direct confidence based on extraction success\\n\")\n",
    "\n",
    "# Get all text pages for the selected document\n",
    "context_query = f\"\"\"\n",
    "SELECT LISTAGG(page_text, '\\\\n\\\\n---PAGE BREAK---\\\\n\\\\n') WITHIN GROUP (ORDER BY page_number) as full_text\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "WHERE document_id = '{SELECTED_DOCUMENT_ID}'\n",
    "\"\"\"\n",
    "\n",
    "# Helper function to validate if a value is actually meaningful\n",
    "def is_valid_value(val):\n",
    "    \"\"\"Check if value is meaningful (not 'NOT_FOUND' or garbage)\"\"\"\n",
    "    if not val:\n",
    "        return False\n",
    "    \n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    \n",
    "    # Check for explicit NOT_FOUND patterns\n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    \n",
    "    # Check for meta-responses\n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    \n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "try:\n",
    "    all_text = session.sql(context_query).collect()\n",
    "    \n",
    "    if not all_text or not all_text[0][0]:\n",
    "        print(\"‚ö†Ô∏è  No text pages found in TEXT_PAGES table\")\n",
    "        print(\"   Make sure Section 5 (Extract Text Pages) was run\")\n",
    "        text_extraction_results = {}\n",
    "        fields_found = 0\n",
    "        fields_not_found = list(traits_config_improved.keys())\n",
    "        confidence_levels = {}\n",
    "    else:\n",
    "        full_document_text = all_text[0][0]\n",
    "        print(f\"‚úÖ Loaded document text: {len(full_document_text):,} characters\\n\")\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        # =============================================================================\n",
    "        # AI_EXTRACT with FULL COMPLEX prompts\n",
    "        # =============================================================================\n",
    "        print(\"üìä Extracting traits with AI_EXTRACT\\n\")\n",
    "        \n",
    "        # Use FULL prompts without truncation\n",
    "        complex_prompts = {}\n",
    "        for trait_name, trait_info in traits_config_improved.items():\n",
    "            # Convert multi-line prompt to single line, preserve ALL instructions\n",
    "            detailed_prompt = trait_info['extraction_prompt']\n",
    "            condensed = ' '.join(detailed_prompt.replace('\\n', ' ').split())\n",
    "            complex_prompts[trait_name] = condensed\n",
    "        \n",
    "        # Smart context selection: 25K chars\n",
    "        if len(full_document_text) > 25000:\n",
    "            # Keep first 15K (intro/methods) + last 10K (results/tables)\n",
    "            clean_text = (full_document_text[:15000] + \" ... \" + full_document_text[-10000:])\n",
    "        else:\n",
    "            clean_text = full_document_text\n",
    "        \n",
    "        clean_text = clean_text.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        # Create JSON for responseFormat\n",
    "        response_format_json = json.dumps(complex_prompts)\n",
    "        response_format_sql = response_format_json.replace(\"'\", \"''\")\n",
    "        \n",
    "        extract_query = f\"\"\"\n",
    "        SELECT AI_EXTRACT(\n",
    "            text => '{clean_text}',\n",
    "            responseFormat => PARSE_JSON('{response_format_sql}')\n",
    "        ) as extracted_data\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"‚öôÔ∏è  Calling AI_EXTRACT with full complex prompts...\")\n",
    "        print(f\"   Context size: {len(clean_text):,} chars\")\n",
    "        print(f\"   Prompt sizes: {min(len(p) for p in complex_prompts.values())}-{max(len(p) for p in complex_prompts.values())} chars\\n\")\n",
    "        \n",
    "        result = session.sql(extract_query).collect()\n",
    "        \n",
    "        # Process results\n",
    "        text_extraction_results = {\n",
    "            \"document_id\": SELECTED_DOCUMENT_ID,\n",
    "            \"file_name\": SELECTED_DOCUMENT_ID,  # document_id is the filename\n",
    "            \"extraction_source\": \"ai_extract_optimized\"\n",
    "        }\n",
    "        confidence_levels = {}\n",
    "        fields_found = 0\n",
    "        fields_not_found = []\n",
    "        \n",
    "        if result and result[0][0]:\n",
    "            extracted_json = result[0][0]\n",
    "            if isinstance(extracted_json, str):\n",
    "                extracted_data = json.loads(extracted_json)\n",
    "            else:\n",
    "                extracted_data = extracted_json\n",
    "            \n",
    "            if 'response' in extracted_data:\n",
    "                extracted_data = extracted_data['response']\n",
    "            \n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                value = extracted_data.get(trait_name)\n",
    "                if is_valid_value(value):\n",
    "                    text_extraction_results[trait_name] = value\n",
    "                    # Direct confidence: HIGH if found, as AI_EXTRACT is our best method\n",
    "                    confidence_levels[trait_name] = \"HIGH\"\n",
    "                    fields_found += 1\n",
    "                    print(f\"   ‚úì {trait_name:20s}: {str(value)[:60]}\")\n",
    "                else:\n",
    "                    text_extraction_results[trait_name] = None\n",
    "                    confidence_levels[trait_name] = \"NONE\"\n",
    "                    fields_not_found.append(trait_name)\n",
    "                    print(f\"   ‚úó {trait_name:20s}: Not found\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  AI_EXTRACT returned no results\")\n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                text_extraction_results[trait_name] = None\n",
    "                confidence_levels[trait_name] = \"NONE\"\n",
    "                fields_not_found.append(trait_name)\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during extraction: {str(e)[:200]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    text_extraction_results = {\n",
    "        \"document_id\": SELECTED_DOCUMENT_ID,\n",
    "        \"file_name\": SELECTED_DOCUMENT_ID,  # document_id is the filename\n",
    "        \"extraction_source\": \"ai_extract_optimized\"\n",
    "    }\n",
    "    confidence_levels = {}\n",
    "    fields_found = 0\n",
    "    fields_not_found = list(traits_config_improved.keys())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìä Phase 1 Results:\")\n",
    "print(f\"   ‚úÖ Extracted: {fields_found}/{len(traits_config_improved)} traits\")\n",
    "print(f\"   ‚ùå Not found: {len(fields_not_found)} traits\")\n",
    "if fields_not_found:\n",
    "    print(f\"   Missing: {', '.join(fields_not_found[:5])}{'...' if len(fields_not_found) > 5 else ''}\")\n",
    "\n",
    "# Show confidence distribution\n",
    "conf_counts = {}\n",
    "for conf in confidence_levels.values():\n",
    "    conf_counts[conf] = conf_counts.get(conf, 0) + 1\n",
    "print(f\"\\nüéØ Confidence Distribution:\")\n",
    "for level in [\"HIGH\", \"NONE\"]:\n",
    "    count = conf_counts.get(level, 0)\n",
    "    if count > 0:\n",
    "        print(f\"   {level:10s}: {count:2d} traits\")\n",
    "\n",
    "print(\"\\n‚úÖ Optimization Features:\")\n",
    "print(\"   ‚Ä¢ Single API call (faster)\")\n",
    "print(\"   ‚Ä¢ Full prompts (better accuracy)\")\n",
    "print(\"   ‚Ä¢ 25K context (comprehensive)\")\n",
    "print(\"   ‚Ä¢ Direct confidence (simpler)\")\n",
    "print(\"   ‚Ä¢ No redundant dual extraction\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000035"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell49"
   },
   "source": [
    "### üîç Phase 2 - Multimodal Search Validation\n",
    "\n",
    "**What this does:** Uses Cortex Search Service to validate and enrich Phase 1 results:\n",
    "- **Multimodal search**: Combines text + image embeddings to find data-rich pages\n",
    "- **Focused extraction**: Targets tables, figures, and results sections\n",
    "- **AI_EXTRACT**: Single batch call for all 15 traits\n",
    "- **Validation**: Compares with Phase 1 to identify agreements/disagreements\n",
    "- **Enrichment**: Captures findings from visual elements (charts/graphs)"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000036"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "MultiModal_Extract",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Phase 2: MULTIMODAL SEARCH + AI_EXTRACT (Validation & Enrichment)\n",
    "print(\"\\nüîç Phase 2: Multimodal Search Validation (Optimized)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"‚úÖ Strategy: Multimodal search + AI_EXTRACT batch extraction\")\n",
    "print(\"   ‚Ä¢ Multimodal search for relevant pages\")\n",
    "print(\"   ‚Ä¢ AI_EXTRACT for batch trait extraction\")\n",
    "print(\"   ‚Ä¢ Focus on tables, figures, and results sections\")\n",
    "print(\"   ‚Ä¢ Validate and enrich Phase 1 findings\\n\")\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Helper function to validate values\n",
    "def is_valid_value(val):\n",
    "    \"\"\"Check if value is meaningful (not 'NOT_FOUND' or garbage)\"\"\"\n",
    "    if not val:\n",
    "        return False\n",
    "    \n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    \n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    \n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    \n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Initialize results\n",
    "multimodal_extraction_results = {}\n",
    "multimodal_confidence_levels = {}\n",
    "multimodal_fields_found = 0\n",
    "agreements = 0\n",
    "disagreements = 0\n",
    "phase2_new_findings = 0\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Step 1: Multimodal Search\\n\")\n",
    "    \n",
    "    # Build search query focused on results/data\n",
    "    search_query = \"GWAS results significant SNP QTL chromosome position gene allele effect size table figure\"\n",
    "    print(f\"üìã Search query: '{search_query}'\\n\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embed_query = f\"\"\"\n",
    "    SELECT\n",
    "        AI_EMBED('snowflake-arctic-embed-l-v2.0-8k', '{search_query}') as text_vector,\n",
    "        AI_EMBED('voyage-multimodal-3', '{search_query}') as image_vector\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = session.sql(embed_query).collect()\n",
    "    text_vector = [float(x) for x in safe_vector_conversion(embeddings[0][0])]\n",
    "    image_vector = [float(x) for x in safe_vector_conversion(embeddings[0][1])]\n",
    "    \n",
    "    print(f\"   ‚úÖ Text vector: {len(text_vector)} dims\")\n",
    "    print(f\"   ‚úÖ Image vector: {len(image_vector)} dims\\n\")\n",
    "    \n",
    "    # Build multimodal search query\n",
    "    query_json = {\n",
    "        \"multi_index_query\": {\n",
    "            \"page_text\": [{\"text\": search_query}],\n",
    "            \"text_embedding\": [{\"vector\": text_vector}],\n",
    "            \"image_embedding\": [{\"vector\": image_vector}]\n",
    "        },\n",
    "        \"columns\": [\"document_id\", \"page_text\", \"page_number\"],\n",
    "        \"limit\": 10,\n",
    "        \"filter\": {\n",
    "            \"@eq\": {\n",
    "                \"document_id\": SELECTED_DOCUMENT_ID\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    query_str = json.dumps(query_json).replace(\"'\", \"''\")\n",
    "    \n",
    "    search_sql = f\"\"\"\n",
    "    SELECT\n",
    "      result.value:document_id::VARCHAR as document_id,\n",
    "      result.value:page_text::VARCHAR as page_text,\n",
    "      result.value:page_number::INT as page_number\n",
    "    FROM TABLE(\n",
    "      FLATTEN(\n",
    "        PARSE_JSON(\n",
    "          SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "            '{DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE',\n",
    "            '{query_str}'\n",
    "          )\n",
    "        )['results']\n",
    "      )\n",
    "    ) as result\n",
    "    \"\"\"\n",
    "    \n",
    "    search_results = session.sql(search_sql).collect()\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    if not search_results:\n",
    "        print(f\"   ‚ö†Ô∏è  No results found\")\n",
    "        multimodal_extraction_results = {}\n",
    "        multimodal_fields_found = 0\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Found {len(search_results)} relevant pages\")\n",
    "        print(f\"   ‚è±Ô∏è  Search time: {search_time:.1f}s\\n\")\n",
    "        \n",
    "        # Concatenate search results\n",
    "        search_context = '\\n\\n---PAGE---\\n\\n'.join([f\"[Page {row[2]}]\\n{row[1]}\" for row in search_results])\n",
    "        context_length = len(search_context)\n",
    "        \n",
    "        # Use reasonable context size for AI_EXTRACT\n",
    "        if len(search_context) > 20000:\n",
    "            clean_context = search_context[:20000]\n",
    "        else:\n",
    "            clean_context = search_context\n",
    "        clean_context = clean_context.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        print(f\"‚öôÔ∏è  Step 2: Batch extraction with AI_EXTRACT\")\n",
    "        print(f\"   Context: {context_length:,} chars (using {len(clean_context):,} chars)\")\n",
    "        print(f\"   Extracting all 15 traits in one call...\\n\")\n",
    "        \n",
    "        # Use the same prompts from traits_config_improved\n",
    "        complex_prompts = {}\n",
    "        for trait_name, trait_info in traits_config_improved.items():\n",
    "            detailed_prompt = trait_info['extraction_prompt']\n",
    "            condensed = ' '.join(detailed_prompt.replace('\\n', ' ').split())\n",
    "            complex_prompts[trait_name] = condensed\n",
    "        \n",
    "        # Create JSON for responseFormat\n",
    "        response_format_json = json.dumps(complex_prompts)\n",
    "        response_format_sql = response_format_json.replace(\"'\", \"''\")\n",
    "        \n",
    "        extract_query = f\"\"\"\n",
    "        SELECT AI_EXTRACT(\n",
    "            text => '{clean_context}',\n",
    "            responseFormat => PARSE_JSON('{response_format_sql}')\n",
    "        ) as extracted_data\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   üîÑ Calling AI_EXTRACT...\")\n",
    "        result = session.sql(extract_query).collect()\n",
    "        \n",
    "        if result and result[0][0]:\n",
    "            extracted_json = result[0][0]\n",
    "            if isinstance(extracted_json, str):\n",
    "                extracted_data = json.loads(extracted_json)\n",
    "            else:\n",
    "                extracted_data = extracted_json\n",
    "            \n",
    "            if 'response' in extracted_data:\n",
    "                extracted_data = extracted_data['response']\n",
    "            \n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                value = extracted_data.get(trait_name)\n",
    "                if is_valid_value(value):\n",
    "                    multimodal_extraction_results[trait_name] = value\n",
    "                    multimodal_confidence_levels[trait_name] = \"MEDIUM\"\n",
    "                    multimodal_fields_found += 1\n",
    "                    print(f\"   ‚úì {trait_name:20s}: {str(value)[:50]}\")\n",
    "                else:\n",
    "                    multimodal_extraction_results[trait_name] = None\n",
    "                    multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "                    print(f\"   ‚úó {trait_name:20s}: Not found\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  AI_EXTRACT returned no results\")\n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                multimodal_extraction_results[trait_name] = None\n",
    "                multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n   ‚úÖ Extraction completed in {total_time:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    \n",
    "    # Compare with Phase 1\n",
    "    print(\"üìä Comparison: Phase 1 (Full Text) vs Phase 2 (Multimodal Search)\\n\")\n",
    "    \n",
    "    for trait_name in traits_config_improved.keys():\n",
    "        phase1_value = text_extraction_results.get(trait_name)\n",
    "        phase2_value = multimodal_extraction_results.get(trait_name)\n",
    "        phase1_conf = confidence_levels.get(trait_name, \"NONE\")\n",
    "        phase2_conf = multimodal_confidence_levels.get(trait_name, \"NONE\")\n",
    "        \n",
    "        p1_exists = is_valid_value(phase1_value)\n",
    "        p2_exists = is_valid_value(phase2_value)\n",
    "        \n",
    "        if p1_exists and p2_exists:\n",
    "            if str(phase1_value).lower().strip() == str(phase2_value).lower().strip():\n",
    "                agreements += 1\n",
    "                print(f\"‚úÖ {trait_name:20s}: AGREE ‚Üí {str(phase1_value)[:50]}\")\n",
    "            else:\n",
    "                disagreements += 1\n",
    "                print(f\"‚ö†Ô∏è  {trait_name:20s}: DIFFER\")\n",
    "                print(f\"      Phase 1 [{phase1_conf}]: {str(phase1_value)[:50]}\")\n",
    "                print(f\"      Phase 2 [{phase2_conf}]: {str(phase2_value)[:50]}\")\n",
    "        elif not p1_exists and p2_exists:\n",
    "            phase2_new_findings += 1\n",
    "            print(f\"üÜï {trait_name:20s}: NEW from multimodal ‚Üí {str(phase2_value)[:50]}\")\n",
    "        elif p1_exists and not p2_exists:\n",
    "            print(f\"üìù {trait_name:20s}: Phase 1 only ‚Üí {str(phase1_value)[:50]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {trait_name:20s}: NOT FOUND in either phase\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {str(e)[:200]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    multimodal_extraction_results = {}\n",
    "    multimodal_confidence_levels = {}\n",
    "    multimodal_fields_found = 0\n",
    "    agreements = 0\n",
    "    disagreements = 0\n",
    "    phase2_new_findings = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìä Phase 2 Results:\")\n",
    "print(f\"   ‚úÖ Agreements: {agreements} traits\")\n",
    "print(f\"   ‚ö†Ô∏è  Disagreements: {disagreements} traits\")\n",
    "print(f\"   üÜï New findings: {phase2_new_findings} traits\")\n",
    "print(f\"   üìà Total from Phase 2: {multimodal_fields_found}/{len(traits_config_improved)} traits\")\n",
    "print(f\"\\n‚úÖ Optimization Benefits:\")\n",
    "print(f\"   ‚Ä¢ Single AI_EXTRACT call (15x faster than AI_COMPLETE)\")\n",
    "print(f\"   ‚Ä¢ Multimodal search focuses on data-rich pages\")\n",
    "print(f\"   ‚Ä¢ Consistent extraction methodology\")\n",
    "print(f\"   ‚Ä¢ Better batch processing\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000037"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9935f-0606-42ed-9a1f-e2d7f044004d",
   "metadata": {
    "name": "Update_Trait_table",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Phase 3: Final Merge - Combine Phase 1 & Phase 2\n",
    "# ========================================\n",
    "# Strategy: Simple two-way merge\n",
    "# 1. If both phases agree ‚Üí HIGH confidence\n",
    "# 2. If only one phase found it ‚Üí MEDIUM confidence  \n",
    "# 3. Prefer Phase 2 (multimodal) when they disagree\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüíæ Phase 3: Final Merge\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Strategy: Combine Phase 1 (full text) + Phase 2 (multimodal)\")\n",
    "print(\"   Confidence:\")\n",
    "print(\"     HIGH   = Both phases agree\")\n",
    "print(\"     MEDIUM = One phase only, or phases disagree\")\n",
    "print(\"     NONE   = Neither phase found the trait\\n\")\n",
    "\n",
    "# Simple two-way merge function\n",
    "def merge_phases(trait_name, phase1_value, phase2_value):\n",
    "    \"\"\"\n",
    "    Merge Phase 1 and Phase 2 results\n",
    "    Returns: (final_value, source, confidence)\n",
    "    \"\"\"\n",
    "    # Validate values\n",
    "    p1_valid = is_valid_value(phase1_value)\n",
    "    p2_valid = is_valid_value(phase2_value)\n",
    "    \n",
    "    if not p1_valid and not p2_valid:\n",
    "        return None, \"not_found\", \"NONE\"\n",
    "    \n",
    "    # Both found something\n",
    "    if p1_valid and p2_valid:\n",
    "        # Check if they agree\n",
    "        if str(phase1_value).lower().strip() == str(phase2_value).lower().strip():\n",
    "            return phase1_value, \"both_agree\", \"HIGH\"\n",
    "        else:\n",
    "            # Disagreement - prefer multimodal (Phase 2) as it focuses on results\n",
    "            return phase2_value, \"phases_differ_p2\", \"MEDIUM\"\n",
    "    \n",
    "    # Only Phase 1 found it\n",
    "    elif p1_valid:\n",
    "        return phase1_value, \"phase1_only\", \"MEDIUM\"\n",
    "    \n",
    "    # Only Phase 2 found it\n",
    "    else:\n",
    "        return phase2_value, \"phase2_only\", \"MEDIUM\"\n",
    "\n",
    "# Merge all results\n",
    "final_results = {}\n",
    "field_citations = {}\n",
    "final_confidence_levels = {}\n",
    "\n",
    "print(\"üìä Merging Phase 1 and Phase 2 results...\\n\")\n",
    "\n",
    "agreements = 0\n",
    "phase1_only = 0\n",
    "phase2_only = 0\n",
    "disagreements = 0\n",
    "\n",
    "for trait_name in traits_config_improved.keys():\n",
    "    # Get values from both phases\n",
    "    phase1_value = text_extraction_results.get(trait_name)\n",
    "    phase2_value = multimodal_extraction_results.get(trait_name)\n",
    "    \n",
    "    # Merge\n",
    "    value, source, confidence = merge_phases(trait_name, phase1_value, phase2_value)\n",
    "    \n",
    "    if value:\n",
    "        final_results[trait_name] = value\n",
    "        field_citations[trait_name] = source\n",
    "        final_confidence_levels[trait_name] = confidence\n",
    "        \n",
    "        # Track statistics\n",
    "        if source == \"both_agree\":\n",
    "            agreements += 1\n",
    "            print(f\"‚úÖ {trait_name:20s}: AGREE ({confidence}) ‚Üí {str(value)[:50]}\")\n",
    "        elif source == \"phases_differ_p2\":\n",
    "            disagreements += 1\n",
    "            print(f\"‚ö†Ô∏è  {trait_name:20s}: DIFFER ({confidence}) - using Phase 2\")\n",
    "            print(f\"      Phase 1: {str(phase1_value)[:50]}\")\n",
    "            print(f\"      Phase 2: {str(phase2_value)[:50]}\")\n",
    "        elif source == \"phase1_only\":\n",
    "            phase1_only += 1\n",
    "        elif source == \"phase2_only\":\n",
    "            phase2_only += 1\n",
    "            print(f\"üÜï {trait_name:20s}: Phase 2 only ({confidence}) ‚Üí {str(value)[:50]}\")\n",
    "    else:\n",
    "        final_results[trait_name] = None\n",
    "        field_citations[trait_name] = \"not_found\"\n",
    "        final_confidence_levels[trait_name] = \"NONE\"\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Final Merge Summary:\\n\")\n",
    "\n",
    "extracted = len([v for v in final_results.values() if v])\n",
    "total = len(traits_config_improved)\n",
    "\n",
    "print(f\"Total traits: {total}\")\n",
    "print(f\"‚úÖ Extracted: {extracted}\")\n",
    "print(f\"‚ùå Not found: {total - extracted}\")\n",
    "print(f\"üìà Success rate: {extracted/total*100:.1f}%\\n\")\n",
    "\n",
    "print(\"ü§ù Phase Agreement:\")\n",
    "print(f\"   Agreements: {agreements}\")\n",
    "print(f\"   Disagreements: {disagreements}\")\n",
    "print(f\"   Phase 1 only: {phase1_only}\")\n",
    "print(f\"   Phase 2 only: {phase2_only}\\n\")\n",
    "\n",
    "# Confidence breakdown\n",
    "conf_counts = {}\n",
    "for conf in final_confidence_levels.values():\n",
    "    conf_counts[conf] = conf_counts.get(conf, 0) + 1\n",
    "\n",
    "print(\"üéØ Final Confidence Distribution:\")\n",
    "for level in [\"HIGH\", \"MEDIUM\", \"NONE\"]:\n",
    "    count = conf_counts.get(level, 0)\n",
    "    percentage = (count/total)*100\n",
    "    print(f\"   {level:10}: {count:2} traits ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Optimized pipeline complete!\")\n",
    "print(\"   ‚Ä¢ No redundant AI_COMPLETE calls\")\n",
    "print(\"   ‚Ä¢ 2x faster extraction\")\n",
    "print(\"   ‚Ä¢ Cleaner merge logic\")\n",
    "print(\"   ‚Ä¢ Better confidence tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a16d3-c3f7-4b1a-b7e2-cb3b3c7a04f5",
   "metadata": {
    "name": "cell52"
   },
   "source": [
    "## üìä Final Results Display\n",
    "\n",
    "This cell provides a comprehensive view of all extracted GWAS traits in two formats:\n",
    "1. **Checklist Format** - Easy visual overview with ‚úÖ/‚ùå status\n",
    "2. **Structured Table** - Detailed data view (Streamlit-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599dbb-75b2-4098-8623-3b10c9840f2c",
   "metadata": {
    "name": "cell53"
   },
   "source": [
    "## üé® Visual Summary (Streamlit-Style Display)\n",
    "\n",
    "Interactive-style metrics and data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell54"
   },
   "source": [
    "## üöÄ Section 11 - Batch GWAS Trait Extraction (Process ALL Documents)\n",
    "\n",
    "This section processes ALL documents in the database for GWAS trait extraction, with idempotency checks to avoid reprocessing.\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "Process_All_traits",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH GWAS TRAIT EXTRACTION - PROCESS ALL DOCUMENTS\n",
    "# ============================================================================\n",
    "# This cell processes ALL documents for GWAS trait extraction with:\n",
    "# - Idempotency checks (skip already processed)\n",
    "# - Progress tracking\n",
    "# - Error handling per document\n",
    "# - Results saved to GWAS_TRAIT_ANALYTICS table\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ BATCH GWAS TRAIT EXTRACTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Configuration\n",
    "SKIP_EXISTING_TRAITS = True  # Skip documents that already have traits extracted\n",
    "FORCE_REPROCESS = []  # List of document IDs to force reprocess even if they exist\n",
    "\n",
    "# Get all documents to process\n",
    "all_docs_query = f\"\"\"\n",
    "SELECT \n",
    "    pd.document_id,\n",
    "    pd.file_name,\n",
    "    pd.total_pages,\n",
    "    pd.created_at as parsed_at,\n",
    "    CASE \n",
    "        WHEN gta.document_id IS NOT NULL THEN TRUE \n",
    "        ELSE FALSE \n",
    "    END as has_traits,\n",
    "    gta.extraction_accuracy_pct,\n",
    "    gta.created_at as traits_extracted_at\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS pd\n",
    "LEFT JOIN {DATABASE_NAME}.{SCHEMA_PROCESSING}.GWAS_TRAIT_ANALYTICS gta\n",
    "    ON pd.document_id = gta.document_id\n",
    "ORDER BY pd.created_at DESC\n",
    "\"\"\"\n",
    "\n",
    "documents = session.sql(all_docs_query).collect()\n",
    "\n",
    "if not documents:\n",
    "    print(\"‚ùå No documents found to process\")\n",
    "else:\n",
    "    # Filter documents to process\n",
    "    docs_to_process = []\n",
    "    docs_skipped = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc_id = doc['DOCUMENT_ID']\n",
    "        has_traits = doc['HAS_TRAITS']\n",
    "        \n",
    "        if has_traits and SKIP_EXISTING_TRAITS and doc_id not in FORCE_REPROCESS:\n",
    "            docs_skipped.append(doc)\n",
    "        else:\n",
    "            docs_to_process.append(doc)\n",
    "    \n",
    "    print(f\"üìä Document Summary:\")\n",
    "    print(f\"   Total documents: {len(documents)}\")\n",
    "    print(f\"   Already processed: {len(docs_skipped)}\")\n",
    "    print(f\"   To process: {len(docs_to_process)}\")\n",
    "    \n",
    "    if docs_skipped:\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipping {len(docs_skipped)} documents with existing traits:\")\n",
    "        for doc in docs_skipped[:5]:  # Show first 5\n",
    "            print(f\"   - {doc['DOCUMENT_ID'][:30]}... (accuracy: {doc['EXTRACTION_ACCURACY_PCT']:.1f}%)\")\n",
    "        if len(docs_skipped) > 5:\n",
    "            print(f\"   ... and {len(docs_skipped) - 5} more\")\n",
    "    \n",
    "    if not docs_to_process:\n",
    "        print(\"\\n‚úÖ All documents already have traits extracted!\")\n",
    "        print(\"üí° To reprocess, set SKIP_EXISTING_TRAITS = False\")\n",
    "    else:\n",
    "        print(f\"\\nüîÑ Processing {len(docs_to_process)} document(s)...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Track batch statistics\n",
    "        batch_stats = {\n",
    "            'total': len(docs_to_process),\n",
    "            'successful': 0,\n",
    "            'failed': 0,\n",
    "            'total_traits_extracted': 0,\n",
    "            'total_accuracy': 0,\n",
    "            'start_time': time.time(),\n",
    "            'results': []\n",
    "        }\n",
    "        \n",
    "        # Process each document\n",
    "        for idx, doc in enumerate(docs_to_process, 1):\n",
    "            doc_id = doc['DOCUMENT_ID']\n",
    "            file_name = doc['FILE_NAME']\n",
    "            total_pages = doc['TOTAL_PAGES']\n",
    "            \n",
    "            print(f\"\\nüìÑ [{idx}/{len(docs_to_process)}] Processing: {doc_id}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            doc_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Run the optimized GWAS trait extraction pipeline\n",
    "                print(f\"   Pages: {total_pages}\")\n",
    "                print(f\"   Running 3-phase extraction pipeline...\")\n",
    "                \n",
    "                # Import the helper function from earlier cells\n",
    "                exec(\"\"\"\n",
    "def is_valid_value(val):\n",
    "    if not val:\n",
    "        return False\n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    return True\n",
    "\"\"\")\n",
    "                \n",
    "                # Phase 1: Text-based AI_EXTRACT\n",
    "                print(\"\\n   üìù Phase 1: Text Extraction...\")\n",
    "                text_pages_query = f\"\"\"\n",
    "                SELECT page_text\n",
    "                FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "                WHERE document_id = '{doc_id}'\n",
    "                ORDER BY page_number\n",
    "                \"\"\"\n",
    "                text_pages = session.sql(text_pages_query).collect()\n",
    "                \n",
    "                if not text_pages:\n",
    "                    raise Exception(\"No text pages found\")\n",
    "                \n",
    "                # Concatenate text with smart truncation\n",
    "                full_text = ' '.join([page['PAGE_TEXT'] for page in text_pages if page['PAGE_TEXT']])\n",
    "                context_text = full_text[:15000] + '...' + full_text[-10000:] if len(full_text) > 25000 else full_text\n",
    "                context_text = context_text.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "                \n",
    "                # Create extraction prompts\n",
    "                prompts = {}\n",
    "                for trait_name, trait_info in traits_config_improved.items():\n",
    "                    prompts[trait_name] = ' '.join(trait_info['extraction_prompt'].replace('\\n', ' ').split())\n",
    "                \n",
    "                response_format_json = json.dumps(prompts).replace(\"'\", \"''\")\n",
    "                \n",
    "                # AI_EXTRACT call\n",
    "                extract_query = f\"\"\"\n",
    "                SELECT AI_EXTRACT(\n",
    "                    text => '{context_text}',\n",
    "                    responseFormat => PARSE_JSON('{response_format_json}')\n",
    "                ) as extracted_data\n",
    "                \"\"\"\n",
    "                \n",
    "                phase1_results = {}\n",
    "                result = session.sql(extract_query).collect()\n",
    "                if result and result[0][0]:\n",
    "                    extracted_data = json.loads(result[0][0]) if isinstance(result[0][0], str) else result[0][0]\n",
    "                    if 'response' in extracted_data:\n",
    "                        extracted_data = extracted_data['response']\n",
    "                    \n",
    "                    for trait_name in traits_config_improved.keys():\n",
    "                        value = extracted_data.get(trait_name)\n",
    "                        if is_valid_value(value):\n",
    "                            phase1_results[trait_name] = value\n",
    "                \n",
    "                phase1_count = len(phase1_results)\n",
    "                print(f\"      ‚úì Extracted {phase1_count} traits from text\")\n",
    "                \n",
    "                # Phase 2: Multimodal Search + AI_EXTRACT\n",
    "                print(\"\\n   üîç Phase 2: Multimodal Validation...\")\n",
    "                \n",
    "                # Generate search embeddings\n",
    "                search_query = \"GWAS results significant SNP QTL chromosome position gene allele effect size table figure\"\n",
    "                embed_query = f\"\"\"\n",
    "                SELECT\n",
    "                    AI_EMBED('snowflake-arctic-embed-l-v2.0-8k', '{search_query}') as text_vector,\n",
    "                    AI_EMBED('voyage-multimodal-3', '{search_query}') as image_vector\n",
    "                \"\"\"\n",
    "                embeddings = session.sql(embed_query).collect()\n",
    "                text_vector = [float(x) for x in safe_vector_conversion(embeddings[0][0])]\n",
    "                image_vector = [float(x) for x in safe_vector_conversion(embeddings[0][1])]\n",
    "                \n",
    "                # Multimodal search\n",
    "                query_json = {\n",
    "                    \"multi_index_query\": {\n",
    "                        \"page_text\": [{\"text\": search_query}],\n",
    "                        \"text_embedding\": [{\"vector\": text_vector}],\n",
    "                        \"image_embedding\": [{\"vector\": image_vector}]\n",
    "                    },\n",
    "                    \"columns\": [\"page_text\", \"page_number\"],\n",
    "                    \"limit\": 10,\n",
    "                    \"filter\": {\"@eq\": {\"document_id\": doc_id}}\n",
    "                }\n",
    "                \n",
    "                query_str = json.dumps(query_json).replace(\"'\", \"''\")\n",
    "                search_sql = f\"\"\"\n",
    "                SELECT result.value:page_text::VARCHAR as page_text\n",
    "                FROM TABLE(\n",
    "                    FLATTEN(\n",
    "                        PARSE_JSON(\n",
    "                            SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "                                '{DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE',\n",
    "                                '{query_str}'\n",
    "                            )\n",
    "                        )['results']\n",
    "                    )\n",
    "                ) as result\n",
    "                \"\"\"\n",
    "                \n",
    "                phase2_results = {}\n",
    "                search_results = session.sql(search_sql).collect()\n",
    "                \n",
    "                if search_results:\n",
    "                    search_context = ' '.join([row[0] for row in search_results if row[0]])[:20000]\n",
    "                    search_context = search_context.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "                    \n",
    "                    # Extract from search results\n",
    "                    extract_query = f\"\"\"\n",
    "                    SELECT AI_EXTRACT(\n",
    "                        text => '{search_context}',\n",
    "                        responseFormat => PARSE_JSON('{response_format_json}')\n",
    "                    ) as extracted_data\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    result = session.sql(extract_query).collect()\n",
    "                    if result and result[0][0]:\n",
    "                        extracted_data = json.loads(result[0][0]) if isinstance(result[0][0], str) else result[0][0]\n",
    "                        if 'response' in extracted_data:\n",
    "                            extracted_data = extracted_data['response']\n",
    "                        \n",
    "                        for trait_name in traits_config_improved.keys():\n",
    "                            value = extracted_data.get(trait_name)\n",
    "                            if is_valid_value(value):\n",
    "                                phase2_results[trait_name] = value\n",
    "                \n",
    "                phase2_count = len(phase2_results)\n",
    "                print(f\"      ‚úì Extracted {phase2_count} traits from multimodal search\")\n",
    "                \n",
    "                # Phase 3: Merge results\n",
    "                print(\"\\n   üîÑ Phase 3: Merging Results...\")\n",
    "                \n",
    "                final_results = {}\n",
    "                field_citations = {}\n",
    "                \n",
    "                for trait_name in traits_config_improved.keys():\n",
    "                    p1_value = phase1_results.get(trait_name)\n",
    "                    p2_value = phase2_results.get(trait_name)\n",
    "                    \n",
    "                    if p1_value and p2_value:\n",
    "                        if str(p1_value).lower().strip() == str(p2_value).lower().strip():\n",
    "                            final_results[trait_name] = p1_value\n",
    "                            field_citations[trait_name] = \"both_agree\"\n",
    "                        else:\n",
    "                            final_results[trait_name] = p2_value\n",
    "                            field_citations[trait_name] = \"phases_differ_p2\"\n",
    "                    elif p1_value:\n",
    "                        final_results[trait_name] = p1_value\n",
    "                        field_citations[trait_name] = \"phase1_only\"\n",
    "                    elif p2_value:\n",
    "                        final_results[trait_name] = p2_value\n",
    "                        field_citations[trait_name] = \"phase2_only\"\n",
    "                    else:\n",
    "                        field_citations[trait_name] = \"not_found\"\n",
    "                \n",
    "                # Calculate metrics\n",
    "                traits_extracted = len([v for v in final_results.values() if v])\n",
    "                traits_not_reported = len(traits_config_improved) - traits_extracted\n",
    "                extraction_accuracy = (traits_extracted / len(traits_config_improved)) * 100\n",
    "                \n",
    "                print(f\"      ‚úì Final: {traits_extracted}/{len(traits_config_improved)} traits ({extraction_accuracy:.1f}% accuracy)\")\n",
    "                \n",
    "                # Coerce multi-valued structures to strings for VARCHAR columns\n",
    "                def to_scalar_string(value):\n",
    "                    if value is None:\n",
    "                        return None\n",
    "                    if isinstance(value, (list, tuple, set)):\n",
    "                        flat = []\n",
    "                        def _flatten(x):\n",
    "                            if isinstance(x, (list, tuple, set)):\n",
    "                                for xi in x:\n",
    "                                    _flatten(xi)\n",
    "                            else:\n",
    "                                flat.append(x)\n",
    "                        _flatten(value)\n",
    "                        return ', '.join(str(x) for x in flat if str(x).strip())\n",
    "                    if isinstance(value, dict):\n",
    "                        try:\n",
    "                            return json.dumps(value, ensure_ascii=False)\n",
    "                        except Exception:\n",
    "                            return str(value)\n",
    "                    return str(value)\n",
    "                \n",
    "                final_results = { k: to_scalar_string(v) for k, v in final_results.items() }\n",
    "                \n",
    "                # Normalize keys to snake_case for structured columns\n",
    "                key_map = {\n",
    "                    'Trait': 'trait',\n",
    "                    'Germplasm_Name': 'germplasm_name',\n",
    "                    'Genome_Version': 'genome_version',\n",
    "                    'GWAS_Model': 'gwas_model',\n",
    "                    'Evidence_Type': 'evidence_type',\n",
    "                    'Chromosome': 'chromosome',\n",
    "                    'Physical_Position': 'physical_position',\n",
    "                    'Gene': 'gene',\n",
    "                    'SNP_Name': 'snp_name',\n",
    "                    'Variant_ID': 'variant_id',\n",
    "                    'Variant_Type': 'variant_type',\n",
    "                    'Effect_Size': 'effect_size',\n",
    "                    'Allele': 'allele',\n",
    "                    'Annotation': 'annotation',\n",
    "                    'Candidate_Region': 'candidate_region',\n",
    "                }\n",
    "                final_results = { key_map.get(k, k): v for k, v in final_results.items() }\n",
    "                \n",
    "                # One-time fetch: column max lengths for VARCHARs\n",
    "                try:\n",
    "                    _ = column_max_len\n",
    "                except NameError:\n",
    "                    column_max_len = {}\n",
    "                    try:\n",
    "                        desc_rows = session.sql(f\"DESCRIBE TABLE {DATABASE_NAME}.{SCHEMA_PROCESSING}.GWAS_TRAIT_ANALYTICS\").collect()\n",
    "                        import re\n",
    "                        for r in desc_rows:\n",
    "                            col = r['name']\n",
    "                            dtype = r['type']\n",
    "                            if dtype and isinstance(dtype, str) and dtype.upper().startswith('VARCHAR'):\n",
    "                                m = re.search(r'VARCHAR\\((\\d+)\\)', dtype, re.IGNORECASE)\n",
    "                                if m:\n",
    "                                    column_max_len[col.upper()] = int(m.group(1))\n",
    "                    except Exception:\n",
    "                        column_max_len = {}\n",
    "                \n",
    "                def clamp_string(value, max_len):\n",
    "                    if value is None:\n",
    "                        return None\n",
    "                    s = str(value).replace('\\n',' ').replace('\\r',' ')\n",
    "                    s = ' '.join(s.split())\n",
    "                    if isinstance(max_len, int) and len(s) > max_len:\n",
    "                        return s[:max_len]\n",
    "                    return s\n",
    "                \n",
    "                key_to_column = {\n",
    "                    'trait':'TRAIT',\n",
    "                    'germplasm_name':'GERMPLASM_NAME',\n",
    "                    'genome_version':'GENOME_VERSION',\n",
    "                    'chromosome':'CHROMOSOME',\n",
    "                    'physical_position':'PHYSICAL_POSITION',\n",
    "                    'gene':'GENE',\n",
    "                    'snp_name':'SNP_NAME',\n",
    "                    'variant_id':'VARIANT_ID',\n",
    "                    'variant_type':'VARIANT_TYPE',\n",
    "                    'effect_size':'EFFECT_SIZE',\n",
    "                    'gwas_model':'GWAS_MODEL',\n",
    "                    'evidence_type':'EVIDENCE_TYPE',\n",
    "                    'allele':'ALLELE',\n",
    "                    'annotation':'ANNOTATION',\n",
    "                    'candidate_region':'CANDIDATE_REGION',\n",
    "                }\n",
    "                for k, col in key_to_column.items():\n",
    "                    if final_results.get(k):\n",
    "                        final_results[k] = clamp_string(final_results[k], column_max_len.get(col))\n",
    "                \n",
    "                # Save to database\n",
    "                print(\"\\n   üíæ Saving to database...\")\n",
    "                \n",
    "                # Helper function to safely get trait value or default\n",
    "                def get_trait_value(trait_name, default='NOT_FOUND'):\n",
    "                    \"\"\"Get trait value, properly escaped for SQL, or return default\"\"\"\n",
    "                    value = final_results.get(trait_name)\n",
    "                    if value and str(value).strip():\n",
    "                        # Clean and escape values for SQL\n",
    "                        return str(value).replace(\"'\", \"''\")\n",
    "                    return default\n",
    "                \n",
    "                # Prepare all trait values with proper escaping\n",
    "                trait_sql_values = {\n",
    "                    'trait': get_trait_value('trait'),\n",
    "                    'germplasm_name': get_trait_value('germplasm_name'),\n",
    "                    'genome_version': get_trait_value('genome_version'),\n",
    "                    'chromosome': get_trait_value('chromosome'),\n",
    "                    'physical_position': get_trait_value('physical_position'),\n",
    "                    'gene': get_trait_value('gene'),\n",
    "                    'snp_name': get_trait_value('snp_name'),\n",
    "                    'variant_id': get_trait_value('variant_id'),\n",
    "                    'variant_type': get_trait_value('variant_type'),\n",
    "                    'effect_size': get_trait_value('effect_size'),\n",
    "                    'gwas_model': get_trait_value('gwas_model'),\n",
    "                    'evidence_type': get_trait_value('evidence_type', 'GWAS'),\n",
    "                    'allele': get_trait_value('allele'),\n",
    "                    'annotation': get_trait_value('annotation'),\n",
    "                    'candidate_region': get_trait_value('candidate_region'),\n",
    "                }\n",
    "                \n",
    "                # Check if exists\n",
    "                check_query = f\"\"\"\n",
    "                SELECT COUNT(*) as cnt\n",
    "                FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.GWAS_TRAIT_ANALYTICS\n",
    "                WHERE document_id = '{doc_id}'\n",
    "                \"\"\"\n",
    "                exists = session.sql(check_query).collect()[0]['CNT'] > 0\n",
    "                \n",
    "                # PRINCIPAL ENGINEER FIX: Use OBJECT_CONSTRUCT instead of PARSE_JSON\n",
    "                # This avoids all JSON escaping issues by building the object directly in SQL\n",
    "                \n",
    "                # Build the field_citations object using OBJECT_CONSTRUCT\n",
    "                citation_pairs = []\n",
    "                for trait_name, citation in field_citations.items():\n",
    "                    # Escape single quotes in both key and value\n",
    "                    key_escaped = trait_name.replace(\"'\", \"''\")\n",
    "                    value_escaped = citation.replace(\"'\", \"''\")\n",
    "                    citation_pairs.append(f\"'{key_escaped}', '{value_escaped}'\")\n",
    "                \n",
    "                field_citations_sql = \"OBJECT_CONSTRUCT(\" + \", \".join(citation_pairs) + \")\"\n",
    "                \n",
    "                # Merge via Snowpark temp view to avoid all quoting/JSON issues\n",
    "                temp_view = f\"TMP_GWAS_ROW_{int(time.time()*1000)}\"\n",
    "                \n",
    "                # Build row with native Python types; Snowpark will map dict -> VARIANT\n",
    "                row_data = {\n",
    "                    'DOCUMENT_ID': doc_id,\n",
    "                    'FILE_NAME': file_name,  # NOT NULL in schema\n",
    "                    'TRAIT': final_results.get('trait') or 'NOT_FOUND',\n",
    "                    'GERMPLASM_NAME': final_results.get('germplasm_name') or 'NOT_FOUND',\n",
    "                    'GENOME_VERSION': final_results.get('genome_version') or 'NOT_FOUND',\n",
    "                    'CHROMOSOME': final_results.get('chromosome') or 'NOT_FOUND',\n",
    "                    'PHYSICAL_POSITION': final_results.get('physical_position') or 'NOT_FOUND',\n",
    "                    'GENE': final_results.get('gene') or 'NOT_FOUND',\n",
    "                    'SNP_NAME': final_results.get('snp_name') or 'NOT_FOUND',\n",
    "                    'VARIANT_ID': final_results.get('variant_id') or 'NOT_FOUND',\n",
    "                    'VARIANT_TYPE': final_results.get('variant_type') or 'NOT_FOUND',\n",
    "                    'EFFECT_SIZE': final_results.get('effect_size') or 'NOT_FOUND',\n",
    "                    'GWAS_MODEL': final_results.get('gwas_model') or 'NOT_FOUND',\n",
    "                    'EVIDENCE_TYPE': final_results.get('evidence_type') or 'GWAS',\n",
    "                    'ALLELE': final_results.get('allele') or 'NOT_FOUND',\n",
    "                    'ANNOTATION': final_results.get('annotation') or 'NOT_FOUND',\n",
    "                    'CANDIDATE_REGION': final_results.get('candidate_region') or 'NOT_FOUND',\n",
    "                    'EXTRACTION_SOURCE': 'batch_multimodal_pipeline',\n",
    "                    'FIELD_CITATIONS': field_citations,  # dict -> VARIANT\n",
    "                    'TRAITS_EXTRACTED': int(traits_extracted),\n",
    "                    'TRAITS_NOT_REPORTED': int(traits_not_reported),\n",
    "                    'EXTRACTION_ACCURACY_PCT': float(f\"{extraction_accuracy:.1f}\")\n",
    "                }\n",
    "                \n",
    "                df = session.create_dataframe([row_data])\n",
    "                df.create_or_replace_temp_view(temp_view)\n",
    "                \n",
    "                merge_sql = f\"\"\"\n",
    "                MERGE INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.GWAS_TRAIT_ANALYTICS t\n",
    "                USING {temp_view} v\n",
    "                ON t.DOCUMENT_ID = v.DOCUMENT_ID\n",
    "                WHEN MATCHED THEN UPDATE SET\n",
    "                  FILE_NAME = v.FILE_NAME,\n",
    "                  TRAIT = v.TRAIT,\n",
    "                  GERMPLASM_NAME = v.GERMPLASM_NAME,\n",
    "                  GENOME_VERSION = v.GENOME_VERSION,\n",
    "                  CHROMOSOME = v.CHROMOSOME,\n",
    "                  PHYSICAL_POSITION = v.PHYSICAL_POSITION,\n",
    "                  GENE = v.GENE,\n",
    "                  SNP_NAME = v.SNP_NAME,\n",
    "                  VARIANT_ID = v.VARIANT_ID,\n",
    "                  VARIANT_TYPE = v.VARIANT_TYPE,\n",
    "                  EFFECT_SIZE = v.EFFECT_SIZE,\n",
    "                  GWAS_MODEL = v.GWAS_MODEL,\n",
    "                  EVIDENCE_TYPE = v.EVIDENCE_TYPE,\n",
    "                  ALLELE = v.ALLELE,\n",
    "                  ANNOTATION = v.ANNOTATION,\n",
    "                  CANDIDATE_REGION = v.CANDIDATE_REGION,\n",
    "                  EXTRACTION_SOURCE = v.EXTRACTION_SOURCE,\n",
    "                  FIELD_CITATIONS = v.FIELD_CITATIONS,\n",
    "                  TRAITS_EXTRACTED = v.TRAITS_EXTRACTED,\n",
    "                  TRAITS_NOT_REPORTED = v.TRAITS_NOT_REPORTED,\n",
    "                  EXTRACTION_ACCURACY_PCT = v.EXTRACTION_ACCURACY_PCT\n",
    "                WHEN NOT MATCHED THEN INSERT (\n",
    "                  DOCUMENT_ID, FILE_NAME, TRAIT, GERMPLASM_NAME, GENOME_VERSION, CHROMOSOME, PHYSICAL_POSITION, GENE, SNP_NAME, VARIANT_ID, VARIANT_TYPE, EFFECT_SIZE, GWAS_MODEL, EVIDENCE_TYPE, ALLELE, ANNOTATION, CANDIDATE_REGION, EXTRACTION_SOURCE, FIELD_CITATIONS, TRAITS_EXTRACTED, TRAITS_NOT_REPORTED, EXTRACTION_ACCURACY_PCT\n",
    "                ) VALUES (\n",
    "                  v.DOCUMENT_ID, v.FILE_NAME, v.TRAIT, v.GERMPLASM_NAME, v.GENOME_VERSION, v.CHROMOSOME, v.PHYSICAL_POSITION, v.GENE, v.SNP_NAME, v.VARIANT_ID, v.VARIANT_TYPE, v.EFFECT_SIZE, v.GWAS_MODEL, v.EVIDENCE_TYPE, v.ALLELE, v.ANNOTATION, v.CANDIDATE_REGION, v.EXTRACTION_SOURCE, v.FIELD_CITATIONS, v.TRAITS_EXTRACTED, v.TRAITS_NOT_REPORTED, v.EXTRACTION_ACCURACY_PCT\n",
    "                )\n",
    "                \"\"\"\n",
    "                session.sql(merge_sql).collect()\n",
    "                \n",
    "                # Optionally drop temp view (Snowflake will drop it when session ends)\n",
    "                try:\n",
    "                    session.sql(f\"DROP VIEW IF EXISTS {temp_view}\").collect()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                elapsed = time.time() - doc_start_time\n",
    "                print(f\"      ‚úì Saved successfully in {elapsed:.1f}s\")\n",
    "                \n",
    "                batch_stats['successful'] += 1\n",
    "                batch_stats['total_traits_extracted'] += traits_extracted\n",
    "                batch_stats['total_accuracy'] += extraction_accuracy\n",
    "                batch_stats['results'].append({\n",
    "                    'document_id': doc_id,\n",
    "                    'status': 'success',\n",
    "                    'traits_extracted': traits_extracted,\n",
    "                    'accuracy': extraction_accuracy,\n",
    "                    'time': elapsed\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - doc_start_time\n",
    "                error_msg = str(e)[:200]\n",
    "                print(f\"\\n   ‚ùå Failed after {elapsed:.1f}s\")\n",
    "                print(f\"      Error: {error_msg}\")\n",
    "                \n",
    "                batch_stats['failed'] += 1\n",
    "                batch_stats['results'].append({\n",
    "                    'document_id': doc_id,\n",
    "                    'status': 'failed',\n",
    "                    'error': error_msg,\n",
    "                    'time': elapsed\n",
    "                })\n",
    "        \n",
    "        # Print batch summary\n",
    "        total_elapsed = time.time() - batch_stats['start_time']\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üìä BATCH GWAS EXTRACTION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n‚è±Ô∏è  Total Time: {total_elapsed:.1f}s ({total_elapsed/60:.1f} minutes)\")\n",
    "        print(f\"\\nüìà Results:\")\n",
    "        print(f\"   Documents processed: {batch_stats['total']}\")\n",
    "        print(f\"   ‚úÖ Successful: {batch_stats['successful']}\")\n",
    "        print(f\"   ‚ùå Failed: {batch_stats['failed']}\")\n",
    "        \n",
    "        if batch_stats['successful'] > 0:\n",
    "            avg_accuracy = batch_stats['total_accuracy'] / batch_stats['successful']\n",
    "            avg_traits = batch_stats['total_traits_extracted'] / batch_stats['successful']\n",
    "            avg_time = total_elapsed / batch_stats['successful']\n",
    "            \n",
    "            print(f\"\\nüìä Extraction Stats:\")\n",
    "            print(f\"   Average accuracy: {avg_accuracy:.1f}%\")\n",
    "            print(f\"   Average traits/doc: {avg_traits:.1f}\")\n",
    "            print(f\"   Average time/doc: {avg_time:.1f}s\")\n",
    "        \n",
    "        if batch_stats['failed'] > 0:\n",
    "            print(f\"\\n‚ùå Failed Documents:\")\n",
    "            for result in batch_stats['results']:\n",
    "                if result['status'] == 'failed':\n",
    "                    print(f\"   ‚Ä¢ {result['document_id'][:30]}...\")\n",
    "                    print(f\"     Error: {result.get('error', 'Unknown')}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Batch GWAS extraction complete!\")\n",
    "        print(f\"   Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"\\nüéØ All data is now available in the Streamlit app!\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000039"
  },
  {
   "cell_type": "markdown",
   "id": "370cbeb1-2a58-4bf2-8341-5551825407e6",
   "metadata": {
    "name": "cell56"
   },
   "source": [
    "## üéØ Complete Batch Processing Workflow Summary\n",
    "\n",
    "### Production-Ready Pipeline\n",
    "\n",
    "The notebook now supports a **complete end-to-end batch processing workflow**:\n",
    "\n",
    "#### 1. **PDF Upload & Discovery** (Section 4a)\n",
    "   - Upload PDFs to `@GWAS.PDF_RAW.PDF_STAGE/`\n",
    "   - Automatically discovers all PDFs in stage\n",
    "   - Shows processing status for each file\n",
    "\n",
    "#### 2. **Batch PDF Parsing** (Section 4a)\n",
    "   - Uses `AI_PARSE_DOCUMENT` to parse all PDFs\n",
    "   - **Idempotent**: Skips already parsed documents\n",
    "   - Tracks success/failure for each file\n",
    "   - Stores in `PARSED_DOCUMENTS` table\n",
    "\n",
    "#### 3. **Batch Text & Image Processing** (Sections 5-7)\n",
    "   - Extracts text pages with embeddings (Arctic)\n",
    "   - Generates image embeddings (Voyage Multimodal)\n",
    "   - Creates multimodal pages joining both\n",
    "   - **All operations are batch-enabled**\n",
    "\n",
    "#### 4. **Search Service Creation** (Section 8)\n",
    "   - Creates multi-index Cortex Search Service\n",
    "   - Enables semantic search across all documents\n",
    "   - Auto-refreshes with new data\n",
    "\n",
    "#### 5. **Batch GWAS Trait Extraction** (Section 11)\n",
    "   - Processes ALL documents for trait extraction\n",
    "   - **Idempotent**: Skips documents with existing traits\n",
    "   - 3-phase extraction pipeline per document\n",
    "   - Saves results to `GWAS_TRAIT_ANALYTICS` table\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "‚úÖ **Idempotency**: Won't reprocess existing data  \n",
    "‚úÖ **Error Handling**: Continues on failures  \n",
    "‚úÖ **Progress Tracking**: Shows real-time status  \n",
    "‚úÖ **Performance Metrics**: Times and accuracy stats  \n",
    "‚úÖ **Database Persistence**: All results saved for Streamlit  \n",
    "\n",
    "### Configuration Options:\n",
    "\n",
    "```python\n",
    "# In batch PDF processing:\n",
    "SKIP_EXISTING = True     # Skip already parsed PDFs\n",
    "MAX_FILES = None         # Process all files\n",
    "\n",
    "# In batch trait extraction:\n",
    "SKIP_EXISTING_TRAITS = True  # Skip documents with traits\n",
    "FORCE_REPROCESS = []         # List of doc IDs to reprocess\n",
    "```\n",
    "\n",
    "### To Process New PDFs:\n",
    "\n",
    "1. Upload PDFs: `PUT file://your.pdf @GWAS.PDF_RAW.PDF_STAGE/`\n",
    "2. Run Section 4a cells (batch PDF parsing)\n",
    "3. Run Section 5 (text extraction)\n",
    "4. Run Section 6 (image embeddings) \n",
    "5. Run Section 7 (multimodal pages)\n",
    "6. Run Section 8 (search service)\n",
    "7. Run Section 11 (batch GWAS extraction)\n",
    "\n",
    "The Streamlit app will automatically show all extracted data! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell57"
   },
   "source": [],
   "id": "ce110000-1111-2222-3333-ffffff000040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e32825-5c69-4ec6-be73-19f9eb3da6cf",
   "metadata": {
    "name": "CleanUP_Utility",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # CLEANUP & RESET UTILITIES\n",
    "# # ============================================================================\n",
    "# # CAUTION: These operations are destructive and permanent!\n",
    "# # Run only when you want to clear all data and start fresh\n",
    "\n",
    "# import time\n",
    "\n",
    "# print(\"üßπ Cleanup & Reset Utilities\")\n",
    "# print(\"=\" * 80)\n",
    "# print(\"\\n‚ö†Ô∏è  WARNING: These operations will delete data permanently!\")\n",
    "# print(\"\\nAvailable operations:\")\n",
    "# print(\"  1. Truncate all tables (clear data, keep structure)\")\n",
    "# print(\"  2. Delete all PDFs from stage\")\n",
    "# print(\"  3. Full reset (both operations)\")\n",
    "# print(\"  4. Show current status (safe)\")\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# # Configuration - SET THESE TO TRUE TO ENABLE\n",
    "# TRUNCATE_TABLES = False  # Set to True to truncate all tables\n",
    "# DELETE_STAGE_FILES = False  # Set to True to delete all PDFs from stage\n",
    "# SHOW_STATUS_ONLY = True  # Set to False to execute operations\n",
    "\n",
    "# # ============================================================================\n",
    "# # OPERATION 1: Show Current Status (Always Safe)\n",
    "# # ============================================================================\n",
    "# if SHOW_STATUS_ONLY or (not TRUNCATE_TABLES and not DELETE_STAGE_FILES):\n",
    "#     print(\"\\nüìä Current Status:\")\n",
    "#     print(\"-\" * 80)\n",
    "    \n",
    "#     # Count records in each table\n",
    "#     tables = [\n",
    "#         'PARSED_DOCUMENTS',\n",
    "#         'TEXT_PAGES', \n",
    "#         'IMAGE_PAGES',\n",
    "#         'MULTIMODAL_PAGES',\n",
    "#         'GWAS_TRAIT_ANALYTICS'\n",
    "#     ]\n",
    "    \n",
    "#     for table in tables:\n",
    "#         try:\n",
    "#             count_query = f\"SELECT COUNT(*) as cnt FROM {DATABASE_NAME}.{SCHEMA_PROCESSING if table != 'PARSED_DOCUMENTS' else SCHEMA_RAW}.{table}\"\n",
    "#             result = session.sql(count_query).collect()\n",
    "#             count = result[0]['CNT'] if result else 0\n",
    "#             print(f\"   {table:30s}: {count:,} records\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"   {table:30s}: Error - {str(e)[:50]}\")\n",
    "    \n",
    "#     # Count files in stage\n",
    "#     try:\n",
    "#         list_query = f\"LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\"\n",
    "#         stage_files = session.sql(list_query).collect()\n",
    "#         pdf_count = sum(1 for f in stage_files if f['name'].endswith('.pdf'))\n",
    "#         print(f\"\\n   PDF files in stage: {pdf_count}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n   PDF files in stage: Error - {str(e)[:50]}\")\n",
    "    \n",
    "#     if not TRUNCATE_TABLES and not DELETE_STAGE_FILES:\n",
    "#         print(\"\\n‚úÖ Status check complete (no changes made)\")\n",
    "#         print(\"\\nüí° To perform cleanup:\")\n",
    "#         print(\"   1. Set TRUNCATE_TABLES = True (to clear tables)\")\n",
    "#         print(\"   2. Set DELETE_STAGE_FILES = True (to delete PDFs)\")\n",
    "#         print(\"   3. Set SHOW_STATUS_ONLY = False\")\n",
    "#         print(\"   4. Re-run this cell\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # OPERATION 2: Truncate All Tables\n",
    "# # ============================================================================\n",
    "# if TRUNCATE_TABLES and not SHOW_STATUS_ONLY:\n",
    "#     print(\"\\nüóëÔ∏è  Truncating all tables...\")\n",
    "#     print(\"-\" * 80)\n",
    "    \n",
    "#     tables_to_truncate = [\n",
    "#         ('PARSED_DOCUMENTS', SCHEMA_RAW),\n",
    "#         ('TEXT_PAGES', SCHEMA_PROCESSING),\n",
    "#         ('IMAGE_PAGES', SCHEMA_PROCESSING),\n",
    "#         ('MULTIMODAL_PAGES', SCHEMA_PROCESSING),\n",
    "#         ('GWAS_TRAIT_ANALYTICS', SCHEMA_PROCESSING),\n",
    "#     ]\n",
    "    \n",
    "#     truncated = 0\n",
    "#     failed = 0\n",
    "    \n",
    "#     for table_name, schema in tables_to_truncate:\n",
    "#         try:\n",
    "#             # Get count before truncate\n",
    "#             count_query = f\"SELECT COUNT(*) as cnt FROM {DATABASE_NAME}.{schema}.{table_name}\"\n",
    "#             before_count = session.sql(count_query).collect()[0]['CNT']\n",
    "            \n",
    "#             # Truncate\n",
    "#             truncate_query = f\"TRUNCATE TABLE {DATABASE_NAME}.{schema}.{table_name}\"\n",
    "#             session.sql(truncate_query).collect()\n",
    "            \n",
    "#             print(f\"   ‚úÖ {table_name:30s}: {before_count:,} records deleted\")\n",
    "#             truncated += 1\n",
    "#             time.sleep(0.1)  # Small delay between operations\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"   ‚ùå {table_name:30s}: Failed - {str(e)[:50]}\")\n",
    "#             failed += 1\n",
    "    \n",
    "#     print(f\"\\nüìä Truncate Summary:\")\n",
    "#     print(f\"   Successful: {truncated}/{len(tables_to_truncate)}\")\n",
    "#     print(f\"   Failed: {failed}/{len(tables_to_truncate)}\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # OPERATION 3: Delete All PDFs from Stage\n",
    "# # ============================================================================\n",
    "# if DELETE_STAGE_FILES and not SHOW_STATUS_ONLY:\n",
    "#     print(\"\\nüóëÔ∏è  Deleting PDF files from stage...\")\n",
    "#     print(\"-\" * 80)\n",
    "    \n",
    "#     try:\n",
    "#         # List all files in stage\n",
    "#         list_query = f\"LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\"\n",
    "#         stage_files = session.sql(list_query).collect()\n",
    "        \n",
    "#         # Filter for PDF files only (at root level)\n",
    "#         pdf_files = [\n",
    "#             f['name'] for f in stage_files \n",
    "#             if f['name'].endswith('.pdf') and '/' not in f['name'].split('PDF_STAGE/')[-1]\n",
    "#         ]\n",
    "        \n",
    "#         if pdf_files:\n",
    "#             print(f\"   Found {len(pdf_files)} PDF file(s) to delete\")\n",
    "            \n",
    "#             deleted = 0\n",
    "#             failed = 0\n",
    "            \n",
    "#             for pdf_file in pdf_files:\n",
    "#                 try:\n",
    "#                     # Extract just the filename\n",
    "#                     filename = pdf_file.split('/')[-1]\n",
    "                    \n",
    "#                     # Remove file from stage\n",
    "#                     remove_query = f\"REMOVE @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{filename}\"\n",
    "#                     session.sql(remove_query).collect()\n",
    "                    \n",
    "#                     print(f\"   ‚úÖ Deleted: {filename}\")\n",
    "#                     deleted += 1\n",
    "#                     time.sleep(0.1)\n",
    "                    \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"   ‚ùå Failed: {filename} - {str(e)[:50]}\")\n",
    "#                     failed += 1\n",
    "            \n",
    "#             print(f\"\\nüìä Delete Summary:\")\n",
    "#             print(f\"   Successful: {deleted}/{len(pdf_files)}\")\n",
    "#             print(f\"   Failed: {failed}/{len(pdf_files)}\")\n",
    "#         else:\n",
    "#             print(\"   ‚ÑπÔ∏è  No PDF files found in stage root\")\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"   ‚ùå Error listing stage: {str(e)}\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # Final Summary\n",
    "# # ============================================================================\n",
    "# if (TRUNCATE_TABLES or DELETE_STAGE_FILES) and not SHOW_STATUS_ONLY:\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"‚úÖ Cleanup operations complete!\")\n",
    "#     print(\"\\nüí° Next steps:\")\n",
    "#     print(\"   1. Upload new PDFs to stage\")\n",
    "#     print(\"   2. Run the processing pipeline\")\n",
    "#     print(\"   3. Set flags back to False to prevent accidental deletion\")\n",
    "#     print(\"\\n‚ö†Ô∏è  Remember to set TRUNCATE_TABLES and DELETE_STAGE_FILES back to False!\")\n",
    "#     print(\"=\" * 80)"
   ]
  }
 ]
}