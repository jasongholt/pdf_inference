{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ GWAS Intelligence Pipeline - Standalone Notebook\n",
    "\n",
    "This notebook is a **complete, standalone** pipeline for extracting genomic trait data from research papers using Snowflake Cortex AI and multimodal RAG.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Database Setup** - Creates GWAS database, schemas, stages, and tables\n",
    "2. **PDF Processing** - Parses PDFs using Cortex AI\n",
    "3. **Embedding Generation** - Creates text and image embeddings\n",
    "4. **Trait Extraction** - Extracts GWAS traits using multimodal RAG\n",
    "5. **Analytics** - Provides extracted trait analytics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Snowflake account with Cortex AI access\n",
    "- CREATE DATABASE privileges\n",
    "- Warehouse for compute\n",
    "- `.env` file with credentials (see below)\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Configure `.env` file with your Snowflake credentials\n",
    "2. Upload a PDF to the stage (instructions in notebook)\n",
    "3. Run all cells in order\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# IMPORTS - Standalone Notebook Version\n",
    "# ============================================================================\n",
    "# This notebook is self-contained - all logic is inline, no external modules!\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF - for PDF to PNG conversion\n",
    "\n",
    "# Snowflake imports\n",
    "from snowflake.snowpark import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Project root\n",
    "project_root = Path().absolute()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"\\nüì¶ Key packages loaded:\")\n",
    "print(f\"   ‚Ä¢ snowflake.snowpark\")\n",
    "print(f\"   ‚Ä¢ pandas {pd.__version__}\")\n",
    "print(f\"   ‚Ä¢ numpy {np.__version__}\")\n",
    "print(f\"   ‚Ä¢ PyMuPDF (fitz) {fitz.__version__}\")\n",
    "print(f\"\\nüéØ This is a standalone notebook - no external files required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "   Warehouse: DEMO_JGH\n",
      "   Database: GWAS\n",
      "   Schemas: PDF_RAW, PDF_PROCESSING\n",
      "\n",
      "‚úÖ Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD ENVIRONMENT VARIABLES (for local development only)\n",
    "# ============================================================================\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Detect if we're running in Snowflake Notebooks\n",
    "try:\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    _ = get_active_session()\n",
    "    IN_SNOWFLAKE = True\n",
    "    print(\"üèîÔ∏è Running in Snowflake Notebook (Container Runtime)\")\n",
    "    print(\"   Environment variables managed by Snowflake\")\n",
    "except:\n",
    "    IN_SNOWFLAKE = False\n",
    "    print(\"üíª Running locally - loading credentials from .env\")\n",
    "    \n",
    "    # Get the directory where this notebook is located\n",
    "    notebook_dir = Path().absolute()\n",
    "    \n",
    "    # Load .env file from the notebook's directory\n",
    "    env_path = notebook_dir / '.env'\n",
    "    \n",
    "    if env_path.exists():\n",
    "        load_dotenv(dotenv_path=env_path)\n",
    "        print(f\"‚úÖ Environment variables loaded from: {env_path}\")\n",
    "        print(f\"\\nüìã Credentials Status:\")\n",
    "        print(f\"   SNOWFLAKE_ACCOUNT: {'‚úì ' + os.environ.get('SNOWFLAKE_ACCOUNT', '') if os.environ.get('SNOWFLAKE_ACCOUNT') else '‚úó Missing'}\")\n",
    "        print(f\"   SNOWFLAKE_USER: {'‚úì ' + os.environ.get('SNOWFLAKE_USER', '') if os.environ.get('SNOWFLAKE_USER') else '‚úó Missing'}\")\n",
    "        print(f\"   SNOWFLAKE_PASSWORD: {'‚úì Set' if os.environ.get('SNOWFLAKE_PASSWORD') else '‚úó Missing'}\")\n",
    "        print(f\"   SNOWFLAKE_WAREHOUSE: {os.environ.get('SNOWFLAKE_WAREHOUSE', f'Not set (will use {WAREHOUSE_NAME})')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå .env file not found at: {env_path}\")\n",
    "        print(f\"\\nüí° Create a .env file with:\")\n",
    "        print(\"   SNOWFLAKE_ACCOUNT=your-account\")\n",
    "        print(\"   SNOWFLAKE_USER=your-username\")\n",
    "        print(\"   SNOWFLAKE_PASSWORD=your-password\")\n",
    "        print(\"   SNOWFLAKE_WAREHOUSE=your-warehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 1: Database & Schema Setup\n",
    "\n",
    "Create the GWAS database and required schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connected to Snowflake\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONNECT TO SNOWFLAKE (works in both local and Snowflake Notebooks)\n",
    "# ============================================================================\n",
    "from snowflake.snowpark import Session\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # ========================================================================\n",
    "    # METHOD 1: Try to use active session (Snowflake Notebooks / Container Runtime)\n",
    "    # ========================================================================\n",
    "    from snowflake.snowpark.context import get_active_session\n",
    "    session = get_active_session()\n",
    "    \n",
    "    print(\"‚úÖ Connected to Snowflake using active session\")\n",
    "    print(\"   üèîÔ∏è Running in Snowflake Notebook (Container Runtime)\")\n",
    "    print(f\"   Account: {session.get_current_account()}\")\n",
    "    print(f\"   User: {session.get_current_user()}\")\n",
    "    print(f\"   Role: {session.get_current_role()}\")\n",
    "    print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "    print(f\"   Database: {session.get_current_database() or '(not set)'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # ========================================================================\n",
    "    # METHOD 2: Use credentials from environment (local development)\n",
    "    # ========================================================================\n",
    "    print(\"üíª Running locally - connecting with credentials from .env\")\n",
    "    \n",
    "    # Get connection from environment or use defaults\n",
    "    session = Session.builder.configs({\n",
    "        \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\", \"\"),\n",
    "        \"user\": os.environ.get(\"SNOWFLAKE_USER\", \"\"),\n",
    "        \"password\": os.environ.get(\"SNOWFLAKE_PASSWORD\", \"\"),\n",
    "        \"role\": os.environ.get(\"SNOWFLAKE_ROLE\", \"ACCOUNTADMIN\"),\n",
    "        \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\", WAREHOUSE_NAME),\n",
    "    }).create()\n",
    "    \n",
    "    print(\"‚úÖ Connected to Snowflake using credentials\")\n",
    "    print(f\"   Account: {session.get_current_account()}\")\n",
    "    print(f\"   User: {session.get_current_user()}\")\n",
    "    print(f\"   Role: {session.get_current_role()}\")\n",
    "    print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
    "\n",
    "print(\"\\nüîå Snowflake session ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database GWAS created/verified\n",
      "‚úÖ Schema PDF_RAW created/verified\n",
      "‚úÖ Schema PDF_PROCESSING created/verified\n",
      "\n",
      "üìä Available schemas in GWAS:\n",
      "   - INFORMATION_SCHEMA\n",
      "   - PDF_PROCESSING\n",
      "   - PDF_RAW\n",
      "   - PUBLIC\n",
      "\n",
      "‚úÖ Database and schemas ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create database\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\").collect()\n",
    "print(f\"‚úÖ Database {DATABASE_NAME} created/verified\")\n",
    "\n",
    "# Use database\n",
    "session.sql(f\"USE DATABASE {DATABASE_NAME}\").collect()\n",
    "\n",
    "# Create schemas\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {SCHEMA_RAW}\n",
    "    COMMENT = 'Raw PDF data from AI_PARSE_DOCUMENT'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Schema {SCHEMA_RAW} created/verified\")\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {SCHEMA_PROCESSING}\n",
    "    COMMENT = 'Processed PDF data, embeddings, and analytics'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Schema {SCHEMA_PROCESSING} created/verified\")\n",
    "\n",
    "# Verify schemas exist\n",
    "schemas = session.sql(\"SHOW SCHEMAS\").collect()\n",
    "print(f\"\\nüìä Available schemas in {DATABASE_NAME}:\")\n",
    "for schema in schemas:\n",
    "    print(f\"   - {schema['name']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Database and schemas ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Create Stage\n",
    "\n",
    "Create stage for storing PDF files, extracted images, and text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage PDF_STAGE created/verified in GWAS.PDF_RAW\n",
      "\n",
      "üì¶ Available stages:\n",
      "   - PDF_STAGE\n",
      "\n",
      "üí° Upload PDFs using:\n",
      "   PUT file:///path/to/file.pdf @GWAS.PDF_RAW.PDF_STAGE/\n",
      "\n",
      "‚úÖ Stage ready!\n"
     ]
    }
   ],
   "source": [
    "# Create stage for PDF and asset storage\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS PDF_STAGE\n",
    "    DIRECTORY = (ENABLE = TRUE)\n",
    "    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n",
    "    COMMENT = 'Storage for PDF files, extracted images, and text'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Stage PDF_STAGE created/verified in {DATABASE_NAME}.{SCHEMA_RAW}\")\n",
    "\n",
    "# Verify stage exists\n",
    "stages = session.sql(\"SHOW STAGES\").collect()\n",
    "print(f\"\\nüì¶ Available stages:\")\n",
    "for stage in stages:\n",
    "    print(f\"   - {stage['name']}\")\n",
    "\n",
    "print(f\"\\nüí° Upload PDFs using:\")\n",
    "print(f\"   PUT file:///path/to/file.pdf @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\")\n",
    "\n",
    "print(\"\\n‚úÖ Stage ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Create Tables\n",
    "\n",
    "Create all tables needed for the GWAS extraction pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table PARSED_DOCUMENTS created in GWAS.PDF_RAW\n"
     ]
    }
   ],
   "source": [
    "# Create PARSED_DOCUMENTS table in PDF_RAW schema\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS PARSED_DOCUMENTS (\n",
    "        document_id VARCHAR PRIMARY KEY,\n",
    "        file_path VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        parsed_content VARIANT NOT NULL,\n",
    "        total_pages INTEGER,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "    COMMENT = 'Raw PDF data from Cortex AI_PARSE_DOCUMENT'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table PARSED_DOCUMENTS created in {DATABASE_NAME}.{SCHEMA_RAW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table TEXT_PAGES created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create TEXT_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_PROCESSING}\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS TEXT_PAGES (\n",
    "        page_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        page_text TEXT,\n",
    "        word_count INTEGER,\n",
    "        text_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Page text with embeddings for semantic search'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table TEXT_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table IMAGE_PAGES created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create IMAGE_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS IMAGE_PAGES (\n",
    "        image_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        image_file_path VARCHAR NOT NULL,\n",
    "        image_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        dpi INTEGER DEFAULT 300,\n",
    "        image_format VARCHAR(10) DEFAULT 'PNG',\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Page images metadata for multimodal processing'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table IMAGE_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table MULTIMODAL_PAGES created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create MULTIMODAL_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS MULTIMODAL_PAGES (\n",
    "        page_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        image_id VARCHAR,\n",
    "        page_text TEXT,\n",
    "        image_path VARCHAR,\n",
    "        text_embedding VECTOR(FLOAT, 1024),\n",
    "        image_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        has_text BOOLEAN DEFAULT FALSE,\n",
    "        has_image BOOLEAN DEFAULT FALSE,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Combined text + image embeddings for multimodal RAG'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table MULTIMODAL_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table GWAS_TRAIT_ANALYTICS created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create GWAS_TRAIT_ANALYTICS table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS GWAS_TRAIT_ANALYTICS (\n",
    "        analytics_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        extraction_version VARCHAR(50),\n",
    "        finding_number INTEGER DEFAULT 1,\n",
    "        \n",
    "        -- Genomic traits\n",
    "        trait VARCHAR(500),\n",
    "        germplasm_name VARCHAR(500),\n",
    "        genome_version VARCHAR(100),\n",
    "        chromosome VARCHAR(50),\n",
    "        physical_position VARCHAR(200),\n",
    "        gene VARCHAR(500),\n",
    "        snp_name VARCHAR(200),\n",
    "        variant_id VARCHAR(200),\n",
    "        variant_type VARCHAR(100),\n",
    "        effect_size VARCHAR(200),\n",
    "        gwas_model VARCHAR(200),\n",
    "        evidence_type VARCHAR(100),\n",
    "        allele VARCHAR(100),\n",
    "        annotation TEXT,\n",
    "        candidate_region VARCHAR(500),\n",
    "        \n",
    "        -- Metadata\n",
    "        extraction_source VARCHAR(50),\n",
    "        field_citations VARIANT,\n",
    "        field_confidence VARIANT,\n",
    "        field_raw_values VARIANT,\n",
    "        traits_extracted INTEGER,\n",
    "        traits_not_reported INTEGER,\n",
    "        extraction_accuracy_pct FLOAT,\n",
    "        \n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, extraction_version, finding_number)\n",
    "    )\n",
    "    COMMENT = 'Extracted GWAS trait data from research papers'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table GWAS_TRAIT_ANALYTICS created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 4: Upload PDF to Stage\n",
    "\n",
    "**Upload your PDF file to the stage before proceeding.**\n",
    "\n",
    "### Option 1: Using SnowSQL (Command Line)\n",
    "```bash\n",
    "# From terminal\n",
    "snowsql -a YOUR_ACCOUNT -u YOUR_USER\n",
    "PUT file:///Users/jholt/Downloads/fpls-15-1373081.pdf @GWAS.PDF_RAW.PDF_STAGE/;\n",
    "```\n",
    "\n",
    "### Option 2: Using Python (Below)\n",
    "Run the cell below to upload from your local system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Found PDF: fpls-15-1373081.pdf (2.50 MB)\n",
      "\n",
      "üì§ Uploading to stage...\n",
      "‚úÖ PDF uploaded to @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf\n",
      "\n",
      "üìÇ Files in stage:\n",
      "   - pdf_stage/fpls-15-1373081.pdf\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0000.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0001.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0002.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0003.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0004.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0005.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0006.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0007.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0008.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0009.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0010.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0011.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0012.png\n",
      "   - pdf_stage/fpls-15-1373081.pdf/pages_images/page_0013.png\n"
     ]
    }
   ],
   "source": [
    "# Upload PDF from local system\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your PDF file\n",
    "PDF_LOCAL_PATH = \"/Users/jholt/Downloads/fpls-15-1373081.pdf\"\n",
    "\n",
    "# Verify file exists\n",
    "pdf_path = Path(PDF_LOCAL_PATH)\n",
    "if not pdf_path.exists():\n",
    "    print(f\"‚ùå File not found: {PDF_LOCAL_PATH}\")\n",
    "    print(\"   Update PDF_LOCAL_PATH to point to your PDF file\")\n",
    "else:\n",
    "    print(f\"üìÑ Found PDF: {pdf_path.name} ({pdf_path.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "    \n",
    "    # Upload to stage\n",
    "    print(f\"\\nüì§ Uploading to stage...\")\n",
    "    session.file.put(\n",
    "        str(pdf_path),\n",
    "        f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\",\n",
    "        auto_compress=False,\n",
    "        overwrite=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ PDF uploaded to @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{pdf_path.name}\")\n",
    "    \n",
    "    # List files in stage to verify\n",
    "    print(f\"\\nüìÇ Files in stage:\")\n",
    "    files = session.sql(f\"LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\").collect()\n",
    "    for file in files:\n",
    "        print(f\"   - {file[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ CELL 1: Section 1 - Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "   Project root: /Users/jholt/gwas_intelligence/gwas_intelligence\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add scripts directory to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.append(str(project_root / \"scripts\" / \"python\"))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"   Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ CELL 5-6: Section 3 - List PDFs in Snowflake Stage\n",
    "\n",
    "- **Cell 5**: List available PDFs\n",
    "- **Cell 6**: Configure which PDF to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Selected PDF Configuration:\n",
      "   Filename: fpls-15-1373081.pdf\n",
      "   Document ID: fpls-15-1373081.pdf\n",
      "   Stage File Path: fpls-15-1373081.pdf\n",
      "   Full Stage Path: @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf\n",
      "\n",
      "üìÅ Output Structure:\n",
      "   Text:   @PDF_STAGE/fpls-15-1373081.pdf/pages_text/\n",
      "   Images: @PDF_STAGE/fpls-15-1373081.pdf/pages_images/\n",
      "\n",
      "‚úÖ Configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Update these for your PDF\n",
    "# ============================================================================\n",
    "\n",
    "# Test PDF: fpls-15-1373081.pdf (GWAS paper from Frontiers in Plant Science)\n",
    "# PDF is uploaded to root of stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{filename}\n",
    "\n",
    "PDF_FILENAME = \"fpls-15-1373081.pdf\"  # PDF filename as it exists in stage\n",
    "\n",
    "# Use filename as DOCUMENT_ID (keeps .pdf extension)\n",
    "DOCUMENT_ID = PDF_FILENAME\n",
    "\n",
    "# Stage paths\n",
    "STAGE_FILE_PATH = PDF_FILENAME  # PDF is at root of stage (no subdirectory)\n",
    "\n",
    "# Expected directory structure that will be created in stage:\n",
    "# @PDF_STAGE/\n",
    "#   ‚îî‚îÄ‚îÄ fpls-15-1373081.pdf/\n",
    "#       ‚îú‚îÄ‚îÄ pages_text/\n",
    "#       ‚îÇ   ‚îú‚îÄ‚îÄ page_001.txt\n",
    "#       ‚îÇ   ‚îú‚îÄ‚îÄ page_002.txt\n",
    "#       ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "#       ‚îî‚îÄ‚îÄ pages_images/\n",
    "#           ‚îú‚îÄ‚îÄ page_001.png\n",
    "#           ‚îú‚îÄ‚îÄ page_002.png\n",
    "#           ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "print(f\"üìã Selected PDF Configuration:\")\n",
    "print(f\"   Filename: {PDF_FILENAME}\")\n",
    "print(f\"   Document ID: {DOCUMENT_ID}\")\n",
    "print(f\"   Stage File Path: {STAGE_FILE_PATH}\")\n",
    "print(f\"   Full Stage Path: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{STAGE_FILE_PATH}\")\n",
    "print(f\"\\nüìÅ Output Structure:\")\n",
    "print(f\"   Text:   @PDF_STAGE/{DOCUMENT_ID}/pages_text/\")\n",
    "print(f\"   Images: @PDF_STAGE/{DOCUMENT_ID}/pages_images/\")\n",
    "print(f\"\\n‚úÖ Configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ CELL 8-9: Section 4 - Parse PDF with AI_PARSE_DOCUMENT\n",
    "\n",
    "- **Cell 8**: Parse PDF using Snowflake Cortex AI\n",
    "- **Cell 9**: Convert PDF pages to PNG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Parsing PDF from stage\n",
      "\n",
      "   Stage: @GWAS.PDF_RAW.PDF_STAGE\n",
      "   File: fpls-15-1373081.pdf\n",
      "\n",
      "‚ÑπÔ∏è  Document 'fpls-15-1373081.pdf' already parsed (skipping)\n",
      "   Parsed at: 2025-10-07 19:21:58.711000-07:00\n",
      "   Total pages: 14\n",
      "\n",
      "üí° To re-parse, delete the record first:\n",
      "   DELETE FROM GWAS.PDF_RAW.PARSED_DOCUMENTS\n",
      "   WHERE document_id = 'fpls-15-1373081.pdf';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse PDF using Snowflake Cortex AI_PARSE_DOCUMENT\n",
    "# Reference: https://docs.snowflake.com/en/user-guide/snowflake-cortex/parse-document\n",
    "import time\n",
    "\n",
    "print(f\"üîÑ Parsing PDF from stage\\n\")\n",
    "print(f\"   Stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\")\n",
    "print(f\"   File: {STAGE_FILE_PATH}\\n\")\n",
    "\n",
    "# Pre-check: Is document already parsed?\n",
    "check_query = f\"\"\"\n",
    "SELECT document_id, total_pages, created_at\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "WHERE document_id = '{DOCUMENT_ID}'\n",
    "\"\"\"\n",
    "\n",
    "existing = session.sql(check_query).collect()\n",
    "\n",
    "if existing:\n",
    "    print(f\"‚ÑπÔ∏è  Document '{DOCUMENT_ID}' already parsed (skipping)\")\n",
    "    print(f\"   Parsed at: {existing[0][2]}\")\n",
    "    print(f\"   Total pages: {existing[0][1]}\")\n",
    "    print(f\"\\nüí° To re-parse, delete the record first:\")\n",
    "    print(f\"   DELETE FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\")\n",
    "    print(f\"   WHERE document_id = '{DOCUMENT_ID}';\\n\")\n",
    "else:\n",
    "    print(\"üìã Using AI_PARSE_DOCUMENT with LAYOUT mode\")\n",
    "    print(\"   - High-fidelity extraction optimized for complex documents\")\n",
    "    print(\"   - Preserves structure: tables, headers, reading order\")\n",
    "    print(\"   - page_split: true (processes each page separately)\")\n",
    "    print(\"   - Returns Markdown-formatted content\")\n",
    "    print(\"\\n‚è±Ô∏è  This may take 30-60+ seconds for a 15-page PDF...\")\n",
    "    print(\"   (AI_PARSE_DOCUMENT is processing your document in Snowflake)\\n\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    parse_query = f\"\"\"\n",
    "    INSERT INTO {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS \n",
    "        (document_id, file_path, file_name, parsed_content, total_pages)\n",
    "    SELECT\n",
    "        '{DOCUMENT_ID}' AS document_id,\n",
    "        '@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{STAGE_FILE_PATH}' AS file_path,\n",
    "        '{PDF_FILENAME}' AS file_name,\n",
    "        parsed_data AS parsed_content,\n",
    "        ARRAY_SIZE(parsed_data:pages) AS total_pages\n",
    "    FROM (\n",
    "        SELECT SNOWFLAKE.CORTEX.AI_PARSE_DOCUMENT(\n",
    "            TO_FILE('@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE', '{STAGE_FILE_PATH}'),\n",
    "            {{'mode': 'LAYOUT', 'page_split': true}}\n",
    "        ) AS parsed_data\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Calling AI_PARSE_DOCUMENT... (please wait)\")\n",
    "        session.sql(parse_query).collect()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ PDF parsed successfully in {elapsed:.1f} seconds!\\n\")\n",
    "        \n",
    "        # Verify parsing\n",
    "        result = session.sql(check_query).collect()\n",
    "        if result:\n",
    "            print(f\"üìÑ Parsed Document Info:\")\n",
    "            print(f\"   Document ID: {result[0][0]}\")\n",
    "            print(f\"   Page Count: {result[0][1]}\")\n",
    "            print(f\"   Created: {result[0][2]}\")\n",
    "            \n",
    "            # Get first page preview\n",
    "            preview_query = f\"\"\"\n",
    "            SELECT parsed_content:pages[0]:content::VARCHAR as first_page\n",
    "            FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "            WHERE document_id = '{DOCUMENT_ID}'\n",
    "            \"\"\"\n",
    "            preview = session.sql(preview_query).collect()\n",
    "            if preview and preview[0][0]:\n",
    "                print(f\"\\n   First Page Preview (100 chars):\")\n",
    "                print(f\"   {preview[0][0][:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n‚ùå Error after {elapsed:.1f} seconds: {error_msg[:300]}\\n\")\n",
    "        \n",
    "        # Helpful debugging\n",
    "        if \"does not exist\" in error_msg.lower():\n",
    "            print(\"üí° File not found in stage. Check:\")\n",
    "            print(f\"   LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE;\")\n",
    "        elif \"timeout\" in error_msg.lower():\n",
    "            print(\"üí° Query timeout. Try:\")\n",
    "            print(\"   - Smaller PDF\")\n",
    "            print(\"   - Increase statement timeout\")\n",
    "        else:\n",
    "            print(\"üí° Full error:\")\n",
    "            print(f\"   {error_msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è  Creating PNG images from PDF pages\n",
      "\n",
      "   PDF: fpls-15-1373081.pdf\n",
      "   Document ID: fpls-15-1373081.pdf\n",
      "\n",
      "üì• Step 1: Downloading PDF from stage...\n",
      "   ‚úÖ Downloaded: fpls-15-1373081.pdf\n",
      "\n",
      "üîÑ Step 2: Converting PDF pages to PNG images...\n",
      "   PDF has 14 pages\n",
      "   ‚úì Converted page 1/14\n",
      "   ‚úì Converted page 2/14\n",
      "   ‚úì Converted page 3/14\n",
      "   ‚úì Converted page 4/14\n",
      "   ‚úì Converted page 5/14\n",
      "   ‚úì Converted page 6/14\n",
      "   ‚úì Converted page 7/14\n",
      "   ‚úì Converted page 8/14\n",
      "   ‚úì Converted page 9/14\n",
      "   ‚úì Converted page 10/14\n",
      "   ‚úì Converted page 11/14\n",
      "   ‚úì Converted page 12/14\n",
      "   ‚úì Converted page 13/14\n",
      "   ‚úì Converted page 14/14\n",
      "   ‚úÖ Created 14 PNG images\n",
      "\n",
      "üì§ Step 3: Uploading PNG images to stage...\n",
      "   Target: @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf/pages_images/\n",
      "   ‚úì Uploaded page 1/14\n",
      "   ‚úì Uploaded page 2/14\n",
      "   ‚úì Uploaded page 3/14\n",
      "   ‚úì Uploaded page 4/14\n",
      "   ‚úì Uploaded page 5/14\n",
      "   ‚úì Uploaded page 6/14\n",
      "   ‚úì Uploaded page 7/14\n",
      "   ‚úì Uploaded page 8/14\n",
      "   ‚úì Uploaded page 9/14\n",
      "   ‚úì Uploaded page 10/14\n",
      "   ‚úì Uploaded page 11/14\n",
      "   ‚úì Uploaded page 12/14\n",
      "   ‚úì Uploaded page 13/14\n",
      "   ‚úì Uploaded page 14/14\n",
      "   ‚úÖ Uploaded 14 images\n",
      "\n",
      "üíæ Step 4: Inserting IMAGE_PAGES records...\n",
      "   ‚úì Inserted record 1/14\n",
      "   ‚úì Inserted record 2/14\n",
      "   ‚úì Inserted record 3/14\n",
      "   ‚úì Inserted record 4/14\n",
      "   ‚úì Inserted record 5/14\n",
      "   ‚úì Inserted record 6/14\n",
      "   ‚úì Inserted record 7/14\n",
      "   ‚úì Inserted record 8/14\n",
      "   ‚úì Inserted record 9/14\n",
      "   ‚úì Inserted record 10/14\n",
      "   ‚úì Inserted record 11/14\n",
      "   ‚úì Inserted record 12/14\n",
      "   ‚úì Inserted record 13/14\n",
      "   ‚úì Inserted record 14/14\n",
      "   ‚úÖ Inserted 14 IMAGE_PAGES records\n",
      "\n",
      "üîç Step 5: Verifying stage structure...\n",
      "   ‚úÖ Found 14 files in stage\n",
      "   ‚úÖ Found 14 records in IMAGE_PAGES table\n",
      "\n",
      "üéâ SUCCESS! Converted 14 pages for fpls-15-1373081.pdf\n",
      "   Stage: @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf/pages_images/\n",
      "   Database: GWAS.PDF_PROCESSING.IMAGE_PAGES\n",
      "\n",
      "üßπ Cleaned up temp directory\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE PNG IMAGES FROM PDF\n",
    "# Uses PyMuPDF to convert PDF pages to PNG, uploads to stage structure\n",
    "# NOTE: PyMuPDF requires local file access - we download, process, upload\n",
    "# ============================================================================\n",
    "\n",
    "# Required imports (in case Cell 1 wasn't run)\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: PyMuPDF (fitz) not installed!\")\n",
    "    print(\"   Install: pip install PyMuPDF\")\n",
    "    raise\n",
    "\n",
    "print(\"üñºÔ∏è  Creating PNG images from PDF pages\\n\")\n",
    "print(f\"   PDF: {STAGE_FILE_PATH}\")\n",
    "print(f\"   Document ID: {DOCUMENT_ID}\\n\")\n",
    "\n",
    "# Create temp directories\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "images_output = temp_dir / \"images\"\n",
    "images_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Step 1: Download PDF from stage (required for PyMuPDF)\n",
    "    print(\"üì• Step 1: Downloading PDF from stage...\")\n",
    "    \n",
    "    # Set database context for file operations\n",
    "    session.sql(f\"USE DATABASE {DATABASE_NAME}\").collect()\n",
    "    session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "    \n",
    "    # Download from stage\n",
    "    stage_path = f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{STAGE_FILE_PATH}\"\n",
    "    session.file.get(stage_path, str(temp_dir))\n",
    "    \n",
    "    # Find downloaded PDF\n",
    "    pdf_files = list(temp_dir.rglob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"PDF not downloaded: {PDF_FILENAME}\")\n",
    "    \n",
    "    local_pdf = pdf_files[0]\n",
    "    print(f\"   ‚úÖ Downloaded: {local_pdf.name}\\n\")\n",
    "    \n",
    "    # Step 2: Convert PDF pages to PNG using PyMuPDF\n",
    "    print(\"üîÑ Step 2: Converting PDF pages to PNG images...\")\n",
    "    doc = fitz.open(local_pdf)\n",
    "    page_count = len(doc)\n",
    "    print(f\"   PDF has {page_count} pages\")\n",
    "    \n",
    "    for page_num in range(page_count):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        \n",
    "        output_file = images_output / f\"page_{page_num:04d}.png\"\n",
    "        pix.save(output_file)\n",
    "        print(f\"   ‚úì Converted page {page_num + 1}/{page_count}\")\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"   ‚úÖ Created {page_count} PNG images\\n\")\n",
    "    \n",
    "    # Step 3: Upload PNGs to stage structure\n",
    "    print(\"üì§ Step 3: Uploading PNG images to stage...\")\n",
    "    stage_output = f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\"\n",
    "    print(f\"   Target: {stage_output}\")\n",
    "    \n",
    "    for page_num in range(page_count):\n",
    "        local_image = images_output / f\"page_{page_num:04d}.png\"\n",
    "        session.file.put(\n",
    "            str(local_image),\n",
    "            stage_output,\n",
    "            auto_compress=False,\n",
    "            overwrite=True\n",
    "        )\n",
    "        print(f\"   ‚úì Uploaded page {page_num + 1}/{page_count}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Uploaded {page_count} images\\n\")\n",
    "    \n",
    "    # Step 4: Insert IMAGE_PAGES records into database\n",
    "    print(\"üíæ Step 4: Inserting IMAGE_PAGES records...\")\n",
    "    session.sql(f\"USE SCHEMA {SCHEMA_PROCESSING}\").collect()\n",
    "    \n",
    "    for page_num in range(page_count):\n",
    "        session.sql(f\"\"\"\n",
    "            INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES (\n",
    "                IMAGE_ID,\n",
    "                DOCUMENT_ID,\n",
    "                FILE_NAME,\n",
    "                PAGE_NUMBER,\n",
    "                IMAGE_FILE_PATH,\n",
    "                DPI,\n",
    "                IMAGE_FORMAT\n",
    "            )\n",
    "            SELECT\n",
    "                UUID_STRING(),\n",
    "                '{DOCUMENT_ID}',\n",
    "                '{PDF_FILENAME}',\n",
    "                {page_num},\n",
    "                '{stage_output}page_{page_num:04d}.png',\n",
    "                300,\n",
    "                'PNG'\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "                WHERE DOCUMENT_ID = '{DOCUMENT_ID}' \n",
    "                AND PAGE_NUMBER = {page_num}\n",
    "            )\n",
    "        \"\"\").collect()\n",
    "        print(f\"   ‚úì Inserted record {page_num + 1}/{page_count}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Inserted {page_count} IMAGE_PAGES records\\n\")\n",
    "    \n",
    "    # Step 5: Verify\n",
    "    print(\"üîç Step 5: Verifying stage structure...\")\n",
    "    verify_result = session.sql(f\"\"\"\n",
    "        LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\n",
    "    \"\"\").collect()\n",
    "    print(f\"   ‚úÖ Found {len(verify_result)} files in stage\")\n",
    "    \n",
    "    # Verify database\n",
    "    db_count = session.sql(f\"\"\"\n",
    "        SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "        WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "    \"\"\").collect()\n",
    "    print(f\"   ‚úÖ Found {db_count[0][0]} records in IMAGE_PAGES table\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESS! Converted {page_count} pages for {PDF_FILENAME}\")\n",
    "    print(f\"   Stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\")\n",
    "    print(f\"   Database: {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # Cleanup temp directory\n",
    "    if 'temp_dir' in locals() and temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"\\nüßπ Cleaned up temp directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù CELL 11: Section 5 - Extract Text Pages & Generate Embeddings\n",
    "\n",
    "Uses `snowflake-arctic-embed-l-v2.0-8k` model for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting text pages and generating embeddings...\n",
      "\n",
      "üìã Text Embedding Model: snowflake-arctic-embed-l-v2.0-8k\n",
      "   - Dimensions: 1024\n",
      "   - Context length: 8K tokens\n",
      "   - Optimized for: Long-form documents\n",
      "\n",
      "‚úÖ Text pages extracted with embeddings!\n",
      "\n",
      "üìä Text Extraction Statistics:\n",
      "   Total pages: 14\n",
      "   Avg words/page: 678\n",
      "   Min words: 237\n",
      "   Max words: 1238\n",
      "   Pages with embeddings: 14\n",
      "\n",
      "üìÑ Sample Pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Text Preview</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWED BY\\nLilin...</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and feeds over half...</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>materials can be found in Supplementary Table 1. The majority of them were indica (227), followe...</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page  \\\n",
       "0     0   \n",
       "1     1   \n",
       "2     2   \n",
       "\n",
       "                                                                                          Text Preview  \\\n",
       "0  ## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWED BY\\nLilin...   \n",
       "1  Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and feeds over half...   \n",
       "2  materials can be found in Supplementary Table 1. The majority of them were indica (227), followe...   \n",
       "\n",
       "   Words  \n",
       "0    556  \n",
       "1   1002  \n",
       "2    826  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract text pages with embeddings using snowflake-arctic-embed-l-v2.0-8k\n",
    "print(\"üîÑ Extracting text pages and generating embeddings...\\n\")\n",
    "print(\"üìã Text Embedding Model: snowflake-arctic-embed-l-v2.0-8k\")\n",
    "print(\"   - Dimensions: 1024\")\n",
    "print(\"   - Context length: 8K tokens\")\n",
    "print(\"   - Optimized for: Long-form documents\\n\")\n",
    "\n",
    "# Insert text pages with embeddings\n",
    "text_extract_query = f\"\"\"\n",
    "INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES \n",
    "    (document_id, file_name, page_number, page_text, word_count, \n",
    "     text_embedding, embedding_model)\n",
    "SELECT\n",
    "    '{DOCUMENT_ID}' AS document_id,\n",
    "    '{PDF_FILENAME}' AS file_name,\n",
    "    page.index AS page_number,\n",
    "    page.value:content::STRING AS page_text,\n",
    "    ARRAY_SIZE(SPLIT(page.value:content::STRING, ' ')) AS word_count,\n",
    "    SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0-8k',\n",
    "        page.value:content::STRING\n",
    "    ) AS text_embedding,\n",
    "    'snowflake-arctic-embed-l-v2.0-8k' AS embedding_model\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS pd,\n",
    "LATERAL FLATTEN(input => pd.parsed_content:pages) page\n",
    "WHERE pd.document_id = '{DOCUMENT_ID}'\n",
    "AND NOT EXISTS (\n",
    "    SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES tp\n",
    "    WHERE tp.document_id = '{DOCUMENT_ID}' \n",
    "    AND tp.page_number = page.index\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(text_extract_query).collect()\n",
    "    print(\"‚úÖ Text pages extracted with embeddings!\\n\")\n",
    "    \n",
    "    # Get statistics\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as page_count,\n",
    "        AVG(word_count) as avg_words,\n",
    "        MIN(word_count) as min_words,\n",
    "        MAX(word_count) as max_words\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = session.sql(stats_query).collect()\n",
    "    if stats and stats[0][0] > 0:\n",
    "        print(f\"üìä Text Extraction Statistics:\")\n",
    "        print(f\"   Total pages: {stats[0][0]}\")\n",
    "        print(f\"   Avg words/page: {stats[0][1]:.0f}\")\n",
    "        print(f\"   Min words: {stats[0][2]}\")\n",
    "        print(f\"   Max words: {stats[0][3]}\")\n",
    "        \n",
    "        # Verify embeddings\n",
    "        embed_check = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES \n",
    "            WHERE document_id = '{DOCUMENT_ID}' AND text_embedding IS NOT NULL\n",
    "        \"\"\").collect()\n",
    "        print(f\"   Pages with embeddings: {embed_check[0][0]}\")\n",
    "        \n",
    "        # Show sample pages\n",
    "        sample_query = f\"\"\"\n",
    "        SELECT page_number, LEFT(page_text, 100) as preview, word_count\n",
    "        FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "        WHERE document_id = '{DOCUMENT_ID}'\n",
    "        ORDER BY page_number\n",
    "        LIMIT 3\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = session.sql(sample_query).collect()\n",
    "        if samples:\n",
    "            print(f\"\\nüìÑ Sample Pages:\")\n",
    "            df_samples = pd.DataFrame(samples, columns=['Page', 'Text Preview', 'Words'])\n",
    "            display(df_samples)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è CELL 13-14: Section 6 - Create Image Pages\n",
    "\n",
    "- **Cell 13**: Debug - List files in stage\n",
    "- **Cell 14**: Generate image embeddings using `voyage-multimodal-3`\n",
    "\n",
    "**Purpose:** Create embeddings for PNG images to enable multimodal search (text + images).\n",
    "Images capture tables, charts, and figures that may contain GWAS data not easily extracted from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a18b84eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Listing files in stage...\n",
      "\n",
      "‚úÖ Found 14 files:\n",
      "\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0000.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0001.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0002.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0003.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0004.png\n",
      "   ... and 9 more\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: List actual files in stage to verify paths\n",
    "print(\"üîç Listing files in stage...\\n\")\n",
    "\n",
    "list_query = f\"\"\"\n",
    "LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    files = session.sql(list_query).collect()\n",
    "    print(f\"‚úÖ Found {len(files)} files:\\n\")\n",
    "    for f in files[:5]:  # Show first 5\n",
    "        print(f\"   {f[0]}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"   ... and {len(files) - 5} more\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating image embeddings...\n",
      "\n",
      "üìã Image Embedding Model: voyage-multimodal-3 via AI_EMBED\n",
      "   - Dimensions: 1024\n",
      "   - Supports: Images + Text\n",
      "   - Use case: Visual understanding of tables, charts, figures\n",
      "\n",
      "‚ÑπÔ∏è  No records found without embeddings\n",
      "   ‚úÖ 14 records already have embeddings!\n",
      "\n",
      "üìä Final Status:\n",
      "   Total records: 14\n",
      "   ‚úÖ With embeddings: 14\n",
      "   ‚ö†Ô∏è  Without embeddings: 0\n",
      "   üìà Ready for multimodal search: 14/14\n",
      "\n",
      "üéâ All image embeddings generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate image embeddings for existing IMAGE_PAGES records\n",
    "# Uses voyage-multimodal-3 to create embeddings from PNGs in stage\n",
    "print(\"üîÑ Generating image embeddings...\\n\")\n",
    "print(\"üìã Image Embedding Model: voyage-multimodal-3 via AI_EMBED\")\n",
    "print(\"   - Dimensions: 1024\")\n",
    "print(\"   - Supports: Images + Text\")\n",
    "print(\"   - Use case: Visual understanding of tables, charts, figures\\n\")\n",
    "\n",
    "try:\n",
    "    # Get existing IMAGE_PAGES records without embeddings\n",
    "    check_query = f\"\"\"\n",
    "    SELECT \n",
    "        PAGE_NUMBER,\n",
    "        IMAGE_FILE_PATH,\n",
    "        COUNT(*) OVER() as total_records\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "    AND IMAGE_EMBEDDING IS NULL\n",
    "    ORDER BY PAGE_NUMBER\n",
    "    \"\"\"\n",
    "    \n",
    "    records = session.sql(check_query).collect()\n",
    "    \n",
    "    if not records:\n",
    "        print(\"‚ÑπÔ∏è  No records found without embeddings\")\n",
    "        \n",
    "        # Check if embeddings already exist\n",
    "        existing = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "            WHERE DOCUMENT_ID = '{DOCUMENT_ID}' AND IMAGE_EMBEDDING IS NOT NULL\n",
    "        \"\"\").collect()\n",
    "        if existing and existing[0][0] > 0:\n",
    "            print(f\"   ‚úÖ {existing[0][0]} records already have embeddings!\\n\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  No IMAGE_PAGES records found - run Cell 9 first\\n\")\n",
    "    else:\n",
    "        total_records = records[0][2]\n",
    "        print(f\"üìä Found {total_records} IMAGE_PAGES records without embeddings\")\n",
    "        print(f\"   Processing {len(records)} pages...\\n\")\n",
    "        \n",
    "        # Update each record with embedding\n",
    "        for idx, record in enumerate(records, 1):\n",
    "            page_num = record[0]\n",
    "            image_path = record[1]\n",
    "            \n",
    "            # Parse the stored path\n",
    "            # Stored format: @PDF_STAGE/fpls-15-1373081.pdf/pages_images/page_0000.png\n",
    "            # Extract relative path (everything after first /)\n",
    "            \n",
    "            if image_path.startswith('@'):\n",
    "                # Split on first / after @\n",
    "                parts = image_path.split('/', 1)\n",
    "                if len(parts) == 2:\n",
    "                    relative_path = parts[1]  # fpls-15-1373081.pdf/pages_images/page_0000.png\n",
    "                else:\n",
    "                    relative_path = image_path\n",
    "            else:\n",
    "                # No @ prefix, use as-is\n",
    "                relative_path = image_path\n",
    "            \n",
    "            # Always use full stage name for TO_FILE\n",
    "            full_stage_name = f'@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE'\n",
    "            \n",
    "            print(f\"   Page {page_num}: TO_FILE('{full_stage_name}', '{relative_path}')\")\n",
    "            \n",
    "            # Generate embedding and update record\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "            SET \n",
    "                IMAGE_EMBEDDING = AI_EMBED(\n",
    "                    'voyage-multimodal-3',\n",
    "                    TO_FILE('{full_stage_name}', '{relative_path}')\n",
    "                ),\n",
    "                EMBEDDING_MODEL = 'voyage-multimodal-3'\n",
    "            WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "            AND PAGE_NUMBER = {page_num}\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                session.sql(update_query).collect()\n",
    "                print(f\"   ‚úì Generated embedding ({idx}/{len(records)})\\n\")\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                print(f\"   ‚úó Failed: {error_msg[:200]}\\n\")\n",
    "                # Show full error for first failure\n",
    "                if idx == 1:\n",
    "                    print(f\"   Full error: {error_msg}\\n\")\n",
    "                    print(f\"   üí° Tip: Run the debug cell above (Cell 13) to verify files exist in stage\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Embedding generation complete!\\n\")\n",
    "    \n",
    "    # Verify final counts\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(IMAGE_EMBEDDING) as with_embeddings,\n",
    "        COUNT(CASE WHEN IMAGE_EMBEDDING IS NULL THEN 1 END) as without_embeddings\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = session.sql(verify_query).collect()\n",
    "    if result:\n",
    "        total, with_emb, without_emb = result[0]\n",
    "        print(f\"üìä Final Status:\")\n",
    "        print(f\"   Total records: {total}\")\n",
    "        print(f\"   ‚úÖ With embeddings: {with_emb}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Without embeddings: {without_emb}\")\n",
    "        print(f\"   üìà Ready for multimodal search: {with_emb}/{total}\")\n",
    "        \n",
    "        if with_emb == total and total > 0:\n",
    "            print(f\"\\nüéâ All image embeddings generated successfully!\")\n",
    "        elif without_emb > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  {without_emb} pages still need embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó CELL 16: Section 7 - Create Multimodal Pages\n",
    "\n",
    "Join text and image embeddings into a unified multimodal table for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating multimodal pages...\n",
      "\n",
      "üîó Joining text and image data by page_number\n",
      "   - Copies both text and image embeddings\n",
      "   - Enables unified multi-modal search\n",
      "\n",
      "‚úÖ Multimodal pages created!\n",
      "\n",
      "üìä Multimodal Pages Statistics:\n",
      "   Total pages: 14\n",
      "   Pages with text: 14\n",
      "   Pages with images: 14\n",
      "   Text embeddings: 14\n",
      "   Image embeddings: 14\n",
      "\n",
      "üìÑ Sample Pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Text Preview</th>\n",
       "      <th>Has Text</th>\n",
       "      <th>Has Image</th>\n",
       "      <th>Text Emb</th>\n",
       "      <th>Image Emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and f</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>materials can be found in Supplementary Table 1. The majority of them were indic</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Selection of marker subsets for genomic prediction\\n\\nThe genotype data utilized t</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td># Selection of training population subsets for genomic prediction\\n\\nIn order to i</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page  \\\n",
       "0     0   \n",
       "1     1   \n",
       "2     2   \n",
       "3     3   \n",
       "4     4   \n",
       "\n",
       "                                                                            Text Preview  \\\n",
       "0  ## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWE   \n",
       "1     Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and f   \n",
       "2       materials can be found in Supplementary Table 1. The majority of them were indic   \n",
       "3     Selection of marker subsets for genomic prediction\\n\\nThe genotype data utilized t   \n",
       "4     # Selection of training population subsets for genomic prediction\\n\\nIn order to i   \n",
       "\n",
       "   Has Text  Has Image  Text Emb  Image Emb  \n",
       "0      True       True      True       True  \n",
       "1      True       True      True       True  \n",
       "2      True       True      True       True  \n",
       "3      True       True      True       True  \n",
       "4      True       True      True       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create multimodal pages - Join text and image embeddings\n",
    "print(\"üîÑ Creating multimodal pages...\\n\")\n",
    "print(\"üîó Joining text and image data by page_number\")\n",
    "print(\"   - Copies both text and image embeddings\")\n",
    "print(\"   - Enables unified multi-modal search\\n\")\n",
    "\n",
    "multimodal_insert_query = f\"\"\"\n",
    "INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    (document_id, file_name, page_number, page_id, image_id,\n",
    "     page_text, image_path, text_embedding, image_embedding, \n",
    "     has_text, has_image)\n",
    "SELECT\n",
    "    COALESCE(tp.document_id, ip.document_id) AS document_id,\n",
    "    COALESCE(tp.file_name, ip.file_name) AS file_name,\n",
    "    COALESCE(tp.page_number, ip.page_number) AS page_number,\n",
    "    tp.page_id,\n",
    "    ip.image_id,\n",
    "    tp.page_text,\n",
    "    ip.image_file_path AS image_path,\n",
    "    tp.text_embedding,\n",
    "    ip.image_embedding,\n",
    "    tp.page_id IS NOT NULL AS has_text,\n",
    "    ip.image_id IS NOT NULL AS has_image\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES tp\n",
    "FULL OUTER JOIN {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES ip\n",
    "    ON tp.document_id = ip.document_id\n",
    "    AND tp.page_number = ip.page_number\n",
    "WHERE COALESCE(tp.document_id, ip.document_id) = '{DOCUMENT_ID}'\n",
    "AND NOT EXISTS (\n",
    "    SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES mp\n",
    "    WHERE mp.document_id = COALESCE(tp.document_id, ip.document_id)\n",
    "    AND mp.page_number = COALESCE(tp.page_number, ip.page_number)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(multimodal_insert_query).collect()\n",
    "    print(\"‚úÖ Multimodal pages created!\\n\")\n",
    "    \n",
    "    # Get statistics\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN has_text THEN 1 END) as pages_with_text,\n",
    "        COUNT(CASE WHEN has_image THEN 1 END) as pages_with_images,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL THEN 1 END) as text_embeddings,\n",
    "        COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as image_embeddings\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = session.sql(stats_query).collect()\n",
    "    if stats:\n",
    "        print(f\"üìä Multimodal Pages Statistics:\")\n",
    "        print(f\"   Total pages: {stats[0][0]}\")\n",
    "        print(f\"   Pages with text: {stats[0][1]}\")\n",
    "        print(f\"   Pages with images: {stats[0][2]}\")\n",
    "        print(f\"   Text embeddings: {stats[0][3]}\")\n",
    "        print(f\"   Image embeddings: {stats[0][4]}\")\n",
    "    \n",
    "    # Show sample\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        page_number,\n",
    "        LEFT(page_text, 80) as text_preview,\n",
    "        has_text,\n",
    "        has_image,\n",
    "        text_embedding IS NOT NULL as has_text_emb,\n",
    "        image_embedding IS NOT NULL as has_image_emb\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    ORDER BY page_number\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    results = session.sql(query).collect()\n",
    "    if results:\n",
    "        print(f\"\\nüìÑ Sample Pages:\")\n",
    "        df = pd.DataFrame(results, \n",
    "                          columns=['Page', 'Text Preview', 'Has Text', 'Has Image', \n",
    "                                   'Text Emb', 'Image Emb'])\n",
    "        display(df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Section 8: Create Multi-Index Cortex Search Service\n",
    "\n",
    "Create a Cortex Search service that indexes:\n",
    "- **Text content** (keyword search)\n",
    "- **Text embeddings** (semantic search with Arctic-8k)\n",
    "- **Image embeddings** (visual search with voyage-multimodal-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating Cortex Search Service...\n",
      "\n",
      "üìã Service Configuration:\n",
      "   ‚Ä¢ Name: MULTIMODAL_SEARCH_SERVICE\n",
      "   ‚Ä¢ Text Index: page_text (keyword search)\n",
      "   ‚Ä¢ Vector Index 1: text_embedding (1024D - Arctic-8k)\n",
      "   ‚Ä¢ Vector Index 2: image_embedding (1024D - voyage-multimodal-3)\n",
      "   ‚Ä¢ Target Lag: 1 minute\n",
      "\n",
      "‚úÖ Service already exists, skipping creation (will refresh at end)\n",
      "\n",
      "üìä Service Status:\n",
      "   Name: MULTIMODAL_SEARCH_SERVICE\n",
      "   Database: GWAS\n",
      "   Schema: PDF_PROCESSING\n",
      "\n",
      "‚ö†Ô∏è  Note: Service may take ~1 minute to build indexes\n",
      "   Wait before running search queries if you get errors\n"
     ]
    }
   ],
   "source": [
    "# Create multi-index Cortex Search Service\n",
    "print(\"üîÑ Creating Cortex Search Service...\\n\")\n",
    "print(\"üìã Service Configuration:\")\n",
    "print(\"   ‚Ä¢ Name: MULTIMODAL_SEARCH_SERVICE\")\n",
    "print(\"   ‚Ä¢ Text Index: page_text (keyword search)\")\n",
    "print(\"   ‚Ä¢ Vector Index 1: text_embedding (1024D - Arctic-8k)\")\n",
    "print(\"   ‚Ä¢ Vector Index 2: image_embedding (1024D - voyage-multimodal-3)\")\n",
    "print(\"   ‚Ä¢ Target Lag: 1 minute\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if service already exists\n",
    "    check_sql = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    \n",
    "    service_exists = False\n",
    "    try:\n",
    "        result = session.sql(check_sql).collect()\n",
    "        service_exists = len(result) > 0\n",
    "    except:\n",
    "        service_exists = False\n",
    "    \n",
    "    if service_exists:\n",
    "        print(\"‚úÖ Service already exists, skipping creation (will refresh at end)\\n\")\n",
    "        # Skip to refresh section\n",
    "    else:\n",
    "        print(\"üÜï Creating new search service...\\n\")\n",
    "        \n",
    "        # Create multi-index search service (Limited Private Preview feature)\n",
    "        # Docs: https://docs.snowflake.com/LIMITEDACCESS/cortex-search/multi-index-service\n",
    "        create_sql = f\"\"\"\n",
    "CREATE CORTEX SEARCH SERVICE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE\n",
    "  TEXT INDEXES page_text\n",
    "  VECTOR INDEXES (\n",
    "    text_embedding,\n",
    "    image_embedding\n",
    "  )\n",
    "  ATTRIBUTES (\n",
    "    page_id,\n",
    "    document_id,\n",
    "    file_name,\n",
    "    page_number,\n",
    "    image_path\n",
    "  )\n",
    "  WAREHOUSE = {WAREHOUSE_NAME}\n",
    "  TARGET_LAG = '1 minute'\n",
    "AS \n",
    "  SELECT \n",
    "    page_id,\n",
    "    document_id,\n",
    "    file_name,\n",
    "    page_number,\n",
    "    page_text,\n",
    "    text_embedding,\n",
    "    image_embedding,\n",
    "    image_path\n",
    "  FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "  WHERE has_text = TRUE AND has_image = TRUE\n",
    "\"\"\"\n",
    "    \n",
    "        session.sql(create_sql).collect()\n",
    "        print(\"‚úÖ Cortex Search Service created!\\n\")\n",
    "    \n",
    "    # Regardless of create or skip, check service status\n",
    "    status_sql = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    status = session.sql(status_sql).collect()\n",
    "    if status:\n",
    "        print(\"üìä Service Status:\")\n",
    "        print(f\"   Name: {status[0][1]}\")  # name column\n",
    "        print(f\"   Database: {status[0][2]}\")  # database_name\n",
    "        print(f\"   Schema: {status[0][3]}\")  # schema_name\n",
    "        print(\"\\n‚ö†Ô∏è  Note: Service may take ~1 minute to build indexes\")\n",
    "        print(\"   Wait before running search queries if you get errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating search service: {e}\")\n",
    "    print(\"\\n   If you see 'already exists', that's OK - service is ready\")\n",
    "    print(\"   If you see 'insufficient privileges', contact your Snowflake admin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Refreshing Search Service...\n",
      "\n",
      "‚è±Ô∏è  Initiating service refresh...\n",
      "‚úÖ Service refresh initiated\n",
      "\n",
      "‚è≥ Waiting 5 seconds for service to sync...\n",
      "‚úÖ Ready to query\n",
      "\n",
      "‚úÖ 14 pages are indexed and ready for search\n"
     ]
    }
   ],
   "source": [
    "# Refresh the search service to pick up any new data\n",
    "# This is fast and updates indexes without recreating the service\n",
    "print(\"üîÑ Refreshing Search Service...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check current refresh status\n",
    "    status_query = \"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        database_name,\n",
    "        schema_name,\n",
    "        created_on,\n",
    "        refresh_on\n",
    "    FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n",
    "    WHERE name = 'MULTIMODAL_SEARCH_SERVICE'\n",
    "    \"\"\"\n",
    "    \n",
    "    # First get the service info\n",
    "    show_query = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    session.sql(show_query)\n",
    "    \n",
    "    # Force a refresh\n",
    "    print(\"‚è±Ô∏è  Initiating service refresh...\")\n",
    "    refresh_query = f\"\"\"\n",
    "    ALTER CORTEX SEARCH SERVICE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE REFRESH\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        session.sql(refresh_query).collect()\n",
    "        print(\"‚úÖ Service refresh initiated\\n\")\n",
    "    except Exception as refresh_error:\n",
    "        if \"does not support manual refresh\" in str(refresh_error):\n",
    "            print(\"‚ÑπÔ∏è  Service auto-refreshes based on TARGET_LAG setting\\n\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Refresh note: {refresh_error}\\n\")\n",
    "    \n",
    "    # Wait a moment for refresh\n",
    "    import time\n",
    "    print(\"‚è≥ Waiting 5 seconds for service to sync...\")\n",
    "    time.sleep(5)\n",
    "    print(\"‚úÖ Ready to query\\n\")\n",
    "    \n",
    "    # Verify data one more time\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT COUNT(*) as ready_pages\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "      AND text_embedding IS NOT NULL\n",
    "      AND image_embedding IS NOT NULL\n",
    "      AND has_text = TRUE\n",
    "      AND has_image = TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "    result = session.sql(verify_query).collect()\n",
    "    if result and result[0][0] > 0:\n",
    "        print(f\"‚úÖ {result[0][0]} pages are indexed and ready for search\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No pages found matching service criteria\")\n",
    "        print(\"   Service filters: has_text = TRUE AND has_image = TRUE\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  {e}\")\n",
    "    print(\"\\n‚ÑπÔ∏è  This is OK - service should still work if it was created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying Search Service Status...\n",
      "\n",
      "‚úÖ Search service exists\n",
      "   Name: MULTIMODAL_SEARCH_SERVICE\n",
      "   Created: 1 minute\n",
      "\n",
      "üìä Data Readiness:\n",
      "   Total pages: 14\n",
      "   With text embeddings: 14\n",
      "   With image embeddings: 14\n",
      "   With BOTH embeddings: 14\n",
      "\n",
      "‚úÖ Ready to search 14 pages\n",
      "\n",
      "üí° If you just created the service, wait ~60 seconds for indexes to build\n"
     ]
    }
   ],
   "source": [
    "# Verify search service and data readiness\n",
    "print(\"üîç Verifying Search Service Status...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if service exists\n",
    "    check_service = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    service_info = session.sql(check_service).collect()\n",
    "    \n",
    "    if service_info:\n",
    "        print(\"‚úÖ Search service exists\")\n",
    "        print(f\"   Name: {service_info[0][1]}\")\n",
    "        print(f\"   Created: {service_info[0][4]}\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Search service NOT found!\")\n",
    "        print(\"   Run the previous cell to create it\\n\")\n",
    "    \n",
    "    # Check data in multimodal pages\n",
    "    data_check = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL THEN 1 END) as with_text_emb,\n",
    "        COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as with_image_emb,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL AND image_embedding IS NOT NULL THEN 1 END) as with_both\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    data_stats = session.sql(data_check).collect()\n",
    "    if data_stats:\n",
    "        total, text_emb, image_emb, both = data_stats[0]\n",
    "        print(f\"üìä Data Readiness:\")\n",
    "        print(f\"   Total pages: {total}\")\n",
    "        print(f\"   With text embeddings: {text_emb}\")\n",
    "        print(f\"   With image embeddings: {image_emb}\")\n",
    "        print(f\"   With BOTH embeddings: {both}\")\n",
    "        \n",
    "        if both == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: No pages have both embeddings!\")\n",
    "            print(\"   Search service filters for: has_text = TRUE AND has_image = TRUE\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Ready to search {both} pages\")\n",
    "    \n",
    "    # Give service time to build indexes\n",
    "    print(\"\\nüí° If you just created the service, wait ~60 seconds for indexes to build\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking service: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 9: Test Multimodal Search\n",
    "\n",
    "Query the multi-index Cortex Search service with:\n",
    "- **Text keyword search** (exact/fuzzy matching on page_text)\n",
    "- **Text embedding search** (semantic similarity with Arctic-8k)\n",
    "- **Image embedding search** (visual similarity with voyage-multimodal-3)\n",
    "\n",
    "The search uses weighted scoring to balance text and visual results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b94e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector conversion helper function defined!\n",
      "\n",
      "Example usage:\n",
      "text_vector = safe_vector_conversion(embeddings[0][0])\n",
      "image_vector = safe_vector_conversion(embeddings[0][1])\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTION: Safely convert embeddings to proper list format\n",
    "def safe_vector_conversion(vector_data):\n",
    "    \"\"\"\n",
    "    Safely convert Snowflake embedding results to Python lists.\n",
    "    Handles various formats that Snowflake might return.\n",
    "    \"\"\"\n",
    "    if vector_data is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's already a list, return it\n",
    "    if isinstance(vector_data, list) and len(vector_data) > 0 and isinstance(vector_data[0], (int, float)):\n",
    "        return vector_data\n",
    "    \n",
    "    # If it's a string representation of a list\n",
    "    if isinstance(vector_data, str):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(vector_data)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except:\n",
    "            # If ast.literal_eval fails, try json\n",
    "            try:\n",
    "                import json\n",
    "                parsed = json.loads(vector_data)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # If it has a tolist method (numpy array or similar)\n",
    "    if hasattr(vector_data, 'tolist'):\n",
    "        return vector_data.tolist()\n",
    "    \n",
    "    # If it's an array-like object that can be converted to list\n",
    "    try:\n",
    "        result = list(vector_data)\n",
    "        # Check if we got a proper numeric list\n",
    "        if result and isinstance(result[0], (int, float)):\n",
    "            return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # If all else fails, raise an error\n",
    "    raise ValueError(f\"Could not convert vector data of type {type(vector_data)} to list\")\n",
    "\n",
    "# Test the function\n",
    "print(\"‚úÖ Vector conversion helper function defined!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"text_vector = safe_vector_conversion(embeddings[0][0])\")\n",
    "print(\"image_vector = safe_vector_conversion(embeddings[0][1])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ Section 10 - Extract GWAS Traits (Optimized AI Pipeline)\n",
    "\n",
    "**Overview of extraction phases:**\n",
    "- **Cell 40**: Define 15 GWAS traits with complex extraction prompts\n",
    "- **Cell 42**: Phase 1 - AI_EXTRACT from full document text\n",
    "- **Cell 44**: Phase 2 - Multimodal search + AI_EXTRACT validation\n",
    "- **Cell 45**: Phase 3 - Final merge of Phase 1 & Phase 2 results\n",
    "- **Cell 46**: Display final results\n",
    "\n",
    "This optimized approach uses AI_EXTRACT exclusively for consistent, fast, and accurate GWAS trait extraction from scientific papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Defined 15 GWAS Traits for Targeted Extraction\n",
      "\n",
      "================================================================================\n",
      "‚ú® IMPROVEMENTS APPLIED:\n",
      "   ‚úÖ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean, tomato)\n",
      "   ‚úÖ Germplasm_Name: Added rice, wheat, Arabidopsis, soybean examples\n",
      "   ‚úÖ Genome_Version: Added 6 crop genome formats\n",
      "   ‚úÖ Gene: Added 5 crop gene ID patterns\n",
      "   ‚úÖ Allele: Shortened from 15 lines to 8 lines (50% reduction)\n",
      "   ‚úÖ Chromosome: Now accepts numbers, letters (3A, X, Y, MT), linkage groups\n",
      "   ‚úÖ Enhanced search queries with GWAS terminology\n",
      "   ‚úÖ NEW: Multi-finding support (extract ALL significant associations, not just strongest)\n",
      "================================================================================\n",
      "\n",
      " 1. Trait                ‚Üí Search: 'trait phenotype disease resistance agronomic chara...'\n",
      " 2. Germplasm_Name       ‚Üí Search: 'germplasm variety line population inbred diversity...'\n",
      " 3. Genome_Version       ‚Üí Search: 'genome version reference assembly RefGen annotatio...'\n",
      " 4. GWAS_Model           ‚Üí Search: 'GWAS model GLM MLM statistical method population s...'\n",
      " 5. Evidence_Type        ‚Üí Search: 'GWAS QTL linkage association mapping study type ge...'\n",
      " 6. Chromosome           ‚Üí Search: 'chromosome chr number genomic location linkage gro...'\n",
      " 7. Physical_Position    ‚Üí Search: 'physical position locus base pairs bp genomic coor...'\n",
      " 8. Gene                 ‚Üí Search: 'candidate gene causal gene functional gene locus g...'\n",
      " 9. SNP_Name             ‚Üí Search: 'SNP marker name identifier genotyping array lead m...'\n",
      "10. Variant_ID           ‚Üí Search: 'variant ID SNP ID rs number dbSNP database identif...'\n",
      "11. Variant_Type         ‚Üí Search: 'variant type SNP InDel polymorphism haplotype mark...'\n",
      "12. Effect_Size          ‚Üí Search: 'effect size R-squared R2 variance explained phenot...'\n",
      "13. Allele               ‚Üí Search: 'allele REF ALT haplotype genotype reference altern...'\n",
      "14. Annotation           ‚Üí Search: 'functional annotation missense synonymous intergen...'\n",
      "15. Candidate_Region     ‚Üí Search: 'QTL region confidence interval linkage disequilibr...'\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Ready to extract 15 traits using multi-phase approach\n",
      "üåæ Now supports: Maize, Rice, Wheat, Arabidopsis, Soybean, Tomato, and more!\n",
      "üéØ NEW: Can extract 10-20 findings per paper (not just strongest SNP)\n"
     ]
    }
   ],
   "source": [
    "# Define 15 GWAS traits with refined, context-aware extraction prompts\n",
    "# Based on GWAS paper structure: Abstract ‚Üí Intro ‚Üí Methods ‚Üí Results ‚Üí Discussion\n",
    "# ‚ú® IMPROVED: Fixed for multi-species plant genomics coverage\n",
    "# ‚ú® NEW: Support for multiple findings extraction (10-20 SNPs per paper)\n",
    "\n",
    "traits_config_improved = {\n",
    "    # ========================================\n",
    "    # DOCUMENT-LEVEL TRAITS (Extract once per paper)\n",
    "    # ========================================\n",
    "    \n",
    "    \"Trait\": {\n",
    "        \"search_query\": \"trait phenotype disease resistance agronomic character quality stress tolerance\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the MAIN phenotypic trait studied in this GWAS paper.\n",
    "\n",
    "Look in: Title, Abstract (first paragraph), Introduction (study objective).\n",
    "\n",
    "Format: Descriptive name of the trait being studied.\n",
    "Examples: 'Disease resistance' (generic), 'Plant height', 'Flowering time', 'Grain yield', 'Drought tolerance'\n",
    "\n",
    "Return the primary trait name ONLY, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Germplasm_Name\": {\n",
    "        \"search_query\": \"germplasm variety line population inbred diversity panel genetic background subpopulation\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the germplasm/population used in this GWAS study.\n",
    "\n",
    "Look in: Methods ‚Üí Plant Materials/Germplasm, Introduction ‚Üí Study population.\n",
    "\n",
    "Common formats across crops:\n",
    "- Inbred lines: 'B73' (maize), 'Nipponbare' (rice), 'Col-0' (Arabidopsis), 'Chinese Spring' (wheat)\n",
    "- Diversity panels: '282 association panel', '3K rice genome panel', 'SoyNAM', 'UK wheat diversity panel'\n",
    "- Population codes: 'DH population', 'RIL population', 'F2:3 families', 'BC1F2'\n",
    "- Specific varieties: 'Williams 82' (soybean), 'Kitaake' (rice)\n",
    "\n",
    "Return the most specific germplasm name, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Genome_Version\": {\n",
    "        \"search_query\": \"genome version reference assembly RefGen annotation build\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the reference genome assembly version used.\n",
    "\n",
    "Look in: Methods ‚Üí Genotyping/Variant Calling, Supplementary Methods.\n",
    "\n",
    "Common formats by crop:\n",
    "- Maize: 'B73 RefGen_v4', 'AGPv4', 'Zm00001e'\n",
    "- Rice: 'IRGSP-1.0', 'MSU7', 'Nipponbare-v7.0'\n",
    "- Wheat: 'IWGSC RefSeq v2.1', 'CS42'\n",
    "- Arabidopsis: 'TAIR10', 'Col-0'\n",
    "- Soybean: 'Glycine_max_v4.0', 'Williams 82 v2.0'\n",
    "- Tomato: 'SL4.0', 'Heinz 1706'\n",
    "\n",
    "Return the version identifier, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"GWAS_Model\": {\n",
    "        \"search_query\": \"GWAS model GLM MLM statistical method population structure kinship software\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the statistical model/software used for GWAS.\n",
    "\n",
    "Look in: Methods ‚Üí Statistical analysis/GWAS analysis section.\n",
    "\n",
    "Common models: MLM (mixed linear model), GLM, CMLM, FarmCPU, BLINK, SUPER,\n",
    "               EMMAX, FastGWA, rrBLUP, BOLT-LMM\n",
    "\n",
    "Common software: TASSEL, GAPIT, GEMMA, PLINK, regenie, GCTA, rMVP, GENESIS\n",
    "\n",
    "Return model name OR software, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Evidence_Type\": {\n",
    "        \"search_query\": \"GWAS QTL linkage association mapping study type genetic analysis\",\n",
    "        \"extraction_prompt\": \"\"\"Identify the genetic mapping approach used.\n",
    "\n",
    "Look in: Title, Abstract, Methods ‚Üí Study design.\n",
    "\n",
    "Types: \n",
    "- 'GWAS' (genome-wide association study) - most common\n",
    "- 'QTL' (quantitative trait loci mapping) - biparental populations\n",
    "- 'Linkage' (family-based mapping)\n",
    "- 'Fine_Mapping' (high-resolution narrowing of QTL)\n",
    "\n",
    "Return ONE type: 'GWAS', 'QTL', 'Linkage', 'Fine_Mapping', or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    # ========================================\n",
    "    # FINDING-LEVEL TRAITS (Extract multiple per paper)\n",
    "    # ========================================\n",
    "    # ‚ú® NEW: These can now extract arrays of findings\n",
    "    \n",
    "    \"Chromosome\": {\n",
    "        \"search_query\": \"chromosome chr number genomic location linkage group significant hits\",\n",
    "        \"extraction_prompt\": \"\"\"Extract ALL chromosomes with significant associations (p < 0.001 or genome-wide significant).\n",
    "\n",
    "Look in: Results ‚Üí GWAS hits, Manhattan plot peaks, Tables of significant SNPs.\n",
    "\n",
    "Format: Return comma-separated list of chromosome identifiers, ranked by significance (lowest p-value first).\n",
    "Examples: '5, 3, 10, 1' or '3A, 5B, 2D' (wheat) or 'X, 3, 5' or 'LG1, LG3, LG5' (linkage groups)\n",
    "\n",
    "If only 1 significant hit: Return that chromosome.\n",
    "If 10+ hits: Return top 10 most significant.\n",
    "\n",
    "Return chromosome identifiers (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Physical_Position\": {\n",
    "        \"search_query\": \"physical position locus base pairs bp genomic coordinate marker location\",\n",
    "        \"extraction_prompt\": \"\"\"Extract physical positions of SIGNIFICANT SNPs (top 10 by p-value).\n",
    "\n",
    "Look in: Results ‚Üí Significant associations, Tables with 'Position' or 'bp' columns.\n",
    "\n",
    "Format: Return comma-separated positions with chromosome context.\n",
    "Examples: \n",
    "- Single: '145.6 Mb'\n",
    "- Multiple: 'Chr5:145.6Mb, Chr3:198.2Mb, Chr10:78.9Mb'\n",
    "- Alt format: '145678901 (Chr5), 198234567 (Chr3)'\n",
    "\n",
    "If positions are in a table: Extract top 10 rows.\n",
    "Include chromosome reference for clarity.\n",
    "\n",
    "Return positions (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Gene\": {\n",
    "        \"search_query\": \"candidate gene causal gene functional gene locus gene model annotation\",\n",
    "        \"extraction_prompt\": \"\"\"Extract ALL candidate genes mentioned for significant associations.\n",
    "\n",
    "Look in: Results ‚Üí Candidate genes, Tables ‚Üí Gene columns, Discussion ‚Üí Gene function.\n",
    "\n",
    "Common formats across crops:\n",
    "- Maize: 'Zm00001d027230', 'GRMZM2G123456', 'tb1', 'dwarf8'\n",
    "- Rice: 'LOC_Os03g01234', 'OsMADS1', 'SD1'\n",
    "- Arabidopsis: 'AT1G12345', 'FLC', 'CO'\n",
    "- Wheat: 'TraesCS3A02G123456', 'Rht-D1'\n",
    "- Soybean: 'Glyma.01G000100', 'E1', 'Dt1'\n",
    "\n",
    "Return comma-separated list if multiple genes.\n",
    "Examples: 'Zm00001d027230, Zm00001d042156, Zm00001d013894'\n",
    "\n",
    "Return candidate genes (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"SNP_Name\": {\n",
    "        \"search_query\": \"SNP marker name identifier genotyping array lead markers\",\n",
    "        \"extraction_prompt\": \"\"\"Extract SNP/marker names for SIGNIFICANT associations (top 10).\n",
    "\n",
    "Look in: Results ‚Üí Significant markers, Tables ‚Üí Marker ID column.\n",
    "\n",
    "Common prefixes vary by genotyping platform:\n",
    "- Array-based: 'PZE-', 'AX-', 'Affx-'\n",
    "- Sequence-based: 'S1_', 'Chr1_', 'ss', 'rs' (if dbSNP)\n",
    "- Custom: May be position-based or study-specific\n",
    "\n",
    "Return comma-separated list if multiple SNPs.\n",
    "Examples: 'PZE-101234567, AX-90812345, S1_145678901'\n",
    "\n",
    "Return marker identifiers (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Variant_ID\": {\n",
    "        \"search_query\": \"variant ID SNP ID rs number dbSNP database identifier\",\n",
    "        \"extraction_prompt\": \"\"\"Extract dbSNP variant IDs if referenced for significant associations.\n",
    "\n",
    "Look in: Methods ‚Üí Variant annotation, Supplementary tables.\n",
    "\n",
    "Format: 'rs' or 'ss' prefixes (human/model organism databases)\n",
    "Examples: 'rs123456789, rs987654321, rs111222333'\n",
    "\n",
    "NOTE: Most plant studies don't use dbSNP IDs (common in human/model organisms).\n",
    "\n",
    "Return dbSNP IDs (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Variant_Type\": {\n",
    "        \"search_query\": \"variant type SNP InDel polymorphism haplotype marker genotyping\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the predominant variant/marker type analyzed.\n",
    "\n",
    "Look in: Methods ‚Üí Variant calling/Genotyping, Results ‚Üí Association type.\n",
    "\n",
    "Common types:\n",
    "- SNP (single nucleotide polymorphism) - most common\n",
    "- InDel (insertion/deletion)\n",
    "- CNV (copy number variant)\n",
    "- SV (structural variant)\n",
    "- PAV (presence/absence variant) - plant pangenomes\n",
    "- Haplotype (multi-marker block)\n",
    "- SSR/Microsatellite (older studies)\n",
    "\n",
    "Return ONE primary type (this is usually uniform across findings), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Effect_Size\": {\n",
    "        \"search_query\": \"effect size R-squared R2 variance explained phenotypic variation proportion\",\n",
    "        \"extraction_prompt\": \"\"\"Extract effect sizes for SIGNIFICANT QTLs (top 10).\n",
    "\n",
    "Look in: Results ‚Üí QTL effect, Tables ‚Üí R¬≤ or 'Variance explained' columns.\n",
    "\n",
    "Format: Return comma-separated if multiple, with chromosome context if helpful.\n",
    "Examples:\n",
    "- Single: 'R¬≤=0.23'\n",
    "- Multiple: '0.31 (Chr10), 0.23 (Chr5), 0.19 (Chr3)'\n",
    "- Alt format: '23%, 19%, 15%'\n",
    "\n",
    "Return effect sizes (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Allele\": {\n",
    "        \"search_query\": \"allele REF ALT haplotype genotype reference alternate favorable effect\",\n",
    "        \"extraction_prompt\": \"\"\"Extract allele information for SIGNIFICANT SNPs.\n",
    "\n",
    "Look in: Results tables (REF, ALT, Allele columns), figures, supplementary data.\n",
    "\n",
    "Common formats:\n",
    "- Slash: 'A/G', 'T/C', 'G/T'\n",
    "- Arrow: 'A>G', 'T>C'\n",
    "- Explicit: 'REF: A ALT: G'\n",
    "- Effect notation: 'favorable: T'\n",
    "\n",
    "If multiple SNPs: Return comma-separated alleles.\n",
    "Examples: 'A/G, T/C, G/A'\n",
    "\n",
    "NOTE: Allele data is typically in tables/charts, not body text.\n",
    "\n",
    "Return allele notations (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Annotation\": {\n",
    "        \"search_query\": \"functional annotation missense synonymous intergenic gene ontology regulatory\",\n",
    "        \"extraction_prompt\": \"\"\"Extract functional annotations for SIGNIFICANT variants.\n",
    "\n",
    "Look in: Results ‚Üí Variant annotation, Discussion ‚Üí Functional impact.\n",
    "\n",
    "Categories: \n",
    "- 'missense_variant', 'synonymous', 'intergenic_region'\n",
    "- 'upstream_gene', '5_prime_UTR', '3_prime_UTR'\n",
    "- 'intronic', 'regulatory_region'\n",
    "\n",
    "If multiple variants: Return comma-separated annotations.\n",
    "Examples: 'missense_variant, intergenic_region, missense_variant'\n",
    "\n",
    "Return annotations (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Candidate_Region\": {\n",
    "        \"search_query\": \"QTL region confidence interval linkage disequilibrium block bin locus interval\",\n",
    "        \"extraction_prompt\": \"\"\"Extract QTL regions or confidence intervals for SIGNIFICANT associations.\n",
    "\n",
    "Look in: Results ‚Üí QTL mapping, Tables ‚Üí QTL interval/region columns.\n",
    "\n",
    "Format: Genomic intervals with units\n",
    "Examples: \n",
    "- Single: 'chr1:145.6-146.1 Mb'\n",
    "- Multiple: 'chr5:145.6-146.1Mb, chr3:198-199Mb, chr10:78-79Mb'\n",
    "- Alt: 'bin 1.04, bin 3.05, bin 10.02'\n",
    "- cM: '10-12 cM (Chr5), 45-47 cM (Chr3)'\n",
    "\n",
    "Return genomic regions (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Defined 15 GWAS Traits for Targeted Extraction\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚ú® IMPROVEMENTS APPLIED:\")\n",
    "print(\"   ‚úÖ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean, tomato)\")\n",
    "print(\"   ‚úÖ Germplasm_Name: Added rice, wheat, Arabidopsis, soybean examples\")\n",
    "print(\"   ‚úÖ Genome_Version: Added 6 crop genome formats\")\n",
    "print(\"   ‚úÖ Gene: Added 5 crop gene ID patterns\")\n",
    "print(\"   ‚úÖ Allele: Shortened from 15 lines to 8 lines (50% reduction)\")\n",
    "print(\"   ‚úÖ Chromosome: Now accepts numbers, letters (3A, X, Y, MT), linkage groups\")\n",
    "print(\"   ‚úÖ Enhanced search queries with GWAS terminology\")\n",
    "print(\"   ‚úÖ NEW: Multi-finding support (extract ALL significant associations, not just strongest)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for idx, (trait_name, trait_info) in enumerate(traits_config_improved.items(), 1):\n",
    "    print(f\"{idx:2d}. {trait_name:20s} ‚Üí Search: '{trait_info['search_query'][:50]}...'\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ Ready to extract {len(traits_config_improved)} traits using multi-phase approach\")\n",
    "print(\"üåæ Now supports: Maize, Rice, Wheat, Arabidopsis, Soybean, Tomato, and more!\")\n",
    "print(\"üéØ NEW: Can extract 10-20 findings per paper (not just strongest SNP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Phase 1 - Optimized Text Extraction with AI_EXTRACT\n",
    "\n",
    "**What this does:** Extracts GWAS traits from the full document text using AI_EXTRACT:\n",
    "- **Single API call**: Batch processing all 15 traits at once\n",
    "- **Enhanced prompts**: Full complex prompts with multi-species examples\n",
    "- **Smart context**: 25K character window for comprehensive coverage\n",
    "- **Output**: Extracted traits with HIGH confidence when found\n",
    "\n",
    "Processes the complete document text to extract all genomic trait information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Phase 1: Text-Based Extraction (Optimized Single Method)\n",
      "\n",
      "================================================================================\n",
      "üéØ Strategy: Use AI_EXTRACT with enhanced prompts\n",
      "   ‚Ä¢ Batch processing all 15 traits in one call\n",
      "   ‚Ä¢ Full complex prompts (no truncation)\n",
      "   ‚Ä¢ 25K context window for better coverage\n",
      "   ‚Ä¢ Direct confidence based on extraction success\n",
      "\n",
      "‚úÖ Loaded document text: 62,758 characters\n",
      "\n",
      "üìä Extracting traits with AI_EXTRACT\n",
      "\n",
      "‚öôÔ∏è  Calling AI_EXTRACT with full complex prompts...\n",
      "   Context size: 25,006 chars\n",
      "   Prompt sizes: 346-584 chars\n",
      "\n",
      "   ‚úì Trait               : Resistance to brown planthopper\n",
      "   ‚úì Germplasm_Name      : 502 rice varieties\n",
      "   ‚úì Genome_Version      : IRGSP-1.0\n",
      "   ‚úì GWAS_Model          : EMMAX\n",
      "   ‚úì Evidence_Type       : GWAS\n",
      "   ‚úì Chromosome          : 11\n",
      "   ‚úó Physical_Position   : Not found\n",
      "   ‚úì Gene                : ['RLK', 'NB-LRR', 'LRR']\n",
      "   ‚úì SNP_Name            : ['rs1234567', 'rs2345678', 'rs3456789', 'rs4567890', 'rs5678\n",
      "   ‚úó Variant_ID          : Not found\n",
      "   ‚úì Variant_Type        : SNP (single nucleotide polymorphism)\n",
      "   ‚úó Effect_Size         : Not found\n",
      "   ‚úó Allele              : Not found\n",
      "   ‚úó Annotation          : Not found\n",
      "   ‚úì Candidate_Region    : chr1:145.6-146.1 Mb\n",
      "\n",
      "================================================================================\n",
      "üìä Phase 1 Results:\n",
      "   ‚úÖ Extracted: 10/15 traits\n",
      "   ‚ùå Not found: 5 traits\n",
      "   Missing: Physical_Position, Variant_ID, Effect_Size, Allele, Annotation\n",
      "\n",
      "üéØ Confidence Distribution:\n",
      "   HIGH      : 10 traits\n",
      "   NONE      :  5 traits\n",
      "\n",
      "‚úÖ Optimization Features:\n",
      "   ‚Ä¢ Single API call (faster)\n",
      "   ‚Ä¢ Full prompts (better accuracy)\n",
      "   ‚Ä¢ 25K context (comprehensive)\n",
      "   ‚Ä¢ Direct confidence (simpler)\n",
      "   ‚Ä¢ No redundant dual extraction\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: OPTIMIZED AI_EXTRACT - Single Method Extraction\n",
    "print(\"üìù Phase 1: Text-Based Extraction (Optimized Single Method)\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Strategy: Use AI_EXTRACT with enhanced prompts\")\n",
    "print(\"   ‚Ä¢ Batch processing all 15 traits in one call\")\n",
    "print(\"   ‚Ä¢ Full complex prompts (no truncation)\")\n",
    "print(\"   ‚Ä¢ 25K context window for better coverage\")\n",
    "print(\"   ‚Ä¢ Direct confidence based on extraction success\\n\")\n",
    "\n",
    "# Get all text pages\n",
    "context_query = f\"\"\"\n",
    "SELECT LISTAGG(page_text, '\\\\n\\\\n---PAGE BREAK---\\\\n\\\\n') WITHIN GROUP (ORDER BY page_number) as full_text\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "WHERE document_id = '{DOCUMENT_ID}'\n",
    "\"\"\"\n",
    "\n",
    "# Helper function to validate if a value is actually meaningful\n",
    "def is_valid_value(val):\n",
    "    \"\"\"Check if value is meaningful (not 'NOT_FOUND' or garbage)\"\"\"\n",
    "    if not val:\n",
    "        return False\n",
    "    \n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    \n",
    "    # Check for explicit NOT_FOUND patterns\n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    \n",
    "    # Check for meta-responses\n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    \n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "try:\n",
    "    all_text = session.sql(context_query).collect()\n",
    "    \n",
    "    if not all_text or not all_text[0][0]:\n",
    "        print(\"‚ö†Ô∏è  No text pages found in TEXT_PAGES table\")\n",
    "        print(\"   Make sure Section 5 (Extract Text Pages) was run\")\n",
    "        text_extraction_results = {}\n",
    "        fields_found = 0\n",
    "        fields_not_found = list(traits_config_improved.keys())\n",
    "        confidence_levels = {}\n",
    "    else:\n",
    "        full_document_text = all_text[0][0]\n",
    "        print(f\"‚úÖ Loaded document text: {len(full_document_text):,} characters\\n\")\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        # =============================================================================\n",
    "        # AI_EXTRACT with FULL COMPLEX prompts\n",
    "        # =============================================================================\n",
    "        print(\"üìä Extracting traits with AI_EXTRACT\\n\")\n",
    "        \n",
    "        # Use FULL prompts without truncation\n",
    "        complex_prompts = {}\n",
    "        for trait_name, trait_info in traits_config_improved.items():\n",
    "            # Convert multi-line prompt to single line, preserve ALL instructions\n",
    "            detailed_prompt = trait_info['extraction_prompt']\n",
    "            condensed = ' '.join(detailed_prompt.replace('\\n', ' ').split())\n",
    "            complex_prompts[trait_name] = condensed\n",
    "        \n",
    "        # Smart context selection: 25K chars\n",
    "        if len(full_document_text) > 25000:\n",
    "            # Keep first 15K (intro/methods) + last 10K (results/tables)\n",
    "            clean_text = (full_document_text[:15000] + \" ... \" + full_document_text[-10000:])\n",
    "        else:\n",
    "            clean_text = full_document_text\n",
    "        \n",
    "        clean_text = clean_text.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        # Create JSON for responseFormat\n",
    "        response_format_json = json.dumps(complex_prompts)\n",
    "        response_format_sql = response_format_json.replace(\"'\", \"''\")\n",
    "        \n",
    "        extract_query = f\"\"\"\n",
    "        SELECT AI_EXTRACT(\n",
    "            text => '{clean_text}',\n",
    "            responseFormat => PARSE_JSON('{response_format_sql}')\n",
    "        ) as extracted_data\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"‚öôÔ∏è  Calling AI_EXTRACT with full complex prompts...\")\n",
    "        print(f\"   Context size: {len(clean_text):,} chars\")\n",
    "        print(f\"   Prompt sizes: {min(len(p) for p in complex_prompts.values())}-{max(len(p) for p in complex_prompts.values())} chars\\n\")\n",
    "        \n",
    "        result = session.sql(extract_query).collect()\n",
    "        \n",
    "        # Process results\n",
    "        text_extraction_results = {\n",
    "            \"document_id\": DOCUMENT_ID,\n",
    "            \"file_name\": PDF_FILENAME,\n",
    "            \"extraction_source\": \"ai_extract_optimized\"\n",
    "        }\n",
    "        confidence_levels = {}\n",
    "        fields_found = 0\n",
    "        fields_not_found = []\n",
    "        \n",
    "        if result and result[0][0]:\n",
    "            extracted_json = result[0][0]\n",
    "            if isinstance(extracted_json, str):\n",
    "                extracted_data = json.loads(extracted_json)\n",
    "            else:\n",
    "                extracted_data = extracted_json\n",
    "            \n",
    "            if 'response' in extracted_data:\n",
    "                extracted_data = extracted_data['response']\n",
    "            \n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                value = extracted_data.get(trait_name)\n",
    "                if is_valid_value(value):\n",
    "                    text_extraction_results[trait_name] = value\n",
    "                    # Direct confidence: HIGH if found, as AI_EXTRACT is our best method\n",
    "                    confidence_levels[trait_name] = \"HIGH\"\n",
    "                    fields_found += 1\n",
    "                    print(f\"   ‚úì {trait_name:20s}: {str(value)[:60]}\")\n",
    "                else:\n",
    "                    text_extraction_results[trait_name] = None\n",
    "                    confidence_levels[trait_name] = \"NONE\"\n",
    "                    fields_not_found.append(trait_name)\n",
    "                    print(f\"   ‚úó {trait_name:20s}: Not found\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  AI_EXTRACT returned no results\")\n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                text_extraction_results[trait_name] = None\n",
    "                confidence_levels[trait_name] = \"NONE\"\n",
    "                fields_not_found.append(trait_name)\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during extraction: {str(e)[:200]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    text_extraction_results = {\n",
    "        \"document_id\": DOCUMENT_ID,\n",
    "        \"file_name\": PDF_FILENAME,\n",
    "        \"extraction_source\": \"ai_extract_optimized\"\n",
    "    }\n",
    "    confidence_levels = {}\n",
    "    fields_found = 0\n",
    "    fields_not_found = list(traits_config_improved.keys())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìä Phase 1 Results:\")\n",
    "print(f\"   ‚úÖ Extracted: {fields_found}/{len(traits_config_improved)} traits\")\n",
    "print(f\"   ‚ùå Not found: {len(fields_not_found)} traits\")\n",
    "if fields_not_found:\n",
    "    print(f\"   Missing: {', '.join(fields_not_found[:5])}{'...' if len(fields_not_found) > 5 else ''}\")\n",
    "\n",
    "# Show confidence distribution\n",
    "conf_counts = {}\n",
    "for conf in confidence_levels.values():\n",
    "    conf_counts[conf] = conf_counts.get(conf, 0) + 1\n",
    "print(f\"\\nüéØ Confidence Distribution:\")\n",
    "for level in [\"HIGH\", \"NONE\"]:\n",
    "    count = conf_counts.get(level, 0)\n",
    "    if count > 0:\n",
    "        print(f\"   {level:10s}: {count:2d} traits\")\n",
    "\n",
    "print(\"\\n‚úÖ Optimization Features:\")\n",
    "print(\"   ‚Ä¢ Single API call (faster)\")\n",
    "print(\"   ‚Ä¢ Full prompts (better accuracy)\")\n",
    "print(\"   ‚Ä¢ 25K context (comprehensive)\")\n",
    "print(\"   ‚Ä¢ Direct confidence (simpler)\")\n",
    "print(\"   ‚Ä¢ No redundant dual extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Phase 2 - Multimodal Search Validation\n",
    "\n",
    "**What this does:** Uses Cortex Search Service to validate and enrich Phase 1 results:\n",
    "- **Multimodal search**: Combines text + image embeddings to find data-rich pages\n",
    "- **Focused extraction**: Targets tables, figures, and results sections\n",
    "- **AI_EXTRACT**: Single batch call for all 15 traits\n",
    "- **Validation**: Compares with Phase 1 to identify agreements/disagreements\n",
    "- **Enrichment**: Captures findings from visual elements (charts/graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Phase 2: Multimodal Search Validation (Optimized)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Strategy: Multimodal search + AI_EXTRACT batch extraction\n",
      "   ‚Ä¢ Multimodal search for relevant pages\n",
      "   ‚Ä¢ AI_EXTRACT for batch trait extraction\n",
      "   ‚Ä¢ Focus on tables, figures, and results sections\n",
      "   ‚Ä¢ Validate and enrich Phase 1 findings\n",
      "\n",
      "‚öôÔ∏è  Step 1: Multimodal Search\n",
      "\n",
      "üìã Search query: 'GWAS results significant SNP QTL chromosome position gene allele effect size table figure'\n",
      "\n",
      "   ‚úÖ Text vector: 1024 dims\n",
      "   ‚úÖ Image vector: 1024 dims\n",
      "\n",
      "   ‚úÖ Found 10 relevant pages\n",
      "   ‚è±Ô∏è  Search time: 1.3s\n",
      "\n",
      "‚öôÔ∏è  Step 2: Batch extraction with AI_EXTRACT\n",
      "   Context: 41,824 chars (using 20,003 chars)\n",
      "   Extracting all 15 traits in one call...\n",
      "\n",
      "   üîÑ Calling AI_EXTRACT...\n",
      "   ‚úì Trait               : Resistance to BPH\n",
      "   ‚úì Germplasm_Name      : 502 rice varieties\n",
      "   ‚úì Genome_Version      : Nipponbare reference genome\n",
      "   ‚úì GWAS_Model          : rrBLUP\n",
      "   ‚úì Evidence_Type       : GWAS\n",
      "   ‚úì Chromosome          : ['2', '4', '6', '11', '12']\n",
      "   ‚úì Physical_Position   : ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "   ‚úì Gene                : ['LOC_Os06g03970 (receptor-like protein kinase)', \n",
      "   ‚úì SNP_Name            : ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "   ‚úì Variant_ID          : ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "   ‚úì Variant_Type        : SNP (single nucleotide polymorphism)\n",
      "   ‚úó Effect_Size         : Not found\n",
      "   ‚úó Allele              : Not found\n",
      "   ‚úó Annotation          : Not found\n",
      "   ‚úì Candidate_Region    : ['chr2:23.86-24.06Mb', 'chr4:21.27-21.52Mb', 'chr6\n",
      "\n",
      "   ‚úÖ Extraction completed in 7.2s\n",
      "\n",
      "================================================================================\n",
      "üìä Comparison: Phase 1 (Full Text) vs Phase 2 (Multimodal Search)\n",
      "\n",
      "‚ö†Ô∏è  Trait               : DIFFER\n",
      "      Phase 1 [HIGH]: Resistance to brown planthopper\n",
      "      Phase 2 [MEDIUM]: Resistance to BPH\n",
      "‚úÖ Germplasm_Name      : AGREE ‚Üí 502 rice varieties\n",
      "‚ö†Ô∏è  Genome_Version      : DIFFER\n",
      "      Phase 1 [HIGH]: IRGSP-1.0\n",
      "      Phase 2 [MEDIUM]: Nipponbare reference genome\n",
      "‚ö†Ô∏è  GWAS_Model          : DIFFER\n",
      "      Phase 1 [HIGH]: EMMAX\n",
      "      Phase 2 [MEDIUM]: rrBLUP\n",
      "‚úÖ Evidence_Type       : AGREE ‚Üí GWAS\n",
      "‚ö†Ô∏è  Chromosome          : DIFFER\n",
      "      Phase 1 [HIGH]: 11\n",
      "      Phase 2 [MEDIUM]: ['2', '4', '6', '11', '12']\n",
      "üÜï Physical_Position   : NEW from multimodal ‚Üí ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "‚ö†Ô∏è  Gene                : DIFFER\n",
      "      Phase 1 [HIGH]: ['RLK', 'NB-LRR', 'LRR']\n",
      "      Phase 2 [MEDIUM]: ['LOC_Os06g03970 (receptor-like protein kinase)', \n",
      "‚ö†Ô∏è  SNP_Name            : DIFFER\n",
      "      Phase 1 [HIGH]: ['rs1234567', 'rs2345678', 'rs3456789', 'rs4567890\n",
      "      Phase 2 [MEDIUM]: ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "üÜï Variant_ID          : NEW from multimodal ‚Üí ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "‚úÖ Variant_Type        : AGREE ‚Üí SNP (single nucleotide polymorphism)\n",
      "‚ùå Effect_Size         : NOT FOUND in either phase\n",
      "‚ùå Allele              : NOT FOUND in either phase\n",
      "‚ùå Annotation          : NOT FOUND in either phase\n",
      "‚ö†Ô∏è  Candidate_Region    : DIFFER\n",
      "      Phase 1 [HIGH]: chr1:145.6-146.1 Mb\n",
      "      Phase 2 [MEDIUM]: ['chr2:23.86-24.06Mb', 'chr4:21.27-21.52Mb', 'chr6\n",
      "\n",
      "================================================================================\n",
      "üìä Phase 2 Results:\n",
      "   ‚úÖ Agreements: 3 traits\n",
      "   ‚ö†Ô∏è  Disagreements: 7 traits\n",
      "   üÜï New findings: 2 traits\n",
      "   üìà Total from Phase 2: 12/15 traits\n",
      "\n",
      "‚úÖ Optimization Benefits:\n",
      "   ‚Ä¢ Single AI_EXTRACT call (15x faster than AI_COMPLETE)\n",
      "   ‚Ä¢ Multimodal search focuses on data-rich pages\n",
      "   ‚Ä¢ Consistent extraction methodology\n",
      "   ‚Ä¢ Better batch processing\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: MULTIMODAL SEARCH + AI_EXTRACT (Validation & Enrichment)\n",
    "print(\"\\nüîç Phase 2: Multimodal Search Validation (Optimized)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"‚úÖ Strategy: Multimodal search + AI_EXTRACT batch extraction\")\n",
    "print(\"   ‚Ä¢ Multimodal search for relevant pages\")\n",
    "print(\"   ‚Ä¢ AI_EXTRACT for batch trait extraction\")\n",
    "print(\"   ‚Ä¢ Focus on tables, figures, and results sections\")\n",
    "print(\"   ‚Ä¢ Validate and enrich Phase 1 findings\\n\")\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Helper function to validate values\n",
    "def is_valid_value(val):\n",
    "    \"\"\"Check if value is meaningful (not 'NOT_FOUND' or garbage)\"\"\"\n",
    "    if not val:\n",
    "        return False\n",
    "    \n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    \n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    \n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    \n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Initialize results\n",
    "multimodal_extraction_results = {}\n",
    "multimodal_confidence_levels = {}\n",
    "multimodal_fields_found = 0\n",
    "agreements = 0\n",
    "disagreements = 0\n",
    "phase2_new_findings = 0\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Step 1: Multimodal Search\\n\")\n",
    "    \n",
    "    # Build search query focused on results/data\n",
    "    search_query = \"GWAS results significant SNP QTL chromosome position gene allele effect size table figure\"\n",
    "    print(f\"üìã Search query: '{search_query}'\\n\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embed_query = f\"\"\"\n",
    "    SELECT\n",
    "        AI_EMBED('snowflake-arctic-embed-l-v2.0-8k', '{search_query}') as text_vector,\n",
    "        AI_EMBED('voyage-multimodal-3', '{search_query}') as image_vector\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = session.sql(embed_query).collect()\n",
    "    text_vector = [float(x) for x in safe_vector_conversion(embeddings[0][0])]\n",
    "    image_vector = [float(x) for x in safe_vector_conversion(embeddings[0][1])]\n",
    "    \n",
    "    print(f\"   ‚úÖ Text vector: {len(text_vector)} dims\")\n",
    "    print(f\"   ‚úÖ Image vector: {len(image_vector)} dims\\n\")\n",
    "    \n",
    "    # Build multimodal search query\n",
    "    query_json = {\n",
    "        \"multi_index_query\": {\n",
    "            \"page_text\": [{\"text\": search_query}],\n",
    "            \"text_embedding\": [{\"vector\": text_vector}],\n",
    "            \"image_embedding\": [{\"vector\": image_vector}]\n",
    "        },\n",
    "        \"columns\": [\"document_id\", \"page_text\", \"page_number\"],\n",
    "        \"limit\": 10,\n",
    "        \"filter\": {\n",
    "            \"@eq\": {\n",
    "                \"document_id\": DOCUMENT_ID\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    query_str = json.dumps(query_json).replace(\"'\", \"''\")\n",
    "    \n",
    "    search_sql = f\"\"\"\n",
    "    SELECT\n",
    "      result.value:document_id::VARCHAR as document_id,\n",
    "      result.value:page_text::VARCHAR as page_text,\n",
    "      result.value:page_number::INT as page_number\n",
    "    FROM TABLE(\n",
    "      FLATTEN(\n",
    "        PARSE_JSON(\n",
    "          SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "            '{DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE',\n",
    "            '{query_str}'\n",
    "          )\n",
    "        )['results']\n",
    "      )\n",
    "    ) as result\n",
    "    \"\"\"\n",
    "    \n",
    "    search_results = session.sql(search_sql).collect()\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    if not search_results:\n",
    "        print(f\"   ‚ö†Ô∏è  No results found\")\n",
    "        multimodal_extraction_results = {}\n",
    "        multimodal_fields_found = 0\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Found {len(search_results)} relevant pages\")\n",
    "        print(f\"   ‚è±Ô∏è  Search time: {search_time:.1f}s\\n\")\n",
    "        \n",
    "        # Concatenate search results\n",
    "        search_context = '\\n\\n---PAGE---\\n\\n'.join([f\"[Page {row[2]}]\\n{row[1]}\" for row in search_results])\n",
    "        context_length = len(search_context)\n",
    "        \n",
    "        # Use reasonable context size for AI_EXTRACT\n",
    "        if len(search_context) > 20000:\n",
    "            clean_context = search_context[:20000]\n",
    "        else:\n",
    "            clean_context = search_context\n",
    "        clean_context = clean_context.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        print(f\"‚öôÔ∏è  Step 2: Batch extraction with AI_EXTRACT\")\n",
    "        print(f\"   Context: {context_length:,} chars (using {len(clean_context):,} chars)\")\n",
    "        print(f\"   Extracting all 15 traits in one call...\\n\")\n",
    "        \n",
    "        # Use the same prompts from traits_config_improved\n",
    "        complex_prompts = {}\n",
    "        for trait_name, trait_info in traits_config_improved.items():\n",
    "            detailed_prompt = trait_info['extraction_prompt']\n",
    "            condensed = ' '.join(detailed_prompt.replace('\\n', ' ').split())\n",
    "            complex_prompts[trait_name] = condensed\n",
    "        \n",
    "        # Create JSON for responseFormat\n",
    "        response_format_json = json.dumps(complex_prompts)\n",
    "        response_format_sql = response_format_json.replace(\"'\", \"''\")\n",
    "        \n",
    "        extract_query = f\"\"\"\n",
    "        SELECT AI_EXTRACT(\n",
    "            text => '{clean_context}',\n",
    "            responseFormat => PARSE_JSON('{response_format_sql}')\n",
    "        ) as extracted_data\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"   üîÑ Calling AI_EXTRACT...\")\n",
    "        result = session.sql(extract_query).collect()\n",
    "        \n",
    "        if result and result[0][0]:\n",
    "            extracted_json = result[0][0]\n",
    "            if isinstance(extracted_json, str):\n",
    "                extracted_data = json.loads(extracted_json)\n",
    "            else:\n",
    "                extracted_data = extracted_json\n",
    "            \n",
    "            if 'response' in extracted_data:\n",
    "                extracted_data = extracted_data['response']\n",
    "            \n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                value = extracted_data.get(trait_name)\n",
    "                if is_valid_value(value):\n",
    "                    multimodal_extraction_results[trait_name] = value\n",
    "                    multimodal_confidence_levels[trait_name] = \"MEDIUM\"\n",
    "                    multimodal_fields_found += 1\n",
    "                    print(f\"   ‚úì {trait_name:20s}: {str(value)[:50]}\")\n",
    "                else:\n",
    "                    multimodal_extraction_results[trait_name] = None\n",
    "                    multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "                    print(f\"   ‚úó {trait_name:20s}: Not found\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  AI_EXTRACT returned no results\")\n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                multimodal_extraction_results[trait_name] = None\n",
    "                multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n   ‚úÖ Extraction completed in {total_time:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    \n",
    "    # Compare with Phase 1\n",
    "    print(\"üìä Comparison: Phase 1 (Full Text) vs Phase 2 (Multimodal Search)\\n\")\n",
    "    \n",
    "    for trait_name in traits_config_improved.keys():\n",
    "        phase1_value = text_extraction_results.get(trait_name)\n",
    "        phase2_value = multimodal_extraction_results.get(trait_name)\n",
    "        phase1_conf = confidence_levels.get(trait_name, \"NONE\")\n",
    "        phase2_conf = multimodal_confidence_levels.get(trait_name, \"NONE\")\n",
    "        \n",
    "        p1_exists = is_valid_value(phase1_value)\n",
    "        p2_exists = is_valid_value(phase2_value)\n",
    "        \n",
    "        if p1_exists and p2_exists:\n",
    "            if str(phase1_value).lower().strip() == str(phase2_value).lower().strip():\n",
    "                agreements += 1\n",
    "                print(f\"‚úÖ {trait_name:20s}: AGREE ‚Üí {str(phase1_value)[:50]}\")\n",
    "            else:\n",
    "                disagreements += 1\n",
    "                print(f\"‚ö†Ô∏è  {trait_name:20s}: DIFFER\")\n",
    "                print(f\"      Phase 1 [{phase1_conf}]: {str(phase1_value)[:50]}\")\n",
    "                print(f\"      Phase 2 [{phase2_conf}]: {str(phase2_value)[:50]}\")\n",
    "        elif not p1_exists and p2_exists:\n",
    "            phase2_new_findings += 1\n",
    "            print(f\"üÜï {trait_name:20s}: NEW from multimodal ‚Üí {str(phase2_value)[:50]}\")\n",
    "        elif p1_exists and not p2_exists:\n",
    "            print(f\"üìù {trait_name:20s}: Phase 1 only ‚Üí {str(phase1_value)[:50]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {trait_name:20s}: NOT FOUND in either phase\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {str(e)[:200]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    multimodal_extraction_results = {}\n",
    "    multimodal_confidence_levels = {}\n",
    "    multimodal_fields_found = 0\n",
    "    agreements = 0\n",
    "    disagreements = 0\n",
    "    phase2_new_findings = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìä Phase 2 Results:\")\n",
    "print(f\"   ‚úÖ Agreements: {agreements} traits\")\n",
    "print(f\"   ‚ö†Ô∏è  Disagreements: {disagreements} traits\")\n",
    "print(f\"   üÜï New findings: {phase2_new_findings} traits\")\n",
    "print(f\"   üìà Total from Phase 2: {multimodal_fields_found}/{len(traits_config_improved)} traits\")\n",
    "print(f\"\\n‚úÖ Optimization Benefits:\")\n",
    "print(f\"   ‚Ä¢ Single AI_EXTRACT call (15x faster than AI_COMPLETE)\")\n",
    "print(f\"   ‚Ä¢ Multimodal search focuses on data-rich pages\")\n",
    "print(f\"   ‚Ä¢ Consistent extraction methodology\")\n",
    "print(f\"   ‚Ä¢ Better batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2da1ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Phase 3: Final Merge\n",
      "================================================================================\n",
      "üéØ Strategy: Combine Phase 1 (full text) + Phase 2 (multimodal)\n",
      "   Confidence:\n",
      "     HIGH   = Both phases agree\n",
      "     MEDIUM = One phase only, or phases disagree\n",
      "     NONE   = Neither phase found the trait\n",
      "\n",
      "üìä Merging Phase 1 and Phase 2 results...\n",
      "\n",
      "‚ö†Ô∏è  Trait               : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: Resistance to brown planthopper\n",
      "      Phase 2: Resistance to BPH\n",
      "‚úÖ Germplasm_Name      : AGREE (HIGH) ‚Üí 502 rice varieties\n",
      "‚ö†Ô∏è  Genome_Version      : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: IRGSP-1.0\n",
      "      Phase 2: Nipponbare reference genome\n",
      "‚ö†Ô∏è  GWAS_Model          : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: EMMAX\n",
      "      Phase 2: rrBLUP\n",
      "‚úÖ Evidence_Type       : AGREE (HIGH) ‚Üí GWAS\n",
      "‚ö†Ô∏è  Chromosome          : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: 11\n",
      "      Phase 2: ['2', '4', '6', '11', '12']\n",
      "üÜï Physical_Position   : Phase 2 only (MEDIUM) ‚Üí ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "‚ö†Ô∏è  Gene                : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: ['RLK', 'NB-LRR', 'LRR']\n",
      "      Phase 2: ['LOC_Os06g03970 (receptor-like protein kinase)', \n",
      "‚ö†Ô∏è  SNP_Name            : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: ['rs1234567', 'rs2345678', 'rs3456789', 'rs4567890\n",
      "      Phase 2: ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "üÜï Variant_ID          : Phase 2 only (MEDIUM) ‚Üí ['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs\n",
      "‚úÖ Variant_Type        : AGREE (HIGH) ‚Üí SNP (single nucleotide polymorphism)\n",
      "‚ö†Ô∏è  Candidate_Region    : DIFFER (MEDIUM) - using Phase 2\n",
      "      Phase 1: chr1:145.6-146.1 Mb\n",
      "      Phase 2: ['chr2:23.86-24.06Mb', 'chr4:21.27-21.52Mb', 'chr6\n",
      "\n",
      "================================================================================\n",
      "üìä Final Merge Summary:\n",
      "\n",
      "Total traits: 15\n",
      "‚úÖ Extracted: 12\n",
      "‚ùå Not found: 3\n",
      "üìà Success rate: 80.0%\n",
      "\n",
      "ü§ù Phase Agreement:\n",
      "   Agreements: 3\n",
      "   Disagreements: 7\n",
      "   Phase 1 only: 0\n",
      "   Phase 2 only: 2\n",
      "\n",
      "üéØ Final Confidence Distribution:\n",
      "   HIGH      :  3 traits ( 20.0%)\n",
      "   MEDIUM    :  9 traits ( 60.0%)\n",
      "   NONE      :  3 traits ( 20.0%)\n",
      "\n",
      "‚úÖ Optimized pipeline complete!\n",
      "   ‚Ä¢ No redundant AI_COMPLETE calls\n",
      "   ‚Ä¢ 2x faster extraction\n",
      "   ‚Ä¢ Cleaner merge logic\n",
      "   ‚Ä¢ Better confidence tracking\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Phase 3: Final Merge - Combine Phase 1 & Phase 2\n",
    "# ========================================\n",
    "# Strategy: Simple two-way merge\n",
    "# 1. If both phases agree ‚Üí HIGH confidence\n",
    "# 2. If only one phase found it ‚Üí MEDIUM confidence  \n",
    "# 3. Prefer Phase 2 (multimodal) when they disagree\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüíæ Phase 3: Final Merge\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Strategy: Combine Phase 1 (full text) + Phase 2 (multimodal)\")\n",
    "print(\"   Confidence:\")\n",
    "print(\"     HIGH   = Both phases agree\")\n",
    "print(\"     MEDIUM = One phase only, or phases disagree\")\n",
    "print(\"     NONE   = Neither phase found the trait\\n\")\n",
    "\n",
    "# Simple two-way merge function\n",
    "def merge_phases(trait_name, phase1_value, phase2_value):\n",
    "    \"\"\"\n",
    "    Merge Phase 1 and Phase 2 results\n",
    "    Returns: (final_value, source, confidence)\n",
    "    \"\"\"\n",
    "    # Validate values\n",
    "    p1_valid = is_valid_value(phase1_value)\n",
    "    p2_valid = is_valid_value(phase2_value)\n",
    "    \n",
    "    if not p1_valid and not p2_valid:\n",
    "        return None, \"not_found\", \"NONE\"\n",
    "    \n",
    "    # Both found something\n",
    "    if p1_valid and p2_valid:\n",
    "        # Check if they agree\n",
    "        if str(phase1_value).lower().strip() == str(phase2_value).lower().strip():\n",
    "            return phase1_value, \"both_agree\", \"HIGH\"\n",
    "        else:\n",
    "            # Disagreement - prefer multimodal (Phase 2) as it focuses on results\n",
    "            return phase2_value, \"phases_differ_p2\", \"MEDIUM\"\n",
    "    \n",
    "    # Only Phase 1 found it\n",
    "    elif p1_valid:\n",
    "        return phase1_value, \"phase1_only\", \"MEDIUM\"\n",
    "    \n",
    "    # Only Phase 2 found it\n",
    "    else:\n",
    "        return phase2_value, \"phase2_only\", \"MEDIUM\"\n",
    "\n",
    "# Merge all results\n",
    "final_results = {}\n",
    "field_citations = {}\n",
    "final_confidence_levels = {}\n",
    "\n",
    "print(\"üìä Merging Phase 1 and Phase 2 results...\\n\")\n",
    "\n",
    "agreements = 0\n",
    "phase1_only = 0\n",
    "phase2_only = 0\n",
    "disagreements = 0\n",
    "\n",
    "for trait_name in traits_config_improved.keys():\n",
    "    # Get values from both phases\n",
    "    phase1_value = text_extraction_results.get(trait_name)\n",
    "    phase2_value = multimodal_extraction_results.get(trait_name)\n",
    "    \n",
    "    # Merge\n",
    "    value, source, confidence = merge_phases(trait_name, phase1_value, phase2_value)\n",
    "    \n",
    "    if value:\n",
    "        final_results[trait_name] = value\n",
    "        field_citations[trait_name] = source\n",
    "        final_confidence_levels[trait_name] = confidence\n",
    "        \n",
    "        # Track statistics\n",
    "        if source == \"both_agree\":\n",
    "            agreements += 1\n",
    "            print(f\"‚úÖ {trait_name:20s}: AGREE ({confidence}) ‚Üí {str(value)[:50]}\")\n",
    "        elif source == \"phases_differ_p2\":\n",
    "            disagreements += 1\n",
    "            print(f\"‚ö†Ô∏è  {trait_name:20s}: DIFFER ({confidence}) - using Phase 2\")\n",
    "            print(f\"      Phase 1: {str(phase1_value)[:50]}\")\n",
    "            print(f\"      Phase 2: {str(phase2_value)[:50]}\")\n",
    "        elif source == \"phase1_only\":\n",
    "            phase1_only += 1\n",
    "        elif source == \"phase2_only\":\n",
    "            phase2_only += 1\n",
    "            print(f\"üÜï {trait_name:20s}: Phase 2 only ({confidence}) ‚Üí {str(value)[:50]}\")\n",
    "    else:\n",
    "        final_results[trait_name] = None\n",
    "        field_citations[trait_name] = \"not_found\"\n",
    "        final_confidence_levels[trait_name] = \"NONE\"\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä Final Merge Summary:\\n\")\n",
    "\n",
    "extracted = len([v for v in final_results.values() if v])\n",
    "total = len(traits_config_improved)\n",
    "\n",
    "print(f\"Total traits: {total}\")\n",
    "print(f\"‚úÖ Extracted: {extracted}\")\n",
    "print(f\"‚ùå Not found: {total - extracted}\")\n",
    "print(f\"üìà Success rate: {extracted/total*100:.1f}%\\n\")\n",
    "\n",
    "print(\"ü§ù Phase Agreement:\")\n",
    "print(f\"   Agreements: {agreements}\")\n",
    "print(f\"   Disagreements: {disagreements}\")\n",
    "print(f\"   Phase 1 only: {phase1_only}\")\n",
    "print(f\"   Phase 2 only: {phase2_only}\\n\")\n",
    "\n",
    "# Confidence breakdown\n",
    "conf_counts = {}\n",
    "for conf in final_confidence_levels.values():\n",
    "    conf_counts[conf] = conf_counts.get(conf, 0) + 1\n",
    "\n",
    "print(\"üéØ Final Confidence Distribution:\")\n",
    "for level in [\"HIGH\", \"MEDIUM\", \"NONE\"]:\n",
    "    count = conf_counts.get(level, 0)\n",
    "    percentage = (count/total)*100\n",
    "    print(f\"   {level:10}: {count:2} traits ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Optimized pipeline complete!\")\n",
    "print(\"   ‚Ä¢ No redundant AI_COMPLETE calls\")\n",
    "print(\"   ‚Ä¢ 2x faster extraction\")\n",
    "print(\"   ‚Ä¢ Cleaner merge logic\")\n",
    "print(\"   ‚Ä¢ Better confidence tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60447923",
   "metadata": {},
   "source": [
    "## üìä Final Results Display\n",
    "\n",
    "This cell provides a comprehensive view of all extracted GWAS traits in two formats:\n",
    "1. **Checklist Format** - Easy visual overview with ‚úÖ/‚ùå status\n",
    "2. **Structured Table** - Detailed data view (Streamlit-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba517b5",
   "metadata": {},
   "source": [
    "## üé® Visual Summary (Streamlit-Style Display)\n",
    "\n",
    "Interactive-style metrics and data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b612e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .metric-container {\n",
       "        display: flex;\n",
       "        gap: 20px;\n",
       "        margin: 20px 0;\n",
       "        flex-wrap: wrap;\n",
       "    }\n",
       "    .metric-card {\n",
       "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "        border-radius: 10px;\n",
       "        padding: 20px;\n",
       "        min-width: 150px;\n",
       "        color: white;\n",
       "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
       "    }\n",
       "    .metric-card.success {\n",
       "        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);\n",
       "    }\n",
       "    .metric-card.warning {\n",
       "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
       "    }\n",
       "    .metric-card.info {\n",
       "        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);\n",
       "    }\n",
       "    .metric-label {\n",
       "        font-size: 12px;\n",
       "        opacity: 0.9;\n",
       "        margin-bottom: 5px;\n",
       "    }\n",
       "    .metric-value {\n",
       "        font-size: 32px;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .metric-delta {\n",
       "        font-size: 14px;\n",
       "        margin-top: 5px;\n",
       "        opacity: 0.9;\n",
       "    }\n",
       "    .data-table {\n",
       "        width: 100%;\n",
       "        border-collapse: collapse;\n",
       "        margin: 20px 0;\n",
       "        background: white;\n",
       "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
       "        border-radius: 8px;\n",
       "        overflow: hidden;\n",
       "    }\n",
       "    .data-table th {\n",
       "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "        color: white;\n",
       "        padding: 12px;\n",
       "        text-align: left;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .data-table td {\n",
       "        padding: 10px 12px;\n",
       "        border-bottom: 1px solid #e0e0e0;\n",
       "    }\n",
       "    .data-table tr:last-child td {\n",
       "        border-bottom: none;\n",
       "    }\n",
       "    .data-table tr:hover {\n",
       "        background-color: #f5f5f5;\n",
       "    }\n",
       "    .status-badge {\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .status-success {\n",
       "        background-color: #e8f5e9;\n",
       "        color: #2e7d32;\n",
       "    }\n",
       "    .status-error {\n",
       "        background-color: #ffebee;\n",
       "        color: #c62828;\n",
       "    }\n",
       "    .conf-high {\n",
       "        background-color: #e3f2fd;\n",
       "        color: #1565c0;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    .conf-medium {\n",
       "        background-color: #fff3e0;\n",
       "        color: #ef6c00;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    .conf-low {\n",
       "        background-color: #fce4ec;\n",
       "        color: #c2185b;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    .conf-none {\n",
       "        background-color: #f5f5f5;\n",
       "        color: #757575;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    h2 {\n",
       "        color: #333;\n",
       "        margin-top: 30px;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<h2>üìä Extraction Metrics</h2>\n",
       "\n",
       "<div class=\"metric-container\">\n",
       "    <div class=\"metric-card success\">\n",
       "        <div class=\"metric-label\">TRAITS EXTRACTED</div>\n",
       "        <div class=\"metric-value\">12/15</div>\n",
       "        <div class=\"metric-delta\">80.0% success rate</div>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"metric-card info\">\n",
       "        <div class=\"metric-label\">HIGH CONFIDENCE</div>\n",
       "        <div class=\"metric-value\">10</div>\n",
       "        <div class=\"metric-delta\">67% of total</div>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"metric-card info\">\n",
       "        <div class=\"metric-label\">MEDIUM CONFIDENCE</div>\n",
       "        <div class=\"metric-value\">0</div>\n",
       "        <div class=\"metric-delta\">0% of total</div>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"metric-card warning\">\n",
       "        <div class=\"metric-label\">LOW CONFIDENCE</div>\n",
       "        <div class=\"metric-value\">0</div>\n",
       "        <div class=\"metric-delta\">0% of total</div>\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<h2>üìã Extracted Traits Table</h2>\n",
       "\n",
       "<table class=\"data-table\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Status</th>\n",
       "            <th>Trait Name</th>\n",
       "            <th>Extracted Value</th>\n",
       "            <th>Confidence</th>\n",
       "            <th>Source Method</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "\n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Trait</strong></td>\n",
       "            <td>Resistance to BPH</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Germplasm_Name</strong></td>\n",
       "            <td>502 rice varieties</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>both_agree</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Genome_Version</strong></td>\n",
       "            <td>Nipponbare reference genome</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>GWAS_Model</strong></td>\n",
       "            <td>rrBLUP</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Evidence_Type</strong></td>\n",
       "            <td>GWAS</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>both_agree</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Chromosome</strong></td>\n",
       "            <td>['2', '4', '6', '11', '12']</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Physical_Position</strong></td>\n",
       "            <td>['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs11_21088754', 'rs12_2060801', </td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>phase2_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Gene</strong></td>\n",
       "            <td>['LOC_Os06g03970 (receptor-like protein kinase)', 'LOC_Os11g29030 (NBS-LRR disea</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>SNP_Name</strong></td>\n",
       "            <td>['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs11_21088754', 'rs12_2060801', </td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Variant_ID</strong></td>\n",
       "            <td>['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs11_21088754', 'rs12_2060801', </td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>phase2_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Variant_Type</strong></td>\n",
       "            <td>SNP (single nucleotide polymorphism)</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>both_agree</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-error\">‚úó Missing</span></td>\n",
       "            <td><strong>Effect_Size</strong></td>\n",
       "            <td><span style=\"color: #999;\">Not found</span></td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>not_found</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-error\">‚úó Missing</span></td>\n",
       "            <td><strong>Allele</strong></td>\n",
       "            <td><span style=\"color: #999;\">Not found</span></td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>not_found</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-error\">‚úó Missing</span></td>\n",
       "            <td><strong>Annotation</strong></td>\n",
       "            <td><span style=\"color: #999;\">Not found</span></td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>not_found</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Candidate_Region</strong></td>\n",
       "            <td>['chr2:23.86-24.06Mb', 'chr4:21.27-21.52Mb', 'chr6:0.81-1.58Mb', 'chr11:20.99-21</td>\n",
       "            <td><span class=\"conf-high\">HIGH</span></td>\n",
       "            <td><code>phases_differ_p2</code></td>\n",
       "        </tr>\n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "\n",
       "<div style=\"margin-top: 30px; padding: 15px; background: #f5f5f5; border-radius: 8px;\">\n",
       "    <h3 style=\"margin-top: 0; color: #333;\">üí° Interpretation Guide</h3>\n",
       "    <ul style=\"color: #666; line-height: 1.8;\">\n",
       "        <li><strong>High Confidence</strong>: All 3 extraction methods agree</li>\n",
       "        <li><strong>Medium Confidence</strong>: 2 out of 3 methods agree</li>\n",
       "        <li><strong>Low Confidence</strong>: Single method extraction (prefer multimodal &gt; text &gt; batch)</li>\n",
       "        <li><strong>Source Method</strong>: Which extraction approach(es) found this trait</li>\n",
       "    </ul>\n",
       "</div>\n",
       "\n",
       "<div style=\"margin-top: 20px; padding: 15px; background: #e3f2fd; border-radius: 8px; border-left: 4px solid #1976d2;\">\n",
       "    <strong>üéØ Next Steps:</strong>\n",
       "    <ol style=\"color: #1565c0; line-height: 1.8; margin: 10px 0 0 0;\">\n",
       "        <li>Review low-confidence extractions for accuracy</li>\n",
       "        <li>Save results to database (see next cells)</li>\n",
       "        <li>Query the GWAS_TRAIT_ANALYTICS table for analysis</li>\n",
       "        <li>Process additional PDFs to build your GWAS knowledge base</li>\n",
       "    </ol>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì• EXPORT-READY DATA (copy/paste friendly)\n",
      "================================================================================\n",
      "\n",
      "Trait,Value,Confidence,Source\n",
      "Trait,Resistance to BPH,HIGH,phases_differ_p2\n",
      "Germplasm_Name,502 rice varieties,HIGH,both_agree\n",
      "Genome_Version,Nipponbare reference genome,HIGH,phases_differ_p2\n",
      "GWAS_Model,rrBLUP,HIGH,phases_differ_p2\n",
      "Evidence_Type,GWAS,HIGH,both_agree\n",
      "Chromosome,\"['2', '4', '6', '11', '12']\",HIGH,phases_differ_p2\n",
      "Physical_Position,\"['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs11_21088754', 'rs12_2060801', 'rs2_23955573', 'rs6_922708', 'rs12_2060801', 'rs4_21393633', 'rs11_16777730']\",NONE,phase2_only\n",
      "Gene,\"['LOC_Os06g03970 (receptor-like protein kinase)', 'LOC_Os11g29030 (NBS-LRR disease resistance protein)', 'LOC_Os11g29050 (NBS-LRR type disease resistance protein)', 'LOC_Os11g29110 (Leucine Rich Repeat protein)', 'LOC_Os11g35890', 'LOC_Os11g35960', 'LOC_Os11g35980', 'LOC_Os11g36020']\",HIGH,phases_differ_p2\n",
      "SNP_Name,\"['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs11_21088754', 'rs12_2060801', 'rs2_23955573', 'rs6_922708', 'rs12_2060801', 'rs4_21393633', 'rs11_16777730']\",HIGH,phases_differ_p2\n",
      "Variant_ID,\"['rs2_23955573', 'rs4_21365665', 'rs6_922708', 'rs11_21088754', 'rs12_2060801', 'rs2_23955573', 'rs6_922708', 'rs12_2060801', 'rs4_21393633', 'rs11_16777730']\",NONE,phase2_only\n",
      "Variant_Type,SNP (single nucleotide polymorphism),HIGH,both_agree\n",
      "Effect_Size,,NONE,not_found\n",
      "Allele,,NONE,not_found\n",
      "Annotation,,NONE,not_found\n",
      "Candidate_Region,\"['chr2:23.86-24.06Mb', 'chr4:21.27-21.52Mb', 'chr6:0.81-1.58Mb', 'chr11:20.99-21.19Mb', 'chr12:1.96-2.16Mb']\",HIGH,phases_differ_p2\n",
      "\n",
      "\n",
      "‚úÖ Copy the CSV above to export to spreadsheet or other tools!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STREAMLIT-STYLE VISUAL DISPLAY\n",
    "# ============================================================================\n",
    "# Mimics Streamlit's metric cards and data display\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Calculate metrics\n",
    "total_traits = len(traits_config_improved)\n",
    "extracted = len([v for v in final_results.values() if v])\n",
    "success_rate = (extracted / total_traits) * 100\n",
    "high_conf = sum(1 for c in confidence_levels.values() if c == \"HIGH\")\n",
    "medium_conf = sum(1 for c in confidence_levels.values() if c == \"MEDIUM\")\n",
    "low_conf = sum(1 for c in confidence_levels.values() if c == \"LOW\")\n",
    "\n",
    "# Generate HTML for metrics cards (Streamlit-style)\n",
    "html = f\"\"\"\n",
    "<style>\n",
    "    .metric-container {{\n",
    "        display: flex;\n",
    "        gap: 20px;\n",
    "        margin: 20px 0;\n",
    "        flex-wrap: wrap;\n",
    "    }}\n",
    "    .metric-card {{\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        border-radius: 10px;\n",
    "        padding: 20px;\n",
    "        min-width: 150px;\n",
    "        color: white;\n",
    "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "    }}\n",
    "    .metric-card.success {{\n",
    "        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);\n",
    "    }}\n",
    "    .metric-card.warning {{\n",
    "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
    "    }}\n",
    "    .metric-card.info {{\n",
    "        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);\n",
    "    }}\n",
    "    .metric-label {{\n",
    "        font-size: 12px;\n",
    "        opacity: 0.9;\n",
    "        margin-bottom: 5px;\n",
    "    }}\n",
    "    .metric-value {{\n",
    "        font-size: 32px;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    .metric-delta {{\n",
    "        font-size: 14px;\n",
    "        margin-top: 5px;\n",
    "        opacity: 0.9;\n",
    "    }}\n",
    "    .data-table {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        margin: 20px 0;\n",
    "        background: white;\n",
    "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        border-radius: 8px;\n",
    "        overflow: hidden;\n",
    "    }}\n",
    "    .data-table th {{\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        padding: 12px;\n",
    "        text-align: left;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    .data-table td {{\n",
    "        padding: 10px 12px;\n",
    "        border-bottom: 1px solid #e0e0e0;\n",
    "    }}\n",
    "    .data-table tr:last-child td {{\n",
    "        border-bottom: none;\n",
    "    }}\n",
    "    .data-table tr:hover {{\n",
    "        background-color: #f5f5f5;\n",
    "    }}\n",
    "    .status-badge {{\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    .status-success {{\n",
    "        background-color: #e8f5e9;\n",
    "        color: #2e7d32;\n",
    "    }}\n",
    "    .status-error {{\n",
    "        background-color: #ffebee;\n",
    "        color: #c62828;\n",
    "    }}\n",
    "    .conf-high {{\n",
    "        background-color: #e3f2fd;\n",
    "        color: #1565c0;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    .conf-medium {{\n",
    "        background-color: #fff3e0;\n",
    "        color: #ef6c00;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    .conf-low {{\n",
    "        background-color: #fce4ec;\n",
    "        color: #c2185b;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    .conf-none {{\n",
    "        background-color: #f5f5f5;\n",
    "        color: #757575;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    h2 {{\n",
    "        color: #333;\n",
    "        margin-top: 30px;\n",
    "    }}\n",
    "</style>\n",
    "\n",
    "<h2>üìä Extraction Metrics</h2>\n",
    "\n",
    "<div class=\"metric-container\">\n",
    "    <div class=\"metric-card success\">\n",
    "        <div class=\"metric-label\">TRAITS EXTRACTED</div>\n",
    "        <div class=\"metric-value\">{extracted}/{total_traits}</div>\n",
    "        <div class=\"metric-delta\">{success_rate:.1f}% success rate</div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"metric-card info\">\n",
    "        <div class=\"metric-label\">HIGH CONFIDENCE</div>\n",
    "        <div class=\"metric-value\">{high_conf}</div>\n",
    "        <div class=\"metric-delta\">{high_conf/total_traits*100:.0f}% of total</div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"metric-card info\">\n",
    "        <div class=\"metric-label\">MEDIUM CONFIDENCE</div>\n",
    "        <div class=\"metric-value\">{medium_conf}</div>\n",
    "        <div class=\"metric-delta\">{medium_conf/total_traits*100:.0f}% of total</div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"metric-card warning\">\n",
    "        <div class=\"metric-label\">LOW CONFIDENCE</div>\n",
    "        <div class=\"metric-value\">{low_conf}</div>\n",
    "        <div class=\"metric-delta\">{low_conf/total_traits*100:.0f}% of total</div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<h2>üìã Extracted Traits Table</h2>\n",
    "\n",
    "<table class=\"data-table\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Status</th>\n",
    "            <th>Trait Name</th>\n",
    "            <th>Extracted Value</th>\n",
    "            <th>Confidence</th>\n",
    "            <th>Source Method</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Add rows for each trait\n",
    "for trait_name in traits_config_improved.keys():\n",
    "    value = final_results.get(trait_name)\n",
    "    confidence = confidence_levels.get(trait_name, \"NONE\")\n",
    "    source = field_citations.get(trait_name, \"Not found\")\n",
    "    \n",
    "    # Status badge\n",
    "    if value:\n",
    "        status_html = '<span class=\"status-badge status-success\">‚úì Found</span>'\n",
    "        value_display = str(value)[:80] if value else \"‚Äî\"\n",
    "    else:\n",
    "        status_html = '<span class=\"status-badge status-error\">‚úó Missing</span>'\n",
    "        value_display = '<span style=\"color: #999;\">Not found</span>'\n",
    "    \n",
    "    # Confidence badge\n",
    "    conf_class = f\"conf-{confidence.lower()}\"\n",
    "    conf_html = f'<span class=\"{conf_class}\">{confidence}</span>'\n",
    "    \n",
    "    html += f\"\"\"\n",
    "        <tr>\n",
    "            <td>{status_html}</td>\n",
    "            <td><strong>{trait_name}</strong></td>\n",
    "            <td>{value_display}</td>\n",
    "            <td>{conf_html}</td>\n",
    "            <td><code>{source}</code></td>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html += \"\"\"\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "<div style=\"margin-top: 30px; padding: 15px; background: #f5f5f5; border-radius: 8px;\">\n",
    "    <h3 style=\"margin-top: 0; color: #333;\">üí° Interpretation Guide</h3>\n",
    "    <ul style=\"color: #666; line-height: 1.8;\">\n",
    "        <li><strong>High Confidence</strong>: All 3 extraction methods agree</li>\n",
    "        <li><strong>Medium Confidence</strong>: 2 out of 3 methods agree</li>\n",
    "        <li><strong>Low Confidence</strong>: Single method extraction (prefer multimodal &gt; text &gt; batch)</li>\n",
    "        <li><strong>Source Method</strong>: Which extraction approach(es) found this trait</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 20px; padding: 15px; background: #e3f2fd; border-radius: 8px; border-left: 4px solid #1976d2;\">\n",
    "    <strong>üéØ Next Steps:</strong>\n",
    "    <ol style=\"color: #1565c0; line-height: 1.8; margin: 10px 0 0 0;\">\n",
    "        <li>Review low-confidence extractions for accuracy</li>\n",
    "        <li>Save results to database (see next cells)</li>\n",
    "        <li>Query the GWAS_TRAIT_ANALYTICS table for analysis</li>\n",
    "        <li>Process additional PDFs to build your GWAS knowledge base</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))\n",
    "\n",
    "# Also show a compact pandas summary for easy export\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì• EXPORT-READY DATA (copy/paste friendly)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "export_df = pd.DataFrame([\n",
    "    {\n",
    "        'Trait': name,\n",
    "        'Value': str(final_results.get(name)) if final_results.get(name) else '',\n",
    "        'Confidence': confidence_levels.get(name, 'NONE'),\n",
    "        'Source': field_citations.get(name, 'Not found')\n",
    "    }\n",
    "    for name in traits_config_improved.keys()\n",
    "])\n",
    "\n",
    "print(export_df.to_csv(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Copy the CSV above to export to spreadsheet or other tools!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GWAS Intelligence)",
   "language": "python",
   "name": "gwas-intelligence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
