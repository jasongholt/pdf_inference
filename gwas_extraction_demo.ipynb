{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ GWAS Intelligence Pipeline - Standalone Notebook\n",
    "\n",
    "This notebook is a **complete, standalone** pipeline for extracting genomic trait data from research papers using Snowflake Cortex AI and multimodal RAG.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Database Setup** - Creates GWAS database, schemas, stages, and tables\n",
    "2. **PDF Processing** - Parses PDFs using Cortex AI\n",
    "3. **Embedding Generation** - Creates text and image embeddings\n",
    "4. **Trait Extraction** - Extracts GWAS traits using multimodal RAG\n",
    "5. **Analytics** - Provides extracted trait analytics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Snowflake account with Cortex AI access\n",
    "- CREATE DATABASE privileges\n",
    "- Warehouse for compute\n",
    "- `.env` file with credentials (see below)\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Configure `.env` file with your Snowflake credentials\n",
    "2. Upload a PDF to the stage (instructions in notebook)\n",
    "3. Run all cells in order\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# IMPORTS - Standalone Notebook Version\n",
    "# ============================================================================\n",
    "# This notebook is self-contained - all logic is inline, no external modules!\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PDF processing\n",
    "import fitz  # PyMuPDF - for PDF to PNG conversion\n",
    "\n",
    "# Snowflake imports\n",
    "from snowflake.snowpark import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Project root\n",
    "project_root = Path().absolute()\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"\\nüì¶ Key packages loaded:\")\n",
    "print(f\"   ‚Ä¢ snowflake.snowpark\")\n",
    "print(f\"   ‚Ä¢ pandas {pd.__version__}\")\n",
    "print(f\"   ‚Ä¢ numpy {np.__version__}\")\n",
    "print(f\"   ‚Ä¢ PyMuPDF (fitz) {fitz.__version__}\")\n",
    "print(f\"\\nüéØ This is a standalone notebook - no external files required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "   Warehouse: DEMO_JGH\n",
      "   Database: GWAS\n",
      "   Schemas: PDF_RAW, PDF_PROCESSING\n",
      "\n",
      "‚úÖ Configuration set!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Update these settings for your environment\n",
    "# ============================================================================\n",
    "\n",
    "# Warehouse for compute (can be overridden by SNOWFLAKE_WAREHOUSE env var)\n",
    "WAREHOUSE_NAME = \"DEMO_JGH\"\n",
    "\n",
    "# Database and schema names\n",
    "DATABASE_NAME = \"GWAS\"\n",
    "SCHEMA_RAW = \"PDF_RAW\"\n",
    "SCHEMA_PROCESSING = \"PDF_PROCESSING\"\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"   Warehouse: {WAREHOUSE_NAME}\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Schemas: {SCHEMA_RAW}, {SCHEMA_PROCESSING}\")\n",
    "print(\"\\n‚úÖ Configuration set!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Step 1: Database & Schema Setup\n",
    "\n",
    "Create the GWAS database and required schemas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connected to Snowflake\n",
      "‚úÖ Database GWAS created/verified\n",
      "‚úÖ Schema PDF_RAW created/verified\n",
      "‚úÖ Schema PDF_PROCESSING created/verified\n",
      "\n",
      "üìä Available schemas in GWAS:\n",
      "   - INFORMATION_SCHEMA\n",
      "   - PDF_PROCESSING\n",
      "   - PDF_RAW\n",
      "   - PUBLIC\n",
      "\n",
      "‚úÖ Database and schemas ready!\n"
     ]
    }
   ],
   "source": [
    "# Create database and schemas\n",
    "from snowflake.snowpark import Session\n",
    "import os\n",
    "\n",
    "# Get connection from environment or use defaults\n",
    "session = Session.builder.configs({\n",
    "    \"account\": os.environ.get(\"SNOWFLAKE_ACCOUNT\", \"\"),\n",
    "    \"user\": os.environ.get(\"SNOWFLAKE_USER\", \"\"),\n",
    "    \"password\": os.environ.get(\"SNOWFLAKE_PASSWORD\", \"\"),\n",
    "    \"role\": \"ACCOUNTADMIN\",\n",
    "    \"warehouse\": os.environ.get(\"SNOWFLAKE_WAREHOUSE\", WAREHOUSE_NAME),\n",
    "}).create()\n",
    "\n",
    "print(\"üîå Connected to Snowflake\")\n",
    "\n",
    "# Create database\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\").collect()\n",
    "print(f\"‚úÖ Database {DATABASE_NAME} created/verified\")\n",
    "\n",
    "# Use database\n",
    "session.sql(f\"USE DATABASE {DATABASE_NAME}\").collect()\n",
    "\n",
    "# Create schemas\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {SCHEMA_RAW}\n",
    "    COMMENT = 'Raw PDF data from AI_PARSE_DOCUMENT'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Schema {SCHEMA_RAW} created/verified\")\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {SCHEMA_PROCESSING}\n",
    "    COMMENT = 'Processed PDF data, embeddings, and analytics'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Schema {SCHEMA_PROCESSING} created/verified\")\n",
    "\n",
    "# Verify schemas exist\n",
    "schemas = session.sql(\"SHOW SCHEMAS\").collect()\n",
    "print(f\"\\nüìä Available schemas in {DATABASE_NAME}:\")\n",
    "for schema in schemas:\n",
    "    print(f\"   - {schema['name']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Database and schemas ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Create Stage\n",
    "\n",
    "Create stage for storing PDF files, extracted images, and text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage PDF_STAGE created/verified in GWAS.PDF_RAW\n",
      "\n",
      "üì¶ Available stages:\n",
      "   - PDF_STAGE\n",
      "\n",
      "üí° Upload PDFs using:\n",
      "   PUT file:///path/to/file.pdf @GWAS.PDF_RAW.PDF_STAGE/\n",
      "\n",
      "‚úÖ Stage ready!\n"
     ]
    }
   ],
   "source": [
    "# Create stage for PDF and asset storage\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "\n",
    "session.sql(f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS PDF_STAGE\n",
    "    DIRECTORY = (ENABLE = TRUE)\n",
    "    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n",
    "    COMMENT = 'Storage for PDF files, extracted images, and text'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Stage PDF_STAGE created/verified in {DATABASE_NAME}.{SCHEMA_RAW}\")\n",
    "\n",
    "# Verify stage exists\n",
    "stages = session.sql(\"SHOW STAGES\").collect()\n",
    "print(f\"\\nüì¶ Available stages:\")\n",
    "for stage in stages:\n",
    "    print(f\"   - {stage['name']}\")\n",
    "\n",
    "print(f\"\\nüí° Upload PDFs using:\")\n",
    "print(f\"   PUT file:///path/to/file.pdf @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\")\n",
    "\n",
    "print(\"\\n‚úÖ Stage ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Create Tables\n",
    "\n",
    "Create all tables needed for the GWAS extraction pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table PARSED_DOCUMENTS created in GWAS.PDF_RAW\n"
     ]
    }
   ],
   "source": [
    "# Create PARSED_DOCUMENTS table in PDF_RAW schema\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS PARSED_DOCUMENTS (\n",
    "        document_id VARCHAR PRIMARY KEY,\n",
    "        file_path VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        parsed_content VARIANT NOT NULL,\n",
    "        total_pages INTEGER,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "    COMMENT = 'Raw PDF data from Cortex AI_PARSE_DOCUMENT'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table PARSED_DOCUMENTS created in {DATABASE_NAME}.{SCHEMA_RAW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table TEXT_PAGES created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create TEXT_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(f\"USE SCHEMA {SCHEMA_PROCESSING}\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS TEXT_PAGES (\n",
    "        page_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        page_text TEXT,\n",
    "        word_count INTEGER,\n",
    "        text_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Page text with embeddings for semantic search'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table TEXT_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table IMAGE_PAGES created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create IMAGE_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS IMAGE_PAGES (\n",
    "        image_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        image_file_path VARCHAR NOT NULL,\n",
    "        image_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        dpi INTEGER DEFAULT 300,\n",
    "        image_format VARCHAR(10) DEFAULT 'PNG',\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Page images metadata for multimodal processing'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table IMAGE_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table MULTIMODAL_PAGES created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create MULTIMODAL_PAGES table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS MULTIMODAL_PAGES (\n",
    "        page_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        page_number INTEGER NOT NULL,\n",
    "        image_id VARCHAR,\n",
    "        page_text TEXT,\n",
    "        image_path VARCHAR,\n",
    "        text_embedding VECTOR(FLOAT, 1024),\n",
    "        image_embedding VECTOR(FLOAT, 1024),\n",
    "        embedding_model VARCHAR(100),\n",
    "        has_text BOOLEAN DEFAULT FALSE,\n",
    "        has_image BOOLEAN DEFAULT FALSE,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, page_number)\n",
    "    )\n",
    "    COMMENT = 'Combined text + image embeddings for multimodal RAG'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table MULTIMODAL_PAGES created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table GWAS_TRAIT_ANALYTICS created in GWAS.PDF_PROCESSING\n"
     ]
    }
   ],
   "source": [
    "# Create GWAS_TRAIT_ANALYTICS table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS GWAS_TRAIT_ANALYTICS (\n",
    "        analytics_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        file_name VARCHAR NOT NULL,\n",
    "        extraction_version VARCHAR(50),\n",
    "        finding_number INTEGER DEFAULT 1,\n",
    "        \n",
    "        -- Genomic traits\n",
    "        trait VARCHAR(500),\n",
    "        germplasm_name VARCHAR(500),\n",
    "        genome_version VARCHAR(100),\n",
    "        chromosome VARCHAR(50),\n",
    "        physical_position VARCHAR(200),\n",
    "        gene VARCHAR(500),\n",
    "        snp_name VARCHAR(200),\n",
    "        variant_id VARCHAR(200),\n",
    "        variant_type VARCHAR(100),\n",
    "        effect_size VARCHAR(200),\n",
    "        gwas_model VARCHAR(200),\n",
    "        evidence_type VARCHAR(100),\n",
    "        allele VARCHAR(100),\n",
    "        annotation TEXT,\n",
    "        candidate_region VARCHAR(500),\n",
    "        \n",
    "        -- Metadata\n",
    "        extraction_source VARCHAR(50),\n",
    "        field_citations VARIANT,\n",
    "        field_confidence VARIANT,\n",
    "        field_raw_values VARIANT,\n",
    "        traits_extracted INTEGER,\n",
    "        traits_not_reported INTEGER,\n",
    "        extraction_accuracy_pct FLOAT,\n",
    "        \n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),\n",
    "        UNIQUE (document_id, extraction_version, finding_number)\n",
    "    )\n",
    "    COMMENT = 'Extracted GWAS trait data from research papers'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table GWAS_TRAIT_ANALYTICS created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table GWAS_TIEBREAKER_LOG created in GWAS.PDF_PROCESSING\n",
      "\n",
      "üìä Verifying tables...\n",
      "\n",
      "‚úÖ Tables in PDF_PROCESSING:\n",
      "   - GWAS_TIEBREAKER_LOG\n",
      "   - GWAS_TRAIT_ANALYTICS\n",
      "   - IMAGE_PAGES\n",
      "   - MULTIMODAL_PAGES\n",
      "   - TEXT_PAGES\n",
      "\n",
      "‚úÖ Tables in PDF_RAW:\n",
      "   - PARSED_DOCUMENTS\n",
      "\n",
      "üéâ All tables created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create GWAS_TIEBREAKER_LOG table in PDF_PROCESSING schema\n",
    "session.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS GWAS_TIEBREAKER_LOG (\n",
    "        log_id VARCHAR PRIMARY KEY DEFAULT UUID_STRING(),\n",
    "        document_id VARCHAR NOT NULL,\n",
    "        extraction_version VARCHAR(50),\n",
    "        finding_number INTEGER,\n",
    "        trait_name VARCHAR(200),\n",
    "        method_a_value VARCHAR(1000),\n",
    "        method_b_value VARCHAR(1000),\n",
    "        method_c_value VARCHAR(1000),\n",
    "        final_decision VARCHAR(1000),\n",
    "        reasoning TEXT,\n",
    "        confidence_score FLOAT,\n",
    "        created_at TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    "    )\n",
    "    COMMENT = 'LLM tiebreaker decisions when extraction methods disagree'\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"‚úÖ Table GWAS_TIEBREAKER_LOG created in {DATABASE_NAME}.{SCHEMA_PROCESSING}\")\n",
    "\n",
    "# Verify all tables created\n",
    "print(f\"\\nüìä Verifying tables...\")\n",
    "tables = session.sql(f\"SHOW TABLES IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\").collect()\n",
    "print(f\"\\n‚úÖ Tables in {SCHEMA_PROCESSING}:\")\n",
    "for table in tables:\n",
    "    print(f\"   - {table['name']}\")\n",
    "\n",
    "tables_raw = session.sql(f\"SHOW TABLES IN SCHEMA {DATABASE_NAME}.{SCHEMA_RAW}\").collect()\n",
    "print(f\"\\n‚úÖ Tables in {SCHEMA_RAW}:\")\n",
    "for table in tables_raw:\n",
    "    print(f\"   - {table['name']}\")\n",
    "\n",
    "print(\"\\nüéâ All tables created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì§ Step 4: Upload PDF to Stage\n",
    "\n",
    "**Upload your PDF file to the stage before proceeding.**\n",
    "\n",
    "### Option 1: Using SnowSQL (Command Line)\n",
    "```bash\n",
    "# From terminal\n",
    "snowsql -a YOUR_ACCOUNT -u YOUR_USER\n",
    "PUT file:///Users/jholt/Downloads/fpls-15-1373081.pdf @GWAS.PDF_RAW.PDF_STAGE/;\n",
    "```\n",
    "\n",
    "### Option 2: Using Python (Below)\n",
    "Run the cell below to upload from your local system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Found PDF: fpls-15-1373081.pdf (2.50 MB)\n",
      "\n",
      "üì§ Uploading to stage...\n",
      "‚úÖ PDF uploaded to @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf\n",
      "\n",
      "üìÇ Files in stage:\n",
      "   - pdf_stage/fpls-15-1373081.pdf\n"
     ]
    }
   ],
   "source": [
    "# Upload PDF from local system\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your PDF file\n",
    "PDF_LOCAL_PATH = \"/Users/jholt/Downloads/fpls-15-1373081.pdf\"\n",
    "\n",
    "# Verify file exists\n",
    "pdf_path = Path(PDF_LOCAL_PATH)\n",
    "if not pdf_path.exists():\n",
    "    print(f\"‚ùå File not found: {PDF_LOCAL_PATH}\")\n",
    "    print(\"   Update PDF_LOCAL_PATH to point to your PDF file\")\n",
    "else:\n",
    "    print(f\"üìÑ Found PDF: {pdf_path.name} ({pdf_path.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "    \n",
    "    # Upload to stage\n",
    "    print(f\"\\nüì§ Uploading to stage...\")\n",
    "    session.file.put(\n",
    "        str(pdf_path),\n",
    "        f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/\",\n",
    "        auto_compress=False,\n",
    "        overwrite=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ PDF uploaded to @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{pdf_path.name}\")\n",
    "    \n",
    "    # List files in stage to verify\n",
    "    print(f\"\\nüìÇ Files in stage:\")\n",
    "    files = session.sql(f\"LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\").collect()\n",
    "    for file in files:\n",
    "        print(f\"   - {file[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ CELL 1: Section 1 - Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "   Project root: /Users/jholt/gwas_intelligence/gwas_intelligence\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add scripts directory to path\n",
    "project_root = Path().absolute()\n",
    "sys.path.append(str(project_root / \"scripts\" / \"python\"))\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"   Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ CELL 5-6: Section 3 - List PDFs in Snowflake Stage\n",
    "\n",
    "- **Cell 5**: List available PDFs\n",
    "- **Cell 6**: Configure which PDF to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Selected PDF Configuration:\n",
      "   Filename: fpls-15-1373081.pdf\n",
      "   Document ID: fpls-15-1373081.pdf\n",
      "   Stage File Path: fpls-15-1373081.pdf\n",
      "   Full Stage Path: @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf\n",
      "\n",
      "üìÅ Output Structure:\n",
      "   Text:   @PDF_STAGE/fpls-15-1373081.pdf/pages_text/\n",
      "   Images: @PDF_STAGE/fpls-15-1373081.pdf/pages_images/\n",
      "\n",
      "‚úÖ Configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Update these for your PDF\n",
    "# ============================================================================\n",
    "\n",
    "# Test PDF: fpls-15-1373081.pdf (GWAS paper from Frontiers in Plant Science)\n",
    "# PDF is uploaded to root of stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{filename}\n",
    "\n",
    "PDF_FILENAME = \"fpls-15-1373081.pdf\"  # PDF filename as it exists in stage\n",
    "\n",
    "# Use filename as DOCUMENT_ID (keeps .pdf extension)\n",
    "DOCUMENT_ID = PDF_FILENAME\n",
    "\n",
    "# Stage paths\n",
    "STAGE_FILE_PATH = PDF_FILENAME  # PDF is at root of stage (no subdirectory)\n",
    "\n",
    "# Expected directory structure that will be created in stage:\n",
    "# @PDF_STAGE/\n",
    "#   ‚îî‚îÄ‚îÄ fpls-15-1373081.pdf/\n",
    "#       ‚îú‚îÄ‚îÄ pages_text/\n",
    "#       ‚îÇ   ‚îú‚îÄ‚îÄ page_001.txt\n",
    "#       ‚îÇ   ‚îú‚îÄ‚îÄ page_002.txt\n",
    "#       ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "#       ‚îî‚îÄ‚îÄ pages_images/\n",
    "#           ‚îú‚îÄ‚îÄ page_001.png\n",
    "#           ‚îú‚îÄ‚îÄ page_002.png\n",
    "#           ‚îî‚îÄ‚îÄ ...\n",
    "\n",
    "print(f\"üìã Selected PDF Configuration:\")\n",
    "print(f\"   Filename: {PDF_FILENAME}\")\n",
    "print(f\"   Document ID: {DOCUMENT_ID}\")\n",
    "print(f\"   Stage File Path: {STAGE_FILE_PATH}\")\n",
    "print(f\"   Full Stage Path: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{STAGE_FILE_PATH}\")\n",
    "print(f\"\\nüìÅ Output Structure:\")\n",
    "print(f\"   Text:   @PDF_STAGE/{DOCUMENT_ID}/pages_text/\")\n",
    "print(f\"   Images: @PDF_STAGE/{DOCUMENT_ID}/pages_images/\")\n",
    "print(f\"\\n‚úÖ Configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ CELL 8-9: Section 4 - Parse PDF with AI_PARSE_DOCUMENT\n",
    "\n",
    "- **Cell 8**: Parse PDF using Snowflake Cortex AI\n",
    "- **Cell 9**: Convert PDF pages to PNG images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Parsing PDF from stage\n",
      "\n",
      "   Stage: @GWAS.PDF_RAW.PDF_STAGE\n",
      "   File: fpls-15-1373081.pdf\n",
      "\n",
      "üìã Using AI_PARSE_DOCUMENT with LAYOUT mode\n",
      "   - High-fidelity extraction optimized for complex documents\n",
      "   - Preserves structure: tables, headers, reading order\n",
      "   - page_split: true (processes each page separately)\n",
      "   - Returns Markdown-formatted content\n",
      "\n",
      "‚è±Ô∏è  This may take 30-60+ seconds for a 15-page PDF...\n",
      "   (AI_PARSE_DOCUMENT is processing your document in Snowflake)\n",
      "\n",
      "üîÑ Calling AI_PARSE_DOCUMENT... (please wait)\n",
      "\n",
      "‚úÖ PDF parsed successfully in 32.2 seconds!\n",
      "\n",
      "üìÑ Parsed Document Info:\n",
      "   Document ID: fpls-15-1373081.pdf\n",
      "   Page Count: 14\n",
      "   Created: 2025-10-07 19:21:58.711000-07:00\n",
      "\n",
      "   First Page Preview (100 chars):\n",
      "   ## OPEN ACCESS\n",
      "\n",
      "EDITED BY\n",
      "Shengli Jing,\n",
      "Xinyang Normal University, China\n",
      "REVIEWED BY\n",
      "Lilin Yin,\n",
      "Huaz...\n"
     ]
    }
   ],
   "source": [
    "# Parse PDF using Snowflake Cortex AI_PARSE_DOCUMENT\n",
    "# Reference: https://docs.snowflake.com/en/user-guide/snowflake-cortex/parse-document\n",
    "import time\n",
    "\n",
    "print(f\"üîÑ Parsing PDF from stage\\n\")\n",
    "print(f\"   Stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE\")\n",
    "print(f\"   File: {STAGE_FILE_PATH}\\n\")\n",
    "\n",
    "# Pre-check: Is document already parsed?\n",
    "check_query = f\"\"\"\n",
    "SELECT document_id, total_pages, created_at\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "WHERE document_id = '{DOCUMENT_ID}'\n",
    "\"\"\"\n",
    "\n",
    "existing = session.sql(check_query).collect()\n",
    "\n",
    "if existing:\n",
    "    print(f\"‚ÑπÔ∏è  Document '{DOCUMENT_ID}' already parsed (skipping)\")\n",
    "    print(f\"   Parsed at: {existing[0][2]}\")\n",
    "    print(f\"   Total pages: {existing[0][1]}\")\n",
    "    print(f\"\\nüí° To re-parse, delete the record first:\")\n",
    "    print(f\"   DELETE FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\")\n",
    "    print(f\"   WHERE document_id = '{DOCUMENT_ID}';\\n\")\n",
    "else:\n",
    "    print(\"üìã Using AI_PARSE_DOCUMENT with LAYOUT mode\")\n",
    "    print(\"   - High-fidelity extraction optimized for complex documents\")\n",
    "    print(\"   - Preserves structure: tables, headers, reading order\")\n",
    "    print(\"   - page_split: true (processes each page separately)\")\n",
    "    print(\"   - Returns Markdown-formatted content\")\n",
    "    print(\"\\n‚è±Ô∏è  This may take 30-60+ seconds for a 15-page PDF...\")\n",
    "    print(\"   (AI_PARSE_DOCUMENT is processing your document in Snowflake)\\n\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    parse_query = f\"\"\"\n",
    "    INSERT INTO {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS \n",
    "        (document_id, file_path, file_name, parsed_content, total_pages)\n",
    "    SELECT\n",
    "        '{DOCUMENT_ID}' AS document_id,\n",
    "        '@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{STAGE_FILE_PATH}' AS file_path,\n",
    "        '{PDF_FILENAME}' AS file_name,\n",
    "        parsed_data AS parsed_content,\n",
    "        ARRAY_SIZE(parsed_data:pages) AS total_pages\n",
    "    FROM (\n",
    "        SELECT SNOWFLAKE.CORTEX.AI_PARSE_DOCUMENT(\n",
    "            TO_FILE('@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE', '{STAGE_FILE_PATH}'),\n",
    "            {{'mode': 'LAYOUT', 'page_split': true}}\n",
    "        ) AS parsed_data\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Calling AI_PARSE_DOCUMENT... (please wait)\")\n",
    "        session.sql(parse_query).collect()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ PDF parsed successfully in {elapsed:.1f} seconds!\\n\")\n",
    "        \n",
    "        # Verify parsing\n",
    "        result = session.sql(check_query).collect()\n",
    "        if result:\n",
    "            print(f\"üìÑ Parsed Document Info:\")\n",
    "            print(f\"   Document ID: {result[0][0]}\")\n",
    "            print(f\"   Page Count: {result[0][1]}\")\n",
    "            print(f\"   Created: {result[0][2]}\")\n",
    "            \n",
    "            # Get first page preview\n",
    "            preview_query = f\"\"\"\n",
    "            SELECT parsed_content:pages[0]:content::VARCHAR as first_page\n",
    "            FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS\n",
    "            WHERE document_id = '{DOCUMENT_ID}'\n",
    "            \"\"\"\n",
    "            preview = session.sql(preview_query).collect()\n",
    "            if preview and preview[0][0]:\n",
    "                print(f\"\\n   First Page Preview (100 chars):\")\n",
    "                print(f\"   {preview[0][0][:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n‚ùå Error after {elapsed:.1f} seconds: {error_msg[:300]}\\n\")\n",
    "        \n",
    "        # Helpful debugging\n",
    "        if \"does not exist\" in error_msg.lower():\n",
    "            print(\"üí° File not found in stage. Check:\")\n",
    "            print(f\"   LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE;\")\n",
    "        elif \"timeout\" in error_msg.lower():\n",
    "            print(\"üí° Query timeout. Try:\")\n",
    "            print(\"   - Smaller PDF\")\n",
    "            print(\"   - Increase statement timeout\")\n",
    "        else:\n",
    "            print(\"üí° Full error:\")\n",
    "            print(f\"   {error_msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è  Creating PNG images from PDF pages\n",
      "\n",
      "   PDF: fpls-15-1373081.pdf\n",
      "   Document ID: fpls-15-1373081.pdf\n",
      "\n",
      "üì• Step 1: Downloading PDF from stage...\n",
      "   ‚úÖ Downloaded: fpls-15-1373081.pdf\n",
      "\n",
      "üîÑ Step 2: Converting PDF pages to PNG images...\n",
      "   PDF has 14 pages\n",
      "   ‚úì Converted page 1/14\n",
      "   ‚úì Converted page 2/14\n",
      "   ‚úì Converted page 3/14\n",
      "   ‚úì Converted page 4/14\n",
      "   ‚úì Converted page 5/14\n",
      "   ‚úì Converted page 6/14\n",
      "   ‚úì Converted page 7/14\n",
      "   ‚úì Converted page 8/14\n",
      "   ‚úì Converted page 9/14\n",
      "   ‚úì Converted page 10/14\n",
      "   ‚úì Converted page 11/14\n",
      "   ‚úì Converted page 12/14\n",
      "   ‚úì Converted page 13/14\n",
      "   ‚úì Converted page 14/14\n",
      "   ‚úÖ Created 14 PNG images\n",
      "\n",
      "üì§ Step 3: Uploading PNG images to stage...\n",
      "   Target: @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf/pages_images/\n",
      "   ‚úì Uploaded page 1/14\n",
      "   ‚úì Uploaded page 2/14\n",
      "   ‚úì Uploaded page 3/14\n",
      "   ‚úì Uploaded page 4/14\n",
      "   ‚úì Uploaded page 5/14\n",
      "   ‚úì Uploaded page 6/14\n",
      "   ‚úì Uploaded page 7/14\n",
      "   ‚úì Uploaded page 8/14\n",
      "   ‚úì Uploaded page 9/14\n",
      "   ‚úì Uploaded page 10/14\n",
      "   ‚úì Uploaded page 11/14\n",
      "   ‚úì Uploaded page 12/14\n",
      "   ‚úì Uploaded page 13/14\n",
      "   ‚úì Uploaded page 14/14\n",
      "   ‚úÖ Uploaded 14 images\n",
      "\n",
      "üíæ Step 4: Inserting IMAGE_PAGES records...\n",
      "   ‚úì Inserted record 1/14\n",
      "   ‚úì Inserted record 2/14\n",
      "   ‚úì Inserted record 3/14\n",
      "   ‚úì Inserted record 4/14\n",
      "   ‚úì Inserted record 5/14\n",
      "   ‚úì Inserted record 6/14\n",
      "   ‚úì Inserted record 7/14\n",
      "   ‚úì Inserted record 8/14\n",
      "   ‚úì Inserted record 9/14\n",
      "   ‚úì Inserted record 10/14\n",
      "   ‚úì Inserted record 11/14\n",
      "   ‚úì Inserted record 12/14\n",
      "   ‚úì Inserted record 13/14\n",
      "   ‚úì Inserted record 14/14\n",
      "   ‚úÖ Inserted 14 IMAGE_PAGES records\n",
      "\n",
      "üîç Step 5: Verifying stage structure...\n",
      "   ‚úÖ Found 14 files in stage\n",
      "   ‚úÖ Found 14 records in IMAGE_PAGES table\n",
      "\n",
      "üéâ SUCCESS! Converted 14 pages for fpls-15-1373081.pdf\n",
      "   Stage: @GWAS.PDF_RAW.PDF_STAGE/fpls-15-1373081.pdf/pages_images/\n",
      "   Database: GWAS.PDF_PROCESSING.IMAGE_PAGES\n",
      "\n",
      "üßπ Cleaned up temp directory\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE PNG IMAGES FROM PDF\n",
    "# Uses PyMuPDF to convert PDF pages to PNG, uploads to stage structure\n",
    "# NOTE: PyMuPDF requires local file access - we download, process, upload\n",
    "# ============================================================================\n",
    "\n",
    "# Required imports (in case Cell 1 wasn't run)\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: PyMuPDF (fitz) not installed!\")\n",
    "    print(\"   Install: pip install PyMuPDF\")\n",
    "    raise\n",
    "\n",
    "print(\"üñºÔ∏è  Creating PNG images from PDF pages\\n\")\n",
    "print(f\"   PDF: {STAGE_FILE_PATH}\")\n",
    "print(f\"   Document ID: {DOCUMENT_ID}\\n\")\n",
    "\n",
    "# Create temp directories\n",
    "temp_dir = Path(tempfile.mkdtemp())\n",
    "images_output = temp_dir / \"images\"\n",
    "images_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Step 1: Download PDF from stage (required for PyMuPDF)\n",
    "    print(\"üì• Step 1: Downloading PDF from stage...\")\n",
    "    \n",
    "    # Set database context for file operations\n",
    "    session.sql(f\"USE DATABASE {DATABASE_NAME}\").collect()\n",
    "    session.sql(f\"USE SCHEMA {SCHEMA_RAW}\").collect()\n",
    "    \n",
    "    # Download from stage\n",
    "    stage_path = f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{STAGE_FILE_PATH}\"\n",
    "    session.file.get(stage_path, str(temp_dir))\n",
    "    \n",
    "    # Find downloaded PDF\n",
    "    pdf_files = list(temp_dir.rglob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"PDF not downloaded: {PDF_FILENAME}\")\n",
    "    \n",
    "    local_pdf = pdf_files[0]\n",
    "    print(f\"   ‚úÖ Downloaded: {local_pdf.name}\\n\")\n",
    "    \n",
    "    # Step 2: Convert PDF pages to PNG using PyMuPDF\n",
    "    print(\"üîÑ Step 2: Converting PDF pages to PNG images...\")\n",
    "    doc = fitz.open(local_pdf)\n",
    "    page_count = len(doc)\n",
    "    print(f\"   PDF has {page_count} pages\")\n",
    "    \n",
    "    for page_num in range(page_count):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        \n",
    "        output_file = images_output / f\"page_{page_num:04d}.png\"\n",
    "        pix.save(output_file)\n",
    "        print(f\"   ‚úì Converted page {page_num + 1}/{page_count}\")\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"   ‚úÖ Created {page_count} PNG images\\n\")\n",
    "    \n",
    "    # Step 3: Upload PNGs to stage structure\n",
    "    print(\"üì§ Step 3: Uploading PNG images to stage...\")\n",
    "    stage_output = f\"@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\"\n",
    "    print(f\"   Target: {stage_output}\")\n",
    "    \n",
    "    for page_num in range(page_count):\n",
    "        local_image = images_output / f\"page_{page_num:04d}.png\"\n",
    "        session.file.put(\n",
    "            str(local_image),\n",
    "            stage_output,\n",
    "            auto_compress=False,\n",
    "            overwrite=True\n",
    "        )\n",
    "        print(f\"   ‚úì Uploaded page {page_num + 1}/{page_count}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Uploaded {page_count} images\\n\")\n",
    "    \n",
    "    # Step 4: Insert IMAGE_PAGES records into database\n",
    "    print(\"üíæ Step 4: Inserting IMAGE_PAGES records...\")\n",
    "    session.sql(f\"USE SCHEMA {SCHEMA_PROCESSING}\").collect()\n",
    "    \n",
    "    for page_num in range(page_count):\n",
    "        session.sql(f\"\"\"\n",
    "            INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES (\n",
    "                IMAGE_ID,\n",
    "                DOCUMENT_ID,\n",
    "                FILE_NAME,\n",
    "                PAGE_NUMBER,\n",
    "                IMAGE_FILE_PATH,\n",
    "                DPI,\n",
    "                IMAGE_FORMAT\n",
    "            )\n",
    "            SELECT\n",
    "                UUID_STRING(),\n",
    "                '{DOCUMENT_ID}',\n",
    "                '{PDF_FILENAME}',\n",
    "                {page_num},\n",
    "                '{stage_output}page_{page_num:04d}.png',\n",
    "                300,\n",
    "                'PNG'\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "                WHERE DOCUMENT_ID = '{DOCUMENT_ID}' \n",
    "                AND PAGE_NUMBER = {page_num}\n",
    "            )\n",
    "        \"\"\").collect()\n",
    "        print(f\"   ‚úì Inserted record {page_num + 1}/{page_count}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Inserted {page_count} IMAGE_PAGES records\\n\")\n",
    "    \n",
    "    # Step 5: Verify\n",
    "    print(\"üîç Step 5: Verifying stage structure...\")\n",
    "    verify_result = session.sql(f\"\"\"\n",
    "        LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\n",
    "    \"\"\").collect()\n",
    "    print(f\"   ‚úÖ Found {len(verify_result)} files in stage\")\n",
    "    \n",
    "    # Verify database\n",
    "    db_count = session.sql(f\"\"\"\n",
    "        SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "        WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "    \"\"\").collect()\n",
    "    print(f\"   ‚úÖ Found {db_count[0][0]} records in IMAGE_PAGES table\")\n",
    "    \n",
    "    print(f\"\\nüéâ SUCCESS! Converted {page_count} pages for {PDF_FILENAME}\")\n",
    "    print(f\"   Stage: @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\")\n",
    "    print(f\"   Database: {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # Cleanup temp directory\n",
    "    if 'temp_dir' in locals() and temp_dir.exists():\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"\\nüßπ Cleaned up temp directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù CELL 11: Section 5 - Extract Text Pages & Generate Embeddings\n",
    "\n",
    "Uses `snowflake-arctic-embed-l-v2.0-8k` model for text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting text pages and generating embeddings...\n",
      "\n",
      "üìã Text Embedding Model: snowflake-arctic-embed-l-v2.0-8k\n",
      "   - Dimensions: 1024\n",
      "   - Context length: 8K tokens\n",
      "   - Optimized for: Long-form documents\n",
      "\n",
      "‚úÖ Text pages extracted with embeddings!\n",
      "\n",
      "üìä Text Extraction Statistics:\n",
      "   Total pages: 14\n",
      "   Avg words/page: 678\n",
      "   Min words: 237\n",
      "   Max words: 1238\n",
      "   Pages with embeddings: 14\n",
      "\n",
      "üìÑ Sample Pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Text Preview</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWED BY\\nLilin...</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and feeds over half...</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>materials can be found in Supplementary Table 1. The majority of them were indica (227), followe...</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page  \\\n",
       "0     0   \n",
       "1     1   \n",
       "2     2   \n",
       "\n",
       "                                                                                          Text Preview  \\\n",
       "0  ## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWED BY\\nLilin...   \n",
       "1  Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and feeds over half...   \n",
       "2  materials can be found in Supplementary Table 1. The majority of them were indica (227), followe...   \n",
       "\n",
       "   Words  \n",
       "0    556  \n",
       "1   1002  \n",
       "2    826  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract text pages with embeddings using snowflake-arctic-embed-l-v2.0-8k\n",
    "print(\"üîÑ Extracting text pages and generating embeddings...\\n\")\n",
    "print(\"üìã Text Embedding Model: snowflake-arctic-embed-l-v2.0-8k\")\n",
    "print(\"   - Dimensions: 1024\")\n",
    "print(\"   - Context length: 8K tokens\")\n",
    "print(\"   - Optimized for: Long-form documents\\n\")\n",
    "\n",
    "# Insert text pages with embeddings\n",
    "text_extract_query = f\"\"\"\n",
    "INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES \n",
    "    (document_id, file_name, page_number, page_text, word_count, \n",
    "     text_embedding, embedding_model)\n",
    "SELECT\n",
    "    '{DOCUMENT_ID}' AS document_id,\n",
    "    '{PDF_FILENAME}' AS file_name,\n",
    "    page.index AS page_number,\n",
    "    page.value:content::STRING AS page_text,\n",
    "    ARRAY_SIZE(SPLIT(page.value:content::STRING, ' ')) AS word_count,\n",
    "    SNOWFLAKE.CORTEX.EMBED_TEXT_1024(\n",
    "        'snowflake-arctic-embed-l-v2.0-8k',\n",
    "        page.value:content::STRING\n",
    "    ) AS text_embedding,\n",
    "    'snowflake-arctic-embed-l-v2.0-8k' AS embedding_model\n",
    "FROM {DATABASE_NAME}.{SCHEMA_RAW}.PARSED_DOCUMENTS pd,\n",
    "LATERAL FLATTEN(input => pd.parsed_content:pages) page\n",
    "WHERE pd.document_id = '{DOCUMENT_ID}'\n",
    "AND NOT EXISTS (\n",
    "    SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES tp\n",
    "    WHERE tp.document_id = '{DOCUMENT_ID}' \n",
    "    AND tp.page_number = page.index\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(text_extract_query).collect()\n",
    "    print(\"‚úÖ Text pages extracted with embeddings!\\n\")\n",
    "    \n",
    "    # Get statistics\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as page_count,\n",
    "        AVG(word_count) as avg_words,\n",
    "        MIN(word_count) as min_words,\n",
    "        MAX(word_count) as max_words\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = session.sql(stats_query).collect()\n",
    "    if stats and stats[0][0] > 0:\n",
    "        print(f\"üìä Text Extraction Statistics:\")\n",
    "        print(f\"   Total pages: {stats[0][0]}\")\n",
    "        print(f\"   Avg words/page: {stats[0][1]:.0f}\")\n",
    "        print(f\"   Min words: {stats[0][2]}\")\n",
    "        print(f\"   Max words: {stats[0][3]}\")\n",
    "        \n",
    "        # Verify embeddings\n",
    "        embed_check = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES \n",
    "            WHERE document_id = '{DOCUMENT_ID}' AND text_embedding IS NOT NULL\n",
    "        \"\"\").collect()\n",
    "        print(f\"   Pages with embeddings: {embed_check[0][0]}\")\n",
    "        \n",
    "        # Show sample pages\n",
    "        sample_query = f\"\"\"\n",
    "        SELECT page_number, LEFT(page_text, 100) as preview, word_count\n",
    "        FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "        WHERE document_id = '{DOCUMENT_ID}'\n",
    "        ORDER BY page_number\n",
    "        LIMIT 3\n",
    "        \"\"\"\n",
    "        \n",
    "        samples = session.sql(sample_query).collect()\n",
    "        if samples:\n",
    "            print(f\"\\nüìÑ Sample Pages:\")\n",
    "            df_samples = pd.DataFrame(samples, columns=['Page', 'Text Preview', 'Words'])\n",
    "            display(df_samples)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è CELL 13-14: Section 6 - Create Image Pages\n",
    "\n",
    "- **Cell 13**: Debug - List files in stage\n",
    "- **Cell 14**: Generate image embeddings using `voyage-multimodal-3`\n",
    "\n",
    "**Purpose:** Create embeddings for PNG images to enable multimodal search (text + images).\n",
    "Images capture tables, charts, and figures that may contain GWAS data not easily extracted from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a18b84eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Listing files in stage...\n",
      "\n",
      "‚úÖ Found 14 files:\n",
      "\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0000.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0001.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0002.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0003.png\n",
      "   pdf_stage/fpls-15-1373081.pdf/pages_images/page_0004.png\n",
      "   ... and 9 more\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: List actual files in stage to verify paths\n",
    "print(\"üîç Listing files in stage...\\n\")\n",
    "\n",
    "list_query = f\"\"\"\n",
    "LIST @{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE/{DOCUMENT_ID}/pages_images/\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    files = session.sql(list_query).collect()\n",
    "    print(f\"‚úÖ Found {len(files)} files:\\n\")\n",
    "    for f in files[:5]:  # Show first 5\n",
    "        print(f\"   {f[0]}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"   ... and {len(files) - 5} more\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating image embeddings...\n",
      "\n",
      "üìã Image Embedding Model: voyage-multimodal-3 via AI_EMBED\n",
      "   - Dimensions: 1024\n",
      "   - Supports: Images + Text\n",
      "   - Use case: Visual understanding of tables, charts, figures\n",
      "\n",
      "üìä Found 14 IMAGE_PAGES records without embeddings\n",
      "   Processing 14 pages...\n",
      "\n",
      "   Page 0: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0000.png')\n",
      "   ‚úì Generated embedding (1/14)\n",
      "\n",
      "   Page 1: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0001.png')\n",
      "   ‚úì Generated embedding (2/14)\n",
      "\n",
      "   Page 2: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0002.png')\n",
      "   ‚úì Generated embedding (3/14)\n",
      "\n",
      "   Page 3: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0003.png')\n",
      "   ‚úì Generated embedding (4/14)\n",
      "\n",
      "   Page 4: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0004.png')\n",
      "   ‚úì Generated embedding (5/14)\n",
      "\n",
      "   Page 5: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0005.png')\n",
      "   ‚úì Generated embedding (6/14)\n",
      "\n",
      "   Page 6: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0006.png')\n",
      "   ‚úì Generated embedding (7/14)\n",
      "\n",
      "   Page 7: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0007.png')\n",
      "   ‚úì Generated embedding (8/14)\n",
      "\n",
      "   Page 8: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0008.png')\n",
      "   ‚úì Generated embedding (9/14)\n",
      "\n",
      "   Page 9: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0009.png')\n",
      "   ‚úì Generated embedding (10/14)\n",
      "\n",
      "   Page 10: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0010.png')\n",
      "   ‚úì Generated embedding (11/14)\n",
      "\n",
      "   Page 11: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0011.png')\n",
      "   ‚úì Generated embedding (12/14)\n",
      "\n",
      "   Page 12: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0012.png')\n",
      "   ‚úì Generated embedding (13/14)\n",
      "\n",
      "   Page 13: TO_FILE('@GWAS.PDF_RAW.PDF_STAGE', 'fpls-15-1373081.pdf/pages_images/page_0013.png')\n",
      "   ‚úì Generated embedding (14/14)\n",
      "\n",
      "‚úÖ Embedding generation complete!\n",
      "\n",
      "üìä Final Status:\n",
      "   Total records: 14\n",
      "   ‚úÖ With embeddings: 14\n",
      "   ‚ö†Ô∏è  Without embeddings: 0\n",
      "   üìà Ready for multimodal search: 14/14\n",
      "\n",
      "üéâ All image embeddings generated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate image embeddings for existing IMAGE_PAGES records\n",
    "# Uses voyage-multimodal-3 to create embeddings from PNGs in stage\n",
    "print(\"üîÑ Generating image embeddings...\\n\")\n",
    "print(\"üìã Image Embedding Model: voyage-multimodal-3 via AI_EMBED\")\n",
    "print(\"   - Dimensions: 1024\")\n",
    "print(\"   - Supports: Images + Text\")\n",
    "print(\"   - Use case: Visual understanding of tables, charts, figures\\n\")\n",
    "\n",
    "try:\n",
    "    # Get existing IMAGE_PAGES records without embeddings\n",
    "    check_query = f\"\"\"\n",
    "    SELECT \n",
    "        PAGE_NUMBER,\n",
    "        IMAGE_FILE_PATH,\n",
    "        COUNT(*) OVER() as total_records\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "    AND IMAGE_EMBEDDING IS NULL\n",
    "    ORDER BY PAGE_NUMBER\n",
    "    \"\"\"\n",
    "    \n",
    "    records = session.sql(check_query).collect()\n",
    "    \n",
    "    if not records:\n",
    "        print(\"‚ÑπÔ∏è  No records found without embeddings\")\n",
    "        \n",
    "        # Check if embeddings already exist\n",
    "        existing = session.sql(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "            WHERE DOCUMENT_ID = '{DOCUMENT_ID}' AND IMAGE_EMBEDDING IS NOT NULL\n",
    "        \"\"\").collect()\n",
    "        if existing and existing[0][0] > 0:\n",
    "            print(f\"   ‚úÖ {existing[0][0]} records already have embeddings!\\n\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  No IMAGE_PAGES records found - run Cell 9 first\\n\")\n",
    "    else:\n",
    "        total_records = records[0][2]\n",
    "        print(f\"üìä Found {total_records} IMAGE_PAGES records without embeddings\")\n",
    "        print(f\"   Processing {len(records)} pages...\\n\")\n",
    "        \n",
    "        # Update each record with embedding\n",
    "        for idx, record in enumerate(records, 1):\n",
    "            page_num = record[0]\n",
    "            image_path = record[1]\n",
    "            \n",
    "            # Parse the stored path\n",
    "            # Stored format: @PDF_STAGE/fpls-15-1373081.pdf/pages_images/page_0000.png\n",
    "            # Extract relative path (everything after first /)\n",
    "            \n",
    "            if image_path.startswith('@'):\n",
    "                # Split on first / after @\n",
    "                parts = image_path.split('/', 1)\n",
    "                if len(parts) == 2:\n",
    "                    relative_path = parts[1]  # fpls-15-1373081.pdf/pages_images/page_0000.png\n",
    "                else:\n",
    "                    relative_path = image_path\n",
    "            else:\n",
    "                # No @ prefix, use as-is\n",
    "                relative_path = image_path\n",
    "            \n",
    "            # Always use full stage name for TO_FILE\n",
    "            full_stage_name = f'@{DATABASE_NAME}.{SCHEMA_RAW}.PDF_STAGE'\n",
    "            \n",
    "            print(f\"   Page {page_num}: TO_FILE('{full_stage_name}', '{relative_path}')\")\n",
    "            \n",
    "            # Generate embedding and update record\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "            SET \n",
    "                IMAGE_EMBEDDING = AI_EMBED(\n",
    "                    'voyage-multimodal-3',\n",
    "                    TO_FILE('{full_stage_name}', '{relative_path}')\n",
    "                ),\n",
    "                EMBEDDING_MODEL = 'voyage-multimodal-3'\n",
    "            WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "            AND PAGE_NUMBER = {page_num}\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                session.sql(update_query).collect()\n",
    "                print(f\"   ‚úì Generated embedding ({idx}/{len(records)})\\n\")\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                print(f\"   ‚úó Failed: {error_msg[:200]}\\n\")\n",
    "                # Show full error for first failure\n",
    "                if idx == 1:\n",
    "                    print(f\"   Full error: {error_msg}\\n\")\n",
    "                    print(f\"   üí° Tip: Run the debug cell above (Cell 13) to verify files exist in stage\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Embedding generation complete!\\n\")\n",
    "    \n",
    "    # Verify final counts\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(IMAGE_EMBEDDING) as with_embeddings,\n",
    "        COUNT(CASE WHEN IMAGE_EMBEDDING IS NULL THEN 1 END) as without_embeddings\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES\n",
    "    WHERE DOCUMENT_ID = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    result = session.sql(verify_query).collect()\n",
    "    if result:\n",
    "        total, with_emb, without_emb = result[0]\n",
    "        print(f\"üìä Final Status:\")\n",
    "        print(f\"   Total records: {total}\")\n",
    "        print(f\"   ‚úÖ With embeddings: {with_emb}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Without embeddings: {without_emb}\")\n",
    "        print(f\"   üìà Ready for multimodal search: {with_emb}/{total}\")\n",
    "        \n",
    "        if with_emb == total and total > 0:\n",
    "            print(f\"\\nüéâ All image embeddings generated successfully!\")\n",
    "        elif without_emb > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  {without_emb} pages still need embeddings\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó CELL 16: Section 7 - Create Multimodal Pages\n",
    "\n",
    "Join text and image embeddings into a unified multimodal table for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating multimodal pages...\n",
      "\n",
      "üîó Joining text and image data by page_number\n",
      "   - Copies both text and image embeddings\n",
      "   - Enables unified multi-modal search\n",
      "\n",
      "‚úÖ Multimodal pages created!\n",
      "\n",
      "üìä Multimodal Pages Statistics:\n",
      "   Total pages: 14\n",
      "   Pages with text: 14\n",
      "   Pages with images: 14\n",
      "   Text embeddings: 14\n",
      "   Image embeddings: 14\n",
      "\n",
      "üìÑ Sample Pages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>Text Preview</th>\n",
       "      <th>Has Text</th>\n",
       "      <th>Has Image</th>\n",
       "      <th>Text Emb</th>\n",
       "      <th>Image Emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWE</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and f</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>materials can be found in Supplementary Table 1. The majority of them were indic</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Selection of marker subsets for genomic prediction\\n\\nThe genotype data utilized t</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td># Selection of training population subsets for genomic prediction\\n\\nIn order to i</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Page  \\\n",
       "0     0   \n",
       "1     1   \n",
       "2     2   \n",
       "3     3   \n",
       "4     4   \n",
       "\n",
       "                                                                            Text Preview  \\\n",
       "0  ## OPEN ACCESS\\n\\nEDITED BY\\nShengli Jing,\\nXinyang Normal University, China\\nREVIEWE   \n",
       "1     Introduction\\n\\nThe cultivated rice (Oryza sativa L.) is a major staple crop and f   \n",
       "2       materials can be found in Supplementary Table 1. The majority of them were indic   \n",
       "3     Selection of marker subsets for genomic prediction\\n\\nThe genotype data utilized t   \n",
       "4     # Selection of training population subsets for genomic prediction\\n\\nIn order to i   \n",
       "\n",
       "   Has Text  Has Image  Text Emb  Image Emb  \n",
       "0      True       True      True       True  \n",
       "1      True       True      True       True  \n",
       "2      True       True      True       True  \n",
       "3      True       True      True       True  \n",
       "4      True       True      True       True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create multimodal pages - Join text and image embeddings\n",
    "print(\"üîÑ Creating multimodal pages...\\n\")\n",
    "print(\"üîó Joining text and image data by page_number\")\n",
    "print(\"   - Copies both text and image embeddings\")\n",
    "print(\"   - Enables unified multi-modal search\\n\")\n",
    "\n",
    "multimodal_insert_query = f\"\"\"\n",
    "INSERT INTO {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    (document_id, file_name, page_number, page_id, image_id,\n",
    "     page_text, image_path, text_embedding, image_embedding, \n",
    "     has_text, has_image)\n",
    "SELECT\n",
    "    COALESCE(tp.document_id, ip.document_id) AS document_id,\n",
    "    COALESCE(tp.file_name, ip.file_name) AS file_name,\n",
    "    COALESCE(tp.page_number, ip.page_number) AS page_number,\n",
    "    tp.page_id,\n",
    "    ip.image_id,\n",
    "    tp.page_text,\n",
    "    ip.image_file_path AS image_path,\n",
    "    tp.text_embedding,\n",
    "    ip.image_embedding,\n",
    "    tp.page_id IS NOT NULL AS has_text,\n",
    "    ip.image_id IS NOT NULL AS has_image\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES tp\n",
    "FULL OUTER JOIN {DATABASE_NAME}.{SCHEMA_PROCESSING}.IMAGE_PAGES ip\n",
    "    ON tp.document_id = ip.document_id\n",
    "    AND tp.page_number = ip.page_number\n",
    "WHERE COALESCE(tp.document_id, ip.document_id) = '{DOCUMENT_ID}'\n",
    "AND NOT EXISTS (\n",
    "    SELECT 1 FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES mp\n",
    "    WHERE mp.document_id = COALESCE(tp.document_id, ip.document_id)\n",
    "    AND mp.page_number = COALESCE(tp.page_number, ip.page_number)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    session.sql(multimodal_insert_query).collect()\n",
    "    print(\"‚úÖ Multimodal pages created!\\n\")\n",
    "    \n",
    "    # Get statistics\n",
    "    stats_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN has_text THEN 1 END) as pages_with_text,\n",
    "        COUNT(CASE WHEN has_image THEN 1 END) as pages_with_images,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL THEN 1 END) as text_embeddings,\n",
    "        COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as image_embeddings\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = session.sql(stats_query).collect()\n",
    "    if stats:\n",
    "        print(f\"üìä Multimodal Pages Statistics:\")\n",
    "        print(f\"   Total pages: {stats[0][0]}\")\n",
    "        print(f\"   Pages with text: {stats[0][1]}\")\n",
    "        print(f\"   Pages with images: {stats[0][2]}\")\n",
    "        print(f\"   Text embeddings: {stats[0][3]}\")\n",
    "        print(f\"   Image embeddings: {stats[0][4]}\")\n",
    "    \n",
    "    # Show sample\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        page_number,\n",
    "        LEFT(page_text, 80) as text_preview,\n",
    "        has_text,\n",
    "        has_image,\n",
    "        text_embedding IS NOT NULL as has_text_emb,\n",
    "        image_embedding IS NOT NULL as has_image_emb\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    ORDER BY page_number\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "    \n",
    "    results = session.sql(query).collect()\n",
    "    if results:\n",
    "        print(f\"\\nüìÑ Sample Pages:\")\n",
    "        df = pd.DataFrame(results, \n",
    "                          columns=['Page', 'Text Preview', 'Has Text', 'Has Image', \n",
    "                                   'Text Emb', 'Image Emb'])\n",
    "        display(df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Section 8: Create Multi-Index Cortex Search Service\n",
    "\n",
    "Create a Cortex Search service that indexes:\n",
    "- **Text content** (keyword search)\n",
    "- **Text embeddings** (semantic search with Arctic-8k)\n",
    "- **Image embeddings** (visual search with voyage-multimodal-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating Cortex Search Service...\n",
      "\n",
      "üìã Service Configuration:\n",
      "   ‚Ä¢ Name: MULTIMODAL_SEARCH_SERVICE\n",
      "   ‚Ä¢ Text Index: page_text (keyword search)\n",
      "   ‚Ä¢ Vector Index 1: text_embedding (1024D - Arctic-8k)\n",
      "   ‚Ä¢ Vector Index 2: image_embedding (1024D - voyage-multimodal-3)\n",
      "   ‚Ä¢ Target Lag: 1 minute\n",
      "\n",
      "üÜï Creating new search service...\n",
      "\n",
      "‚úÖ Cortex Search Service created!\n",
      "\n",
      "üìä Service Status:\n",
      "   Name: MULTIMODAL_SEARCH_SERVICE\n",
      "   Database: GWAS\n",
      "   Schema: PDF_PROCESSING\n",
      "\n",
      "‚ö†Ô∏è  Note: Service may take ~1 minute to build indexes\n",
      "   Wait before running search queries if you get errors\n"
     ]
    }
   ],
   "source": [
    "# Create multi-index Cortex Search Service\n",
    "print(\"üîÑ Creating Cortex Search Service...\\n\")\n",
    "print(\"üìã Service Configuration:\")\n",
    "print(\"   ‚Ä¢ Name: MULTIMODAL_SEARCH_SERVICE\")\n",
    "print(\"   ‚Ä¢ Text Index: page_text (keyword search)\")\n",
    "print(\"   ‚Ä¢ Vector Index 1: text_embedding (1024D - Arctic-8k)\")\n",
    "print(\"   ‚Ä¢ Vector Index 2: image_embedding (1024D - voyage-multimodal-3)\")\n",
    "print(\"   ‚Ä¢ Target Lag: 1 minute\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if service already exists\n",
    "    check_sql = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    \n",
    "    service_exists = False\n",
    "    try:\n",
    "        result = session.sql(check_sql).collect()\n",
    "        service_exists = len(result) > 0\n",
    "    except:\n",
    "        service_exists = False\n",
    "    \n",
    "    if service_exists:\n",
    "        print(\"‚úÖ Service already exists, skipping creation (will refresh at end)\\n\")\n",
    "        # Skip to refresh section\n",
    "    else:\n",
    "        print(\"üÜï Creating new search service...\\n\")\n",
    "        \n",
    "        # Create multi-index search service\n",
    "        create_sql = f\"\"\"\n",
    "        CREATE CORTEX SEARCH SERVICE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE\n",
    "      TEXT INDEXES page_text\n",
    "      VECTOR INDEXES (\n",
    "        text_embedding,\n",
    "        image_embedding\n",
    "      )\n",
    "      ATTRIBUTES (\n",
    "        page_id,\n",
    "        document_id,\n",
    "        file_name,\n",
    "        page_number,\n",
    "        image_path\n",
    "      )\n",
    "      WAREHOUSE = {WAREHOUSE_NAME}\n",
    "      TARGET_LAG = '1 minute'\n",
    "    AS \n",
    "      SELECT \n",
    "        page_id,\n",
    "        document_id,\n",
    "        file_name,\n",
    "        page_number,\n",
    "        page_text,\n",
    "        text_embedding,\n",
    "        image_embedding,\n",
    "        image_path\n",
    "      FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "      WHERE has_text = TRUE AND has_image = TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "        session.sql(create_sql).collect()\n",
    "        print(\"‚úÖ Cortex Search Service created!\\n\")\n",
    "    \n",
    "    # Regardless of create or skip, check service status\n",
    "    status_sql = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    status = session.sql(status_sql).collect()\n",
    "    if status:\n",
    "        print(\"üìä Service Status:\")\n",
    "        print(f\"   Name: {status[0][1]}\")  # name column\n",
    "        print(f\"   Database: {status[0][2]}\")  # database_name\n",
    "        print(f\"   Schema: {status[0][3]}\")  # schema_name\n",
    "        print(\"\\n‚ö†Ô∏è  Note: Service may take ~1 minute to build indexes\")\n",
    "        print(\"   Wait before running search queries if you get errors\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating search service: {e}\")\n",
    "    print(\"\\n   If you see 'already exists', that's OK - service is ready\")\n",
    "    print(\"   If you see 'insufficient privileges', contact your Snowflake admin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Refreshing Search Service...\n",
      "\n",
      "‚è±Ô∏è  Initiating service refresh...\n",
      "‚úÖ Service refresh initiated\n",
      "\n",
      "‚è≥ Waiting 5 seconds for service to sync...\n",
      "‚úÖ Ready to query\n",
      "\n",
      "‚úÖ 14 pages are indexed and ready for search\n"
     ]
    }
   ],
   "source": [
    "# Refresh the search service to pick up any new data\n",
    "# This is fast and updates indexes without recreating the service\n",
    "print(\"üîÑ Refreshing Search Service...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check current refresh status\n",
    "    status_query = \"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        database_name,\n",
    "        schema_name,\n",
    "        created_on,\n",
    "        refresh_on\n",
    "    FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n",
    "    WHERE name = 'MULTIMODAL_SEARCH_SERVICE'\n",
    "    \"\"\"\n",
    "    \n",
    "    # First get the service info\n",
    "    show_query = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    session.sql(show_query)\n",
    "    \n",
    "    # Force a refresh\n",
    "    print(\"‚è±Ô∏è  Initiating service refresh...\")\n",
    "    refresh_query = f\"\"\"\n",
    "    ALTER CORTEX SEARCH SERVICE {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE REFRESH\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        session.sql(refresh_query).collect()\n",
    "        print(\"‚úÖ Service refresh initiated\\n\")\n",
    "    except Exception as refresh_error:\n",
    "        if \"does not support manual refresh\" in str(refresh_error):\n",
    "            print(\"‚ÑπÔ∏è  Service auto-refreshes based on TARGET_LAG setting\\n\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Refresh note: {refresh_error}\\n\")\n",
    "    \n",
    "    # Wait a moment for refresh\n",
    "    import time\n",
    "    print(\"‚è≥ Waiting 5 seconds for service to sync...\")\n",
    "    time.sleep(5)\n",
    "    print(\"‚úÖ Ready to query\\n\")\n",
    "    \n",
    "    # Verify data one more time\n",
    "    verify_query = f\"\"\"\n",
    "    SELECT COUNT(*) as ready_pages\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "      AND text_embedding IS NOT NULL\n",
    "      AND image_embedding IS NOT NULL\n",
    "      AND has_text = TRUE\n",
    "      AND has_image = TRUE\n",
    "    \"\"\"\n",
    "    \n",
    "    result = session.sql(verify_query).collect()\n",
    "    if result and result[0][0] > 0:\n",
    "        print(f\"‚úÖ {result[0][0]} pages are indexed and ready for search\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No pages found matching service criteria\")\n",
    "        print(\"   Service filters: has_text = TRUE AND has_image = TRUE\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  {e}\")\n",
    "    print(\"\\n‚ÑπÔ∏è  This is OK - service should still work if it was created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying Search Service Status...\n",
      "\n",
      "‚úÖ Search service exists\n",
      "   Name: MULTIMODAL_SEARCH_SERVICE\n",
      "   Created: 1 minute\n",
      "\n",
      "üìä Data Readiness:\n",
      "   Total pages: 14\n",
      "   With text embeddings: 14\n",
      "   With image embeddings: 14\n",
      "   With BOTH embeddings: 14\n",
      "\n",
      "‚úÖ Ready to search 14 pages\n",
      "\n",
      "üí° If you just created the service, wait ~60 seconds for indexes to build\n"
     ]
    }
   ],
   "source": [
    "# Verify search service and data readiness\n",
    "print(\"üîç Verifying Search Service Status...\\n\")\n",
    "\n",
    "try:\n",
    "    # Check if service exists\n",
    "    check_service = f\"\"\"\n",
    "    SHOW CORTEX SEARCH SERVICES LIKE 'MULTIMODAL_SEARCH_SERVICE' IN SCHEMA {DATABASE_NAME}.{SCHEMA_PROCESSING}\n",
    "    \"\"\"\n",
    "    service_info = session.sql(check_service).collect()\n",
    "    \n",
    "    if service_info:\n",
    "        print(\"‚úÖ Search service exists\")\n",
    "        print(f\"   Name: {service_info[0][1]}\")\n",
    "        print(f\"   Created: {service_info[0][4]}\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå Search service NOT found!\")\n",
    "        print(\"   Run the previous cell to create it\\n\")\n",
    "    \n",
    "    # Check data in multimodal pages\n",
    "    data_check = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_pages,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL THEN 1 END) as with_text_emb,\n",
    "        COUNT(CASE WHEN image_embedding IS NOT NULL THEN 1 END) as with_image_emb,\n",
    "        COUNT(CASE WHEN text_embedding IS NOT NULL AND image_embedding IS NOT NULL THEN 1 END) as with_both\n",
    "    FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_PAGES\n",
    "    WHERE document_id = '{DOCUMENT_ID}'\n",
    "    \"\"\"\n",
    "    \n",
    "    data_stats = session.sql(data_check).collect()\n",
    "    if data_stats:\n",
    "        total, text_emb, image_emb, both = data_stats[0]\n",
    "        print(f\"üìä Data Readiness:\")\n",
    "        print(f\"   Total pages: {total}\")\n",
    "        print(f\"   With text embeddings: {text_emb}\")\n",
    "        print(f\"   With image embeddings: {image_emb}\")\n",
    "        print(f\"   With BOTH embeddings: {both}\")\n",
    "        \n",
    "        if both == 0:\n",
    "            print(\"\\n‚ö†Ô∏è  WARNING: No pages have both embeddings!\")\n",
    "            print(\"   Search service filters for: has_text = TRUE AND has_image = TRUE\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Ready to search {both} pages\")\n",
    "    \n",
    "    # Give service time to build indexes\n",
    "    print(\"\\nüí° If you just created the service, wait ~60 seconds for indexes to build\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking service: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Section 9: Test Multimodal Search\n",
    "\n",
    "Query the multi-index Cortex Search service with:\n",
    "- **Text keyword search** (exact/fuzzy matching on page_text)\n",
    "- **Text embedding search** (semantic similarity with Arctic-8k)\n",
    "- **Image embedding search** (visual similarity with voyage-multimodal-3)\n",
    "\n",
    "The search uses weighted scoring to balance text and visual results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b94e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector conversion helper function defined!\n",
      "\n",
      "Example usage:\n",
      "text_vector = safe_vector_conversion(embeddings[0][0])\n",
      "image_vector = safe_vector_conversion(embeddings[0][1])\n"
     ]
    }
   ],
   "source": [
    "# HELPER FUNCTION: Safely convert embeddings to proper list format\n",
    "def safe_vector_conversion(vector_data):\n",
    "    \"\"\"\n",
    "    Safely convert Snowflake embedding results to Python lists.\n",
    "    Handles various formats that Snowflake might return.\n",
    "    \"\"\"\n",
    "    if vector_data is None:\n",
    "        return []\n",
    "    \n",
    "    # If it's already a list, return it\n",
    "    if isinstance(vector_data, list) and len(vector_data) > 0 and isinstance(vector_data[0], (int, float)):\n",
    "        return vector_data\n",
    "    \n",
    "    # If it's a string representation of a list\n",
    "    if isinstance(vector_data, str):\n",
    "        try:\n",
    "            import ast\n",
    "            parsed = ast.literal_eval(vector_data)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except:\n",
    "            # If ast.literal_eval fails, try json\n",
    "            try:\n",
    "                import json\n",
    "                parsed = json.loads(vector_data)\n",
    "                if isinstance(parsed, list):\n",
    "                    return parsed\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # If it has a tolist method (numpy array or similar)\n",
    "    if hasattr(vector_data, 'tolist'):\n",
    "        return vector_data.tolist()\n",
    "    \n",
    "    # If it's an array-like object that can be converted to list\n",
    "    try:\n",
    "        result = list(vector_data)\n",
    "        # Check if we got a proper numeric list\n",
    "        if result and isinstance(result[0], (int, float)):\n",
    "            return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # If all else fails, raise an error\n",
    "    raise ValueError(f\"Could not convert vector data of type {type(vector_data)} to list\")\n",
    "\n",
    "# Test the function\n",
    "print(\"‚úÖ Vector conversion helper function defined!\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"text_vector = safe_vector_conversion(embeddings[0][0])\")\n",
    "print(\"image_vector = safe_vector_conversion(embeddings[0][1])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß¨ CELL 25-33: Section 10 - Extract GWAS Traits (Multi-Phase AI Pipeline)\n",
    "\n",
    "**Overview of extraction phases:**\n",
    "- **Cell 25**: Define 15 GWAS traits with complex extraction prompts\n",
    "- **Cell 27**: Phase 1 - Dual extraction (AI_EXTRACT vs COMPLETE) from text\n",
    "- **Cell 29-30**: Phase 2 - Multimodal search validation (text + images)\n",
    "- **Cell 31**: Phase 3 - Smart merge with LLM tie-breaker\n",
    "- **Cell 33**: Phase 4 - Display final results\n",
    "\n",
    "This multi-phase approach combines multiple extraction methods to maximize accuracy and completeness of GWAS trait extraction from scientific papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Defined 15 GWAS Traits for Targeted Extraction\n",
      "\n",
      "================================================================================\n",
      "‚ú® IMPROVEMENTS APPLIED:\n",
      "   ‚úÖ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean, tomato)\n",
      "   ‚úÖ Germplasm_Name: Added rice, wheat, Arabidopsis, soybean examples\n",
      "   ‚úÖ Genome_Version: Added 6 crop genome formats\n",
      "   ‚úÖ Gene: Added 5 crop gene ID patterns\n",
      "   ‚úÖ Allele: Shortened from 15 lines to 8 lines (50% reduction)\n",
      "   ‚úÖ Chromosome: Now accepts numbers, letters (3A, X, Y, MT), linkage groups\n",
      "   ‚úÖ Enhanced search queries with GWAS terminology\n",
      "   ‚úÖ NEW: Multi-finding support (extract ALL significant associations, not just strongest)\n",
      "================================================================================\n",
      "\n",
      " 1. Trait                ‚Üí Search: 'trait phenotype disease resistance agronomic chara...'\n",
      " 2. Germplasm_Name       ‚Üí Search: 'germplasm variety line population inbred diversity...'\n",
      " 3. Genome_Version       ‚Üí Search: 'genome version reference assembly RefGen annotatio...'\n",
      " 4. GWAS_Model           ‚Üí Search: 'GWAS model GLM MLM statistical method population s...'\n",
      " 5. Evidence_Type        ‚Üí Search: 'GWAS QTL linkage association mapping study type ge...'\n",
      " 6. Chromosome           ‚Üí Search: 'chromosome chr number genomic location linkage gro...'\n",
      " 7. Physical_Position    ‚Üí Search: 'physical position locus base pairs bp genomic coor...'\n",
      " 8. Gene                 ‚Üí Search: 'candidate gene causal gene functional gene locus g...'\n",
      " 9. SNP_Name             ‚Üí Search: 'SNP marker name identifier genotyping array lead m...'\n",
      "10. Variant_ID           ‚Üí Search: 'variant ID SNP ID rs number dbSNP database identif...'\n",
      "11. Variant_Type         ‚Üí Search: 'variant type SNP InDel polymorphism haplotype mark...'\n",
      "12. Effect_Size          ‚Üí Search: 'effect size R-squared R2 variance explained phenot...'\n",
      "13. Allele               ‚Üí Search: 'allele REF ALT haplotype genotype reference altern...'\n",
      "14. Annotation           ‚Üí Search: 'functional annotation missense synonymous intergen...'\n",
      "15. Candidate_Region     ‚Üí Search: 'QTL region confidence interval linkage disequilibr...'\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Ready to extract 15 traits using multi-phase approach\n",
      "üåæ Now supports: Maize, Rice, Wheat, Arabidopsis, Soybean, Tomato, and more!\n",
      "üéØ NEW: Can extract 10-20 findings per paper (not just strongest SNP)\n"
     ]
    }
   ],
   "source": [
    "# Define 15 GWAS traits with refined, context-aware extraction prompts\n",
    "# Based on GWAS paper structure: Abstract ‚Üí Intro ‚Üí Methods ‚Üí Results ‚Üí Discussion\n",
    "# ‚ú® IMPROVED: Fixed for multi-species plant genomics coverage\n",
    "# ‚ú® NEW: Support for multiple findings extraction (10-20 SNPs per paper)\n",
    "\n",
    "traits_config_improved = {\n",
    "    # ========================================\n",
    "    # DOCUMENT-LEVEL TRAITS (Extract once per paper)\n",
    "    # ========================================\n",
    "    \n",
    "    \"Trait\": {\n",
    "        \"search_query\": \"trait phenotype disease resistance agronomic character quality stress tolerance\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the MAIN phenotypic trait studied in this GWAS paper.\n",
    "\n",
    "Look in: Title, Abstract (first paragraph), Introduction (study objective).\n",
    "\n",
    "Format: Descriptive name of the trait being studied.\n",
    "Examples: 'Disease resistance' (generic), 'Plant height', 'Flowering time', 'Grain yield', 'Drought tolerance'\n",
    "\n",
    "Return the primary trait name ONLY, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Germplasm_Name\": {\n",
    "        \"search_query\": \"germplasm variety line population inbred diversity panel genetic background subpopulation\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the germplasm/population used in this GWAS study.\n",
    "\n",
    "Look in: Methods ‚Üí Plant Materials/Germplasm, Introduction ‚Üí Study population.\n",
    "\n",
    "Common formats across crops:\n",
    "- Inbred lines: 'B73' (maize), 'Nipponbare' (rice), 'Col-0' (Arabidopsis), 'Chinese Spring' (wheat)\n",
    "- Diversity panels: '282 association panel', '3K rice genome panel', 'SoyNAM', 'UK wheat diversity panel'\n",
    "- Population codes: 'DH population', 'RIL population', 'F2:3 families', 'BC1F2'\n",
    "- Specific varieties: 'Williams 82' (soybean), 'Kitaake' (rice)\n",
    "\n",
    "Return the most specific germplasm name, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Genome_Version\": {\n",
    "        \"search_query\": \"genome version reference assembly RefGen annotation build\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the reference genome assembly version used.\n",
    "\n",
    "Look in: Methods ‚Üí Genotyping/Variant Calling, Supplementary Methods.\n",
    "\n",
    "Common formats by crop:\n",
    "- Maize: 'B73 RefGen_v4', 'AGPv4', 'Zm00001e'\n",
    "- Rice: 'IRGSP-1.0', 'MSU7', 'Nipponbare-v7.0'\n",
    "- Wheat: 'IWGSC RefSeq v2.1', 'CS42'\n",
    "- Arabidopsis: 'TAIR10', 'Col-0'\n",
    "- Soybean: 'Glycine_max_v4.0', 'Williams 82 v2.0'\n",
    "- Tomato: 'SL4.0', 'Heinz 1706'\n",
    "\n",
    "Return the version identifier, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"GWAS_Model\": {\n",
    "        \"search_query\": \"GWAS model GLM MLM statistical method population structure kinship software\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the statistical model/software used for GWAS.\n",
    "\n",
    "Look in: Methods ‚Üí Statistical analysis/GWAS analysis section.\n",
    "\n",
    "Common models: MLM (mixed linear model), GLM, CMLM, FarmCPU, BLINK, SUPER,\n",
    "               EMMAX, FastGWA, rrBLUP, BOLT-LMM\n",
    "\n",
    "Common software: TASSEL, GAPIT, GEMMA, PLINK, regenie, GCTA, rMVP, GENESIS\n",
    "\n",
    "Return model name OR software, or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Evidence_Type\": {\n",
    "        \"search_query\": \"GWAS QTL linkage association mapping study type genetic analysis\",\n",
    "        \"extraction_prompt\": \"\"\"Identify the genetic mapping approach used.\n",
    "\n",
    "Look in: Title, Abstract, Methods ‚Üí Study design.\n",
    "\n",
    "Types: \n",
    "- 'GWAS' (genome-wide association study) - most common\n",
    "- 'QTL' (quantitative trait loci mapping) - biparental populations\n",
    "- 'Linkage' (family-based mapping)\n",
    "- 'Fine_Mapping' (high-resolution narrowing of QTL)\n",
    "\n",
    "Return ONE type: 'GWAS', 'QTL', 'Linkage', 'Fine_Mapping', or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    # ========================================\n",
    "    # FINDING-LEVEL TRAITS (Extract multiple per paper)\n",
    "    # ========================================\n",
    "    # ‚ú® NEW: These can now extract arrays of findings\n",
    "    \n",
    "    \"Chromosome\": {\n",
    "        \"search_query\": \"chromosome chr number genomic location linkage group significant hits\",\n",
    "        \"extraction_prompt\": \"\"\"Extract ALL chromosomes with significant associations (p < 0.001 or genome-wide significant).\n",
    "\n",
    "Look in: Results ‚Üí GWAS hits, Manhattan plot peaks, Tables of significant SNPs.\n",
    "\n",
    "Format: Return comma-separated list of chromosome identifiers, ranked by significance (lowest p-value first).\n",
    "Examples: '5, 3, 10, 1' or '3A, 5B, 2D' (wheat) or 'X, 3, 5' or 'LG1, LG3, LG5' (linkage groups)\n",
    "\n",
    "If only 1 significant hit: Return that chromosome.\n",
    "If 10+ hits: Return top 10 most significant.\n",
    "\n",
    "Return chromosome identifiers (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Physical_Position\": {\n",
    "        \"search_query\": \"physical position locus base pairs bp genomic coordinate marker location\",\n",
    "        \"extraction_prompt\": \"\"\"Extract physical positions of SIGNIFICANT SNPs (top 10 by p-value).\n",
    "\n",
    "Look in: Results ‚Üí Significant associations, Tables with 'Position' or 'bp' columns.\n",
    "\n",
    "Format: Return comma-separated positions with chromosome context.\n",
    "Examples: \n",
    "- Single: '145.6 Mb'\n",
    "- Multiple: 'Chr5:145.6Mb, Chr3:198.2Mb, Chr10:78.9Mb'\n",
    "- Alt format: '145678901 (Chr5), 198234567 (Chr3)'\n",
    "\n",
    "If positions are in a table: Extract top 10 rows.\n",
    "Include chromosome reference for clarity.\n",
    "\n",
    "Return positions (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Gene\": {\n",
    "        \"search_query\": \"candidate gene causal gene functional gene locus gene model annotation\",\n",
    "        \"extraction_prompt\": \"\"\"Extract ALL candidate genes mentioned for significant associations.\n",
    "\n",
    "Look in: Results ‚Üí Candidate genes, Tables ‚Üí Gene columns, Discussion ‚Üí Gene function.\n",
    "\n",
    "Common formats across crops:\n",
    "- Maize: 'Zm00001d027230', 'GRMZM2G123456', 'tb1', 'dwarf8'\n",
    "- Rice: 'LOC_Os03g01234', 'OsMADS1', 'SD1'\n",
    "- Arabidopsis: 'AT1G12345', 'FLC', 'CO'\n",
    "- Wheat: 'TraesCS3A02G123456', 'Rht-D1'\n",
    "- Soybean: 'Glyma.01G000100', 'E1', 'Dt1'\n",
    "\n",
    "Return comma-separated list if multiple genes.\n",
    "Examples: 'Zm00001d027230, Zm00001d042156, Zm00001d013894'\n",
    "\n",
    "Return candidate genes (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"SNP_Name\": {\n",
    "        \"search_query\": \"SNP marker name identifier genotyping array lead markers\",\n",
    "        \"extraction_prompt\": \"\"\"Extract SNP/marker names for SIGNIFICANT associations (top 10).\n",
    "\n",
    "Look in: Results ‚Üí Significant markers, Tables ‚Üí Marker ID column.\n",
    "\n",
    "Common prefixes vary by genotyping platform:\n",
    "- Array-based: 'PZE-', 'AX-', 'Affx-'\n",
    "- Sequence-based: 'S1_', 'Chr1_', 'ss', 'rs' (if dbSNP)\n",
    "- Custom: May be position-based or study-specific\n",
    "\n",
    "Return comma-separated list if multiple SNPs.\n",
    "Examples: 'PZE-101234567, AX-90812345, S1_145678901'\n",
    "\n",
    "Return marker identifiers (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Variant_ID\": {\n",
    "        \"search_query\": \"variant ID SNP ID rs number dbSNP database identifier\",\n",
    "        \"extraction_prompt\": \"\"\"Extract dbSNP variant IDs if referenced for significant associations.\n",
    "\n",
    "Look in: Methods ‚Üí Variant annotation, Supplementary tables.\n",
    "\n",
    "Format: 'rs' or 'ss' prefixes (human/model organism databases)\n",
    "Examples: 'rs123456789, rs987654321, rs111222333'\n",
    "\n",
    "NOTE: Most plant studies don't use dbSNP IDs (common in human/model organisms).\n",
    "\n",
    "Return dbSNP IDs (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Variant_Type\": {\n",
    "        \"search_query\": \"variant type SNP InDel polymorphism haplotype marker genotyping\",\n",
    "        \"extraction_prompt\": \"\"\"Extract the predominant variant/marker type analyzed.\n",
    "\n",
    "Look in: Methods ‚Üí Variant calling/Genotyping, Results ‚Üí Association type.\n",
    "\n",
    "Common types:\n",
    "- SNP (single nucleotide polymorphism) - most common\n",
    "- InDel (insertion/deletion)\n",
    "- CNV (copy number variant)\n",
    "- SV (structural variant)\n",
    "- PAV (presence/absence variant) - plant pangenomes\n",
    "- Haplotype (multi-marker block)\n",
    "- SSR/Microsatellite (older studies)\n",
    "\n",
    "Return ONE primary type (this is usually uniform across findings), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Effect_Size\": {\n",
    "        \"search_query\": \"effect size R-squared R2 variance explained phenotypic variation proportion\",\n",
    "        \"extraction_prompt\": \"\"\"Extract effect sizes for SIGNIFICANT QTLs (top 10).\n",
    "\n",
    "Look in: Results ‚Üí QTL effect, Tables ‚Üí R¬≤ or 'Variance explained' columns.\n",
    "\n",
    "Format: Return comma-separated if multiple, with chromosome context if helpful.\n",
    "Examples:\n",
    "- Single: 'R¬≤=0.23'\n",
    "- Multiple: '0.31 (Chr10), 0.23 (Chr5), 0.19 (Chr3)'\n",
    "- Alt format: '23%, 19%, 15%'\n",
    "\n",
    "Return effect sizes (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Allele\": {\n",
    "        \"search_query\": \"allele REF ALT haplotype genotype reference alternate favorable effect\",\n",
    "        \"extraction_prompt\": \"\"\"Extract allele information for SIGNIFICANT SNPs.\n",
    "\n",
    "Look in: Results tables (REF, ALT, Allele columns), figures, supplementary data.\n",
    "\n",
    "Common formats:\n",
    "- Slash: 'A/G', 'T/C', 'G/T'\n",
    "- Arrow: 'A>G', 'T>C'\n",
    "- Explicit: 'REF: A ALT: G'\n",
    "- Effect notation: 'favorable: T'\n",
    "\n",
    "If multiple SNPs: Return comma-separated alleles.\n",
    "Examples: 'A/G, T/C, G/A'\n",
    "\n",
    "NOTE: Allele data is typically in tables/charts, not body text.\n",
    "\n",
    "Return allele notations (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Annotation\": {\n",
    "        \"search_query\": \"functional annotation missense synonymous intergenic gene ontology regulatory\",\n",
    "        \"extraction_prompt\": \"\"\"Extract functional annotations for SIGNIFICANT variants.\n",
    "\n",
    "Look in: Results ‚Üí Variant annotation, Discussion ‚Üí Functional impact.\n",
    "\n",
    "Categories: \n",
    "- 'missense_variant', 'synonymous', 'intergenic_region'\n",
    "- 'upstream_gene', '5_prime_UTR', '3_prime_UTR'\n",
    "- 'intronic', 'regulatory_region'\n",
    "\n",
    "If multiple variants: Return comma-separated annotations.\n",
    "Examples: 'missense_variant, intergenic_region, missense_variant'\n",
    "\n",
    "Return annotations (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    },\n",
    "    \n",
    "    \"Candidate_Region\": {\n",
    "        \"search_query\": \"QTL region confidence interval linkage disequilibrium block bin locus interval\",\n",
    "        \"extraction_prompt\": \"\"\"Extract QTL regions or confidence intervals for SIGNIFICANT associations.\n",
    "\n",
    "Look in: Results ‚Üí QTL mapping, Tables ‚Üí QTL interval/region columns.\n",
    "\n",
    "Format: Genomic intervals with units\n",
    "Examples: \n",
    "- Single: 'chr1:145.6-146.1 Mb'\n",
    "- Multiple: 'chr5:145.6-146.1Mb, chr3:198-199Mb, chr10:78-79Mb'\n",
    "- Alt: 'bin 1.04, bin 3.05, bin 10.02'\n",
    "- cM: '10-12 cM (Chr5), 45-47 cM (Chr3)'\n",
    "\n",
    "Return genomic regions (comma-separated if multiple), or 'NOT_FOUND'.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Defined 15 GWAS Traits for Targeted Extraction\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚ú® IMPROVEMENTS APPLIED:\")\n",
    "print(\"   ‚úÖ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean, tomato)\")\n",
    "print(\"   ‚úÖ Germplasm_Name: Added rice, wheat, Arabidopsis, soybean examples\")\n",
    "print(\"   ‚úÖ Genome_Version: Added 6 crop genome formats\")\n",
    "print(\"   ‚úÖ Gene: Added 5 crop gene ID patterns\")\n",
    "print(\"   ‚úÖ Allele: Shortened from 15 lines to 8 lines (50% reduction)\")\n",
    "print(\"   ‚úÖ Chromosome: Now accepts numbers, letters (3A, X, Y, MT), linkage groups\")\n",
    "print(\"   ‚úÖ Enhanced search queries with GWAS terminology\")\n",
    "print(\"   ‚úÖ NEW: Multi-finding support (extract ALL significant associations, not just strongest)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for idx, (trait_name, trait_info) in enumerate(traits_config_improved.items(), 1):\n",
    "    print(f\"{idx:2d}. {trait_name:20s} ‚Üí Search: '{trait_info['search_query'][:50]}...'\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ Ready to extract {len(traits_config_improved)} traits using multi-phase approach\")\n",
    "print(\"üåæ Now supports: Maize, Rice, Wheat, Arabidopsis, Soybean, Tomato, and more!\")\n",
    "print(\"üéØ NEW: Can extract 10-20 findings per paper (not just strongest SNP)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä CELL 27: Phase 1 - Dual Extraction (AI_EXTRACT vs COMPLETE)\n",
    "\n",
    "**What this does:** Extracts GWAS traits from text pages using TWO methods:\n",
    "- **Method A**: AI_EXTRACT with complex prompts (batch processing)\n",
    "- **Method B**: COMPLETE with simplified direct questions (individual processing)\n",
    "- **Output**: Merged results with method comparison\n",
    "\n",
    "Search text-only pages for each of the 15 traits individually using targeted queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Phase 1: Text-Based Extraction (Dual Method, Optimized)\n",
      "\n",
      "================================================================================\n",
      "üéØ Strategy: Extract using BOTH methods, intelligently merge results\n",
      "   Method A: AI_EXTRACT with full complex prompts (batch, structured)\n",
      "   Method B: COMPLETE with simplified prompts (individual, flexible)\n",
      "   ‚Üí Prefer agreement, then longer/more specific values\n",
      "   ‚úÖ IMPROVEMENTS: Full prompts, confidence tracking, smart merge\n",
      "\n",
      "‚úÖ Loaded document text: 62,758 characters\n",
      "\n",
      "üìä Method A: AI_EXTRACT with FULL Complex Prompts\n",
      "\n",
      "‚öôÔ∏è  Calling AI_EXTRACT with FULL complex prompts...\n",
      "   Context size: 25,006 chars\n",
      "   Prompt sizes: 346-584 chars\n",
      "\n",
      "   ‚úì Trait               : Resistance to brown planthopper\n",
      "   ‚úì Germplasm_Name      : 502 rice varieties\n",
      "   ‚úì Genome_Version      : IRGSP-1.0\n",
      "   ‚úì GWAS_Model          : EMMAX\n",
      "   ‚úì Evidence_Type       : GWAS\n",
      "   ‚úì Chromosome          : 11\n",
      "   ‚úó Physical_Position   : Not found\n",
      "   ‚úì Gene                : ['RLK', 'NB-LRR', 'LRR']\n",
      "   ‚úì SNP_Name            : ['rs1234567', 'rs2345678', 'rs3456789', 'rs4567890\n",
      "   ‚úó Variant_ID          : Not found\n",
      "   ‚úì Variant_Type        : SNP (single nucleotide polymorphism)\n",
      "   ‚úó Effect_Size         : Not found\n",
      "   ‚úó Allele              : Not found\n",
      "   ‚úó Annotation          : Not found\n",
      "   ‚úì Candidate_Region    : chr1:145.6-146.1 Mb\n",
      "\n",
      "‚úÖ AI_EXTRACT: Found 10/15 traits\n",
      "\n",
      "================================================================================\n",
      "üìä Method B: COMPLETE with Simplified Prompts\n",
      "\n",
      "‚öôÔ∏è  Processing traits individually with COMPLETE...\n",
      "   ‚úì  1/15 Trait               : \"Brown planthopper resistance\"\n",
      "   ‚úì  2/15 Germplasm_Name      : \"502 rice varieties diversity panel\"\n",
      "   ‚úó  3/15 Genome_Version      : Not found\n",
      "   ‚úì  4/15 Chromosome          : \"11\"\n",
      "   ‚úó  5/15 Physical_Position   : Not found\n",
      "   ‚úó  6/15 Gene                : Not found\n",
      "   ‚úó  7/15 SNP_Name            : Not found\n",
      "   ‚úó  8/15 Variant_ID          : Not found\n",
      "   ‚úì  9/15 Variant_Type        : \"SNP\"\n",
      "   ‚úó 10/15 Effect_Size         : Not found\n",
      "   ‚úó 11/15 GWAS_Model          : Not found\n",
      "   ‚úì 12/15 Evidence_Type       : \"GWAS\"\n",
      "   ‚úó 13/15 Allele              : Not found\n",
      "   ‚úó 14/15 Annotation          : Not found\n",
      "   ‚úó 15/15 Candidate_Region    : Not found\n",
      "\n",
      "‚úÖ COMPLETE: Found 5/15 traits\n",
      "\n",
      "================================================================================\n",
      "üìä INTELLIGENT MERGE: Choosing Best Values\n",
      "\n",
      "‚ö†Ô∏è  Trait               : DIFFER (MEDIUM) - chose A (longer)\n",
      "      AI_EXTRACT: Resistance to brown planthopper\n",
      "      COMPLETE:   \"Brown planthopper resistance\"\n",
      "‚ö†Ô∏è  Germplasm_Name      : DIFFER (MEDIUM) - chose B (longer)\n",
      "      AI_EXTRACT: 502 rice varieties\n",
      "      COMPLETE:   \"502 rice varieties diversity panel\"\n",
      "üÖ∞Ô∏è  Genome_Version      : AI_EXTRACT only (LOW) ‚Üí IRGSP-1.0\n",
      "üÖ∞Ô∏è  GWAS_Model          : AI_EXTRACT only (LOW) ‚Üí EMMAX\n",
      "‚ö†Ô∏è  Evidence_Type       : DIFFER (MEDIUM) - chose B (longer)\n",
      "      AI_EXTRACT: GWAS\n",
      "      COMPLETE:   \"GWAS\"\n",
      "‚ö†Ô∏è  Chromosome          : DIFFER (MEDIUM) - chose B (longer)\n",
      "      AI_EXTRACT: 11\n",
      "      COMPLETE:   \"11\"\n",
      "‚ùå Physical_Position   : Not found by either method\n",
      "üÖ∞Ô∏è  Gene                : AI_EXTRACT only (LOW) ‚Üí ['RLK', 'NB-LRR', 'LRR']\n",
      "üÖ∞Ô∏è  SNP_Name            : AI_EXTRACT only (LOW) ‚Üí ['rs1234567', 'rs2345678', 'rs3456789', 'rs4567890\n",
      "‚ùå Variant_ID          : Not found by either method\n",
      "‚ö†Ô∏è  Variant_Type        : DIFFER (MEDIUM) - chose A (longer)\n",
      "      AI_EXTRACT: SNP (single nucleotide polymorphism)\n",
      "      COMPLETE:   \"SNP\"\n",
      "‚ùå Effect_Size         : Not found by either method\n",
      "‚ùå Allele              : Not found by either method\n",
      "‚ùå Annotation          : Not found by either method\n",
      "üÖ∞Ô∏è  Candidate_Region    : AI_EXTRACT only (LOW) ‚Üí chr1:145.6-146.1 Mb\n",
      "\n",
      "================================================================================\n",
      "üìä Phase 1 Final Results (IMPROVED):\n",
      "   ‚úÖ Total found: 10/15 traits\n",
      "   ü§ù Agreements: 0 (HIGH confidence)\n",
      "   üÖ∞Ô∏è  AI_EXTRACT wins: 7\n",
      "   üÖ±Ô∏è  COMPLETE wins: 3\n",
      "   ‚ùå Not found: 5 traits\n",
      "   Missing: Physical_Position, Variant_ID, Effect_Size, Allele, Annotation\n",
      "\n",
      "üéØ Confidence Distribution:\n",
      "   MEDIUM    :  5 traits\n",
      "   LOW       :  5 traits\n",
      "   NONE      :  5 traits\n",
      "\n",
      "‚úÖ IMPROVEMENTS APPLIED:\n",
      "   ‚Ä¢ Full prompts (no truncation)\n",
      "   ‚Ä¢ 25K context (from 15K)\n",
      "   ‚Ä¢ Confidence tracking\n",
      "   ‚Ä¢ Smart merge (prefer agreement, then longer values)\n",
      "   ‚Ä¢ Never overwrite valid with invalid\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: DUAL EXTRACTION - AI_EXTRACT vs COMPLETE (IMPROVED)\n",
    "print(\"üìù Phase 1: Text-Based Extraction (Dual Method, Optimized)\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Strategy: Extract using BOTH methods, intelligently merge results\")\n",
    "print(\"   Method A: AI_EXTRACT with full complex prompts (batch, structured)\")\n",
    "print(\"   Method B: COMPLETE with simplified prompts (individual, flexible)\")\n",
    "print(\"   ‚Üí Prefer agreement, then longer/more specific values\")\n",
    "print(\"   ‚úÖ IMPROVEMENTS: Full prompts, confidence tracking, smart merge\\n\")\n",
    "\n",
    "# Get all text pages\n",
    "context_query = f\"\"\"\n",
    "SELECT LISTAGG(page_text, '\\\\n\\\\n---PAGE BREAK---\\\\n\\\\n') WITHIN GROUP (ORDER BY page_number) as full_text\n",
    "FROM {DATABASE_NAME}.{SCHEMA_PROCESSING}.TEXT_PAGES\n",
    "WHERE document_id = '{DOCUMENT_ID}'\n",
    "\"\"\"\n",
    "\n",
    "# Helper function to validate if a value is actually meaningful\n",
    "def is_valid_value(val):\n",
    "    \"\"\"Check if value is meaningful (not 'NOT_FOUND' or garbage)\"\"\"\n",
    "    if not val:\n",
    "        return False\n",
    "    \n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    \n",
    "    # Check for explicit NOT_FOUND patterns\n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    \n",
    "    # Check for meta-responses\n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    \n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "try:\n",
    "    all_text = session.sql(context_query).collect()\n",
    "    \n",
    "    if not all_text or not all_text[0][0]:\n",
    "        print(\"‚ö†Ô∏è  No text pages found in TEXT_PAGES table\")\n",
    "        print(\"   Make sure Section 5 (Extract Text Pages) was run\")\n",
    "        ai_extract_results = {}\n",
    "        ai_complete_results = {}\n",
    "        text_extraction_results = {}\n",
    "        fields_found = 0\n",
    "        fields_not_found = list(traits_config_improved.keys())\n",
    "        confidence_levels = {}\n",
    "    else:\n",
    "        full_document_text = all_text[0][0]\n",
    "        print(f\"‚úÖ Loaded document text: {len(full_document_text):,} characters\\n\")\n",
    "        \n",
    "        import json\n",
    "        \n",
    "        # =============================================================================\n",
    "        # METHOD A: AI_EXTRACT with FULL COMPLEX prompts (NO TRUNCATION)\n",
    "        # =============================================================================\n",
    "        print(\"üìä Method A: AI_EXTRACT with FULL Complex Prompts\\n\")\n",
    "        \n",
    "        # ‚úÖ FIXED: Use FULL prompts without truncation\n",
    "        complex_prompts = {}\n",
    "        for trait_name, trait_info in traits_config_improved.items():\n",
    "            # Convert multi-line prompt to single line, preserve ALL instructions\n",
    "            detailed_prompt = trait_info['extraction_prompt']\n",
    "            condensed = ' '.join(detailed_prompt.replace('\\n', ' ').split())\n",
    "            # ‚úÖ NO TRUNCATION - keep full prompt!\n",
    "            complex_prompts[trait_name] = condensed\n",
    "        \n",
    "        # ‚úÖ FIXED: Increase context to 25K chars (from 15K)\n",
    "        # Smart truncation: keep more content for better table capture\n",
    "        if len(full_document_text) > 25000:\n",
    "            # Keep first 15K (intro/methods) + last 10K (results/tables)\n",
    "            clean_text = (full_document_text[:15000] + \" ... \" + full_document_text[-10000:])\n",
    "        else:\n",
    "            clean_text = full_document_text\n",
    "        \n",
    "        clean_text = clean_text.replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        # Create JSON for responseFormat\n",
    "        response_format_json = json.dumps(complex_prompts)\n",
    "        response_format_sql = response_format_json.replace(\"'\", \"''\")\n",
    "        \n",
    "        extract_query = f\"\"\"\n",
    "        SELECT AI_EXTRACT(\n",
    "            text => '{clean_text}',\n",
    "            responseFormat => PARSE_JSON('{response_format_sql}')\n",
    "        ) as extracted_data\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"‚öôÔ∏è  Calling AI_EXTRACT with FULL complex prompts...\")\n",
    "        print(f\"   Context size: {len(clean_text):,} chars\")\n",
    "        print(f\"   Prompt sizes: {min(len(p) for p in complex_prompts.values())}-{max(len(p) for p in complex_prompts.values())} chars\\n\")\n",
    "        \n",
    "        result_a = session.sql(extract_query).collect()\n",
    "        \n",
    "        ai_extract_results = {}\n",
    "        extract_found = 0\n",
    "        \n",
    "        if result_a and result_a[0][0]:\n",
    "            extracted_json = result_a[0][0]\n",
    "            if isinstance(extracted_json, str):\n",
    "                extracted_data = json.loads(extracted_json)\n",
    "            else:\n",
    "                extracted_data = extracted_json\n",
    "            \n",
    "            if 'response' in extracted_data:\n",
    "                extracted_data = extracted_data['response']\n",
    "            \n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                value = extracted_data.get(trait_name)\n",
    "                if is_valid_value(value):\n",
    "                    ai_extract_results[trait_name] = value\n",
    "                    extract_found += 1\n",
    "                    print(f\"   ‚úì {trait_name:20s}: {str(value)[:50]}\")\n",
    "                else:\n",
    "                    ai_extract_results[trait_name] = None\n",
    "                    print(f\"   ‚úó {trait_name:20s}: Not found\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  AI_EXTRACT returned no results\")\n",
    "            for trait_name in traits_config_improved.keys():\n",
    "                ai_extract_results[trait_name] = None\n",
    "        \n",
    "        print(f\"\\n‚úÖ AI_EXTRACT: Found {extract_found}/{len(traits_config_improved)} traits\\n\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # METHOD B: COMPLETE with SIMPLIFIED prompts\n",
    "        # =============================================================================\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üìä Method B: COMPLETE with Simplified Prompts\\n\")\n",
    "        \n",
    "        simple_questions = {\n",
    "    \"Trait\": \"What is the main phenotypic trait studied (e.g., disease resistance, plant height, yield, drought tolerance)?\",\n",
    "    \n",
    "    \"Germplasm_Name\": \"What germplasm or population was used? Examples: B73 (maize), Nipponbare (rice), Col-0 (Arabidopsis), Chinese Spring (wheat), Williams 82 (soybean), diversity panels.\",\n",
    "    \n",
    "    \"Genome_Version\": \"What reference genome version was used? Examples: B73 RefGen_v4 (maize), IRGSP-1.0 (rice), TAIR10 (Arabidopsis), IWGSC v2.1 (wheat), Glycine_max_v4.0 (soybean).\",\n",
    "    \n",
    "    \"Chromosome\": \"What chromosome showed the strongest GWAS signal? Can be: number (5), letter (3A for wheat), sex chromosome (X, Y), organellar (MT), or linkage group (LG1).\",\n",
    "    \n",
    "    \"Physical_Position\": \"What is the physical position (bp or Mb) of the lead SNP?\",\n",
    "    \n",
    "    \"Gene\": \"What is the candidate gene? Examples: Zm00001d* (maize), LOC_Os* (rice), AT1G* (Arabidopsis), TraesCS* (wheat), Glyma.* (soybean).\",\n",
    "    \n",
    "    \"SNP_Name\": \"What is the lead SNP or marker name? May have prefixes like: PZE-, AX-, S1_, Chr*, or be position-based.\",\n",
    "    \n",
    "    \"Variant_ID\": \"What is the variant ID (e.g., rs123456789)? Note: Most plant studies don't use dbSNP IDs.\",\n",
    "    \n",
    "    \"Variant_Type\": \"What variant type was analyzed? Options: SNP, InDel, CNV, SV, PAV (presence/absence), Haplotype, or SSR/Microsatellite.\",\n",
    "    \n",
    "    \"Effect_Size\": \"What is the effect size, R-squared, or variance explained by the lead QTL?\",\n",
    "    \n",
    "    \"GWAS_Model\": \"What GWAS model or software was used? Examples: MLM, GLM, FarmCPU, BLINK, EMMAX, FastGWA, TASSEL, GAPIT, rMVP, regenie.\",\n",
    "    \n",
    "    \"Evidence_Type\": \"What type of study is this? Options: GWAS, QTL, Linkage, or Fine_Mapping.\",\n",
    "    \n",
    "    \"Allele\": \"What are the alleles for the lead variant? Formats: A/G, T>C, REF: A ALT: G, or favorable: T.\",\n",
    "    \n",
    "    \"Annotation\": \"What is the functional annotation? Examples: missense_variant, synonymous, intergenic_region, upstream_gene, 5_prime_UTR, intronic, regulatory_region.\",\n",
    "    \n",
    "    \"Candidate_Region\": \"What is the QTL region or confidence interval? Examples: chr1:145.6-146.1 Mb, bin 1.04, 10-12 cM, ¬±500 kb, 3A:450-480 Mb.\"\n",
    "}\n",
    "        \n",
    "        ai_complete_results = {}\n",
    "        complete_found = 0\n",
    "        complete_errors = 0\n",
    "        \n",
    "        print(\"‚öôÔ∏è  Processing traits individually with COMPLETE...\")\n",
    "        for idx, (trait_name, question) in enumerate(simple_questions.items(), 1):\n",
    "            try:\n",
    "                clean_question = question.replace(\"'\", \"''\")\n",
    "                \n",
    "                complete_query = f\"\"\"\n",
    "                SELECT AI_COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    '{clean_text[:12000]}'\n",
    "                    || '\\\\n\\\\n=== QUESTION ===\\\\n'\n",
    "                    || 'Based on the GWAS paper text above, answer this question:\\\\n'\n",
    "                    || '{clean_question}\\\\n\\\\n'\n",
    "                    || 'IMPORTANT RULES:\\\\n'\n",
    "                    || '1. Return ONLY the direct answer value (no explanations)\\\\n'\n",
    "                    || '2. Be specific and concise\\\\n'\n",
    "                    || '3. If the information is not in the text, return exactly: NOT_FOUND\\\\n'\n",
    "                    || '4. Do not return phrases like \\\"Looking through\\\" or \\\"Based on\\\"\\\\n\\\\n'\n",
    "                    || 'Answer:'\n",
    "                ) as result\n",
    "                \"\"\"\n",
    "                \n",
    "                result_b = session.sql(complete_query).collect()\n",
    "                \n",
    "                if result_b and result_b[0][0]:\n",
    "                    value = result_b[0][0].strip()\n",
    "                    value = value.replace('**', '').replace('Answer:', '').strip()\n",
    "                    \n",
    "                    if is_valid_value(value) and len(value) < 200:\n",
    "                        ai_complete_results[trait_name] = value\n",
    "                        complete_found += 1\n",
    "                        print(f\"   ‚úì {idx:2d}/{len(simple_questions)} {trait_name:20s}: {value[:50]}\")\n",
    "                    else:\n",
    "                        ai_complete_results[trait_name] = None\n",
    "                        print(f\"   ‚úó {idx:2d}/{len(simple_questions)} {trait_name:20s}: Not found\")\n",
    "                else:\n",
    "                    ai_complete_results[trait_name] = None\n",
    "                    complete_errors += 1\n",
    "                    print(f\"   ‚úó {idx:2d}/{len(simple_questions)} {trait_name:20s}: No result\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                ai_complete_results[trait_name] = None\n",
    "                complete_errors += 1\n",
    "                print(f\"   ‚úó {idx:2d}/{len(simple_questions)} {trait_name:20s}: Error - {str(e)[:30]}\")\n",
    "        \n",
    "        if complete_errors > 0:\n",
    "            print(f\"\\n   ‚ö†Ô∏è  {complete_errors} traits had errors\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ COMPLETE: Found {complete_found}/{len(traits_config_improved)} traits\\n\")\n",
    "        \n",
    "        # =============================================================================\n",
    "        # ‚úÖ IMPROVED MERGE: Smart logic with confidence tracking\n",
    "        # =============================================================================\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üìä INTELLIGENT MERGE: Choosing Best Values\\n\")\n",
    "        \n",
    "        text_extraction_results = {\n",
    "            \"document_id\": DOCUMENT_ID,\n",
    "            \"file_name\": PDF_FILENAME,\n",
    "            \"extraction_source\": \"dual_method_smart\"\n",
    "        }\n",
    "        \n",
    "        # ‚úÖ NEW: Track confidence levels\n",
    "        confidence_levels = {}\n",
    "        \n",
    "        fields_found = 0\n",
    "        fields_not_found = []\n",
    "        method_a_wins = 0\n",
    "        method_b_wins = 0\n",
    "        agreements = 0\n",
    "        \n",
    "        for trait_name in traits_config_improved.keys():\n",
    "            val_a = ai_extract_results.get(trait_name)\n",
    "            val_b = ai_complete_results.get(trait_name)\n",
    "            \n",
    "            a_exists = is_valid_value(val_a)\n",
    "            b_exists = is_valid_value(val_b)\n",
    "            \n",
    "            # ‚úÖ FIXED: Smart merge logic\n",
    "            if a_exists and b_exists:\n",
    "                # Both found - check if they agree\n",
    "                a_norm = str(val_a).lower().strip()\n",
    "                b_norm = str(val_b).lower().strip()\n",
    "                \n",
    "                if a_norm == b_norm:\n",
    "                    # ‚úÖ Perfect agreement - HIGH confidence!\n",
    "                    text_extraction_results[trait_name] = val_a\n",
    "                    confidence_levels[trait_name] = \"HIGH\"\n",
    "                    fields_found += 1\n",
    "                    agreements += 1\n",
    "                    print(f\"‚úÖ {trait_name:20s}: AGREE (HIGH) ‚Üí {str(val_a)[:50]}\")\n",
    "                else:\n",
    "                    # ‚ö†Ô∏è Disagreement - choose intelligently\n",
    "                    # Prefer longer, more specific values (likely more accurate)\n",
    "                    if len(str(val_a)) >= len(str(val_b)):\n",
    "                        text_extraction_results[trait_name] = val_a\n",
    "                        confidence_levels[trait_name] = \"MEDIUM\"\n",
    "                        method_a_wins += 1\n",
    "                        chosen = \"A (longer)\"\n",
    "                    else:\n",
    "                        text_extraction_results[trait_name] = val_b\n",
    "                        confidence_levels[trait_name] = \"MEDIUM\"\n",
    "                        method_b_wins += 1\n",
    "                        chosen = \"B (longer)\"\n",
    "                    \n",
    "                    fields_found += 1\n",
    "                    print(f\"‚ö†Ô∏è  {trait_name:20s}: DIFFER (MEDIUM) - chose {chosen}\")\n",
    "                    print(f\"      AI_EXTRACT: {str(val_a)[:40]}\")\n",
    "                    print(f\"      COMPLETE:   {str(val_b)[:40]}\")\n",
    "                    \n",
    "            elif a_exists:\n",
    "                # ‚úÖ Only AI_EXTRACT found it - USE IT (don't overwrite with null!)\n",
    "                text_extraction_results[trait_name] = val_a\n",
    "                confidence_levels[trait_name] = \"LOW\"\n",
    "                fields_found += 1\n",
    "                method_a_wins += 1\n",
    "                print(f\"üÖ∞Ô∏è  {trait_name:20s}: AI_EXTRACT only (LOW) ‚Üí {str(val_a)[:50]}\")\n",
    "                \n",
    "            elif b_exists:\n",
    "                # ‚úÖ Only COMPLETE found it - USE IT (don't overwrite with null!)\n",
    "                text_extraction_results[trait_name] = val_b\n",
    "                confidence_levels[trait_name] = \"LOW\"\n",
    "                fields_found += 1\n",
    "                method_b_wins += 1\n",
    "                print(f\"üÖ±Ô∏è  {trait_name:20s}: COMPLETE only (LOW) ‚Üí {str(val_b)[:50]}\")\n",
    "                \n",
    "            else:\n",
    "                # ‚ùå Neither found it - mark as missing\n",
    "                text_extraction_results[trait_name] = None\n",
    "                confidence_levels[trait_name] = \"NONE\"\n",
    "                fields_not_found.append(trait_name)\n",
    "                print(f\"‚ùå {trait_name:20s}: Not found by either method\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during dual extraction: {str(e)[:200]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    ai_extract_results = {}\n",
    "    ai_complete_results = {}\n",
    "    text_extraction_results = {}\n",
    "    confidence_levels = {}\n",
    "    fields_found = 0\n",
    "    fields_not_found = list(traits_config_improved.keys())\n",
    "    method_a_wins = 0\n",
    "    method_b_wins = 0\n",
    "    agreements = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìä Phase 1 Final Results (IMPROVED):\")\n",
    "print(f\"   ‚úÖ Total found: {fields_found}/{len(traits_config_improved)} traits\")\n",
    "print(f\"   ü§ù Agreements: {agreements} (HIGH confidence)\")\n",
    "print(f\"   üÖ∞Ô∏è  AI_EXTRACT wins: {method_a_wins}\")\n",
    "print(f\"   üÖ±Ô∏è  COMPLETE wins: {method_b_wins}\")\n",
    "print(f\"   ‚ùå Not found: {len(fields_not_found)} traits\")\n",
    "if fields_not_found:\n",
    "    print(f\"   Missing: {', '.join(fields_not_found[:5])}{'...' if len(fields_not_found) > 5 else ''}\")\n",
    "\n",
    "# ‚úÖ NEW: Show confidence distribution\n",
    "conf_counts = {}\n",
    "for conf in confidence_levels.values():\n",
    "    conf_counts[conf] = conf_counts.get(conf, 0) + 1\n",
    "print(f\"\\nüéØ Confidence Distribution:\")\n",
    "for level in [\"HIGH\", \"MEDIUM\", \"LOW\", \"NONE\"]:\n",
    "    count = conf_counts.get(level, 0)\n",
    "    if count > 0:\n",
    "        print(f\"   {level:10s}: {count:2d} traits\")\n",
    "\n",
    "print(\"\\n‚úÖ IMPROVEMENTS APPLIED:\")\n",
    "print(\"   ‚Ä¢ Full prompts (no truncation)\")\n",
    "print(\"   ‚Ä¢ 25K context (from 15K)\")\n",
    "print(\"   ‚Ä¢ Confidence tracking\")\n",
    "print(\"   ‚Ä¢ Smart merge (prefer agreement, then longer values)\")\n",
    "print(\"   ‚Ä¢ Never overwrite valid with invalid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç CELL 29-30: Phase 2 - Multimodal Search Validation\n",
    "\n",
    "**What this does:** Uses Cortex Search Service to validate and enrich Phase 1 results:\n",
    "- **Cell 29**: Generate image embeddings (if needed)\n",
    "- **Cell 30**: Multimodal search (text + images) + extraction\n",
    "- Search for ALL 15 traits using multimodal vectors (text + images)\n",
    "- Extract traits from search results using COMPLETE\n",
    "- Compare with Phase 1 to see if they agree\n",
    "- Enrich with additional findings from charts/graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Phase 2: Multimodal Search Validation (WORKING VERSION)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Strategy: Multimodal search + individual AI_COMPLETE calls\n",
      "   ‚Ä¢ Multimodal search working ‚úÖ\n",
      "   ‚Ä¢ Using AI_COMPLETE (proven, reliable)\n",
      "   ‚Ä¢ IMPROVED prompts aligned with Cell 24 enhancements\n",
      "   ‚Ä¢ Multi-species support, modern methods, flexible formats\n",
      "\n",
      "‚öôÔ∏è  Step 1: Multimodal Search\n",
      "\n",
      "üìã Search query: 'GWAS trait gene SNP allele chromosome position phenotype germplasm'\n",
      "\n",
      "   ‚úÖ Text vector: 1024 dims\n",
      "   ‚úÖ Image vector: 1024 dims\n",
      "\n",
      "   ‚úÖ Found 10 relevant pages\n",
      "   ‚è±Ô∏è  Search time: 1.2s\n",
      "\n",
      "‚öôÔ∏è  Step 2: Individual extraction with AI_COMPLETE\n",
      "   Context: 44,778 chars (using 15,002 chars)\n",
      "   Processing 15 traits with IMPROVED prompts...\n",
      "\n",
      "   ‚úì  1/15 Trait               : \"BPH resistance\"\n",
      "   ‚úì  2/15 Germplasm_Name      : \"502 rice varieties diversity panel\"\n",
      "   ‚úì  3/15 Genome_Version      : \"Nipponbare reference genome\"\n",
      "   ‚úó  4/15 Chromosome          : Not found\n",
      "   ‚úó  5/15 Physical_Position   : Not found\n",
      "   ‚úì  6/15 Gene                : \"LOC_Os06g03970, LOC_Os11g35890, LOC_Os11g35960, L\n",
      "   ‚úì  7/15 SNP_Name            : \"rs6_922708\"\n",
      "   ‚úì  8/15 Variant_ID          : \"rs2_23955573\"\n",
      "   ‚úì  9/15 Variant_Type        : \"SNP\"\n",
      "   ‚úì 10/15 Effect_Size         : \"19.69%\"\n",
      "   ‚úó 11/15 GWAS_Model          : Not found\n",
      "   ‚úì 12/15 Evidence_Type       : \"GWAS\"\n",
      "   ‚úó 13/15 Allele              : Not found\n",
      "   ‚úó 14/15 Annotation          : Not found\n",
      "   ‚úì 15/15 Candidate_Region    : \"23.86-24.06 Mbp, 21.27-21.52 Mbp, 0.81-1.58 Mbp, \n",
      "\n",
      "   ‚úÖ Extraction completed in 38.2s\n",
      "\n",
      "================================================================================\n",
      "üìä Comparison: Phase 1 (Text) vs Phase 2 (Multimodal)\n",
      "\n",
      "‚ö†Ô∏è  Trait               : DIFFER [MEDIUM]\n",
      "      Phase 1: Resistance to brown planthopper\n",
      "      Phase 2: \"BPH resistance\"\n",
      "‚úÖ Germplasm_Name      : AGREE [MEDIUM] ‚Üí \"502 rice varieties diversity panel\"\n",
      "‚ö†Ô∏è  Genome_Version      : DIFFER [MEDIUM]\n",
      "      Phase 1: IRGSP-1.0\n",
      "      Phase 2: \"Nipponbare reference genome\"\n",
      "üìù GWAS_Model          : TEXT-ONLY ‚Üí EMMAX\n",
      "‚úÖ Evidence_Type       : AGREE [MEDIUM] ‚Üí \"GWAS\"\n",
      "üìù Chromosome          : TEXT-ONLY ‚Üí \"11\"\n",
      "‚ùå Physical_Position   : NOT FOUND\n",
      "‚ö†Ô∏è  Gene                : DIFFER [MEDIUM]\n",
      "      Phase 1: ['RLK', 'NB-LRR', 'LRR']\n",
      "      Phase 2: \"LOC_Os06g03970, LOC_Os11g35890, LOC_Os11g35960, L\n",
      "‚ö†Ô∏è  SNP_Name            : DIFFER [MEDIUM]\n",
      "      Phase 1: ['rs1234567', 'rs2345678', 'rs3456789', 'rs4567890\n",
      "      Phase 2: \"rs6_922708\"\n",
      "üÜï Variant_ID          : NEW [MEDIUM] ‚Üí \"rs2_23955573\"\n",
      "‚ö†Ô∏è  Variant_Type        : DIFFER [MEDIUM]\n",
      "      Phase 1: SNP (single nucleotide polymorphism)\n",
      "      Phase 2: \"SNP\"\n",
      "üÜï Effect_Size         : NEW [MEDIUM] ‚Üí \"19.69%\"\n",
      "‚ùå Allele              : NOT FOUND\n",
      "‚ùå Annotation          : NOT FOUND\n",
      "‚ö†Ô∏è  Candidate_Region    : DIFFER [MEDIUM]\n",
      "      Phase 1: chr1:145.6-146.1 Mb\n",
      "      Phase 2: \"23.86-24.06 Mbp, 21.27-21.52 Mbp, 0.81-1.58 Mbp, \n",
      "\n",
      "================================================================================\n",
      "üìä Phase 2 Results:\n",
      "   ‚úÖ Agreements: 2 traits\n",
      "   ‚ö†Ô∏è  Disagreements: 6 traits\n",
      "   üÜï New findings: 2 traits\n",
      "   üìà Total from Phase 2: 10/15 traits\n",
      "\n",
      "‚úÖ IMPROVED APPROACH:\n",
      "   ‚Ä¢ Multimodal search: SUCCESS ‚úÖ\n",
      "   ‚Ä¢ AI_COMPLETE with enhanced prompts ‚úÖ\n",
      "   ‚Ä¢ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean)\n",
      "   ‚Ä¢ Modern methods (EMMAX, FastGWA, rMVP, regenie)\n",
      "   ‚Ä¢ Flexible formats (3A, X, Y, MT, PAV, Haplotypes)\n",
      "   ‚Ä¢ Confidence tracking: ENABLED ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: MULTIMODAL SEARCH + COMPLETE (Validation & Enrichment)\n",
    "# ‚úÖ WORKING: Using proven AI_COMPLETE approach with IMPROVED prompts\n",
    "print(\"\\nüîç Phase 2: Multimodal Search Validation (WORKING VERSION)\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"‚úÖ Strategy: Multimodal search + individual AI_COMPLETE calls\")\n",
    "print(\"   ‚Ä¢ Multimodal search working ‚úÖ\")\n",
    "print(\"   ‚Ä¢ Using AI_COMPLETE (proven, reliable)\")\n",
    "print(\"   ‚Ä¢ IMPROVED prompts aligned with Cell 24 enhancements\")\n",
    "print(\"   ‚Ä¢ Multi-species support, modern methods, flexible formats\\n\")\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Helper function to validate values\n",
    "def is_valid_value(val):\n",
    "    \"\"\"Check if value is meaningful (not 'NOT_FOUND' or garbage)\"\"\"\n",
    "    if not val:\n",
    "        return False\n",
    "    \n",
    "    s = str(val).strip().strip('\"').strip(\"'\").strip()\n",
    "    s_upper = s.upper()\n",
    "    \n",
    "    bad_values = ['NOT_FOUND', 'NOT FOUND', 'NONE', 'NULL', 'N/A', 'NA', '']\n",
    "    if s_upper in bad_values:\n",
    "        return False\n",
    "    \n",
    "    bad_patterns = ['LOOKING THROUGH', 'BASED ON', 'NOT MENTIONED', 'NOT PROVIDED', \n",
    "                    'DOES NOT', 'NOT SPECIFIED', 'NOT AVAILABLE', 'NOT IN THE TEXT']\n",
    "    if any(pattern in s_upper for pattern in bad_patterns):\n",
    "        return False\n",
    "    \n",
    "    if len(s) < 2:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ‚ú® IMPROVED: Aligned with Cell 24 enhancements\n",
    "simple_questions = {\n",
    "    \"Trait\": \"What is the main phenotypic trait studied (e.g., disease resistance, plant height, yield, drought tolerance)?\",\n",
    "    \n",
    "    \"Germplasm_Name\": \"What germplasm or population was used? Examples: B73 (maize), Nipponbare (rice), Col-0 (Arabidopsis), Chinese Spring (wheat), Williams 82 (soybean), diversity panels.\",\n",
    "    \n",
    "    \"Genome_Version\": \"What reference genome version was used? Examples: B73 RefGen_v4 (maize), IRGSP-1.0 (rice), TAIR10 (Arabidopsis), IWGSC v2.1 (wheat), Glycine_max_v4.0 (soybean).\",\n",
    "    \n",
    "    \"Chromosome\": \"What chromosome showed the strongest GWAS signal? Can be: number (5), letter (3A for wheat), sex chromosome (X, Y), organellar (MT), or linkage group (LG1).\",\n",
    "    \n",
    "    \"Physical_Position\": \"What is the physical position (bp or Mb) of the lead SNP?\",\n",
    "    \n",
    "    \"Gene\": \"What is the candidate gene? Examples: Zm00001d* (maize), LOC_Os* (rice), AT1G* (Arabidopsis), TraesCS* (wheat), Glyma.* (soybean).\",\n",
    "    \n",
    "    \"SNP_Name\": \"What is the lead SNP or marker name? May have prefixes like: PZE-, AX-, S1_, Chr*, or be position-based.\",\n",
    "    \n",
    "    \"Variant_ID\": \"What is the variant ID (e.g., rs123456789)? Note: Most plant studies don't use dbSNP IDs.\",\n",
    "    \n",
    "    \"Variant_Type\": \"What variant type was analyzed? Options: SNP, InDel, CNV, SV, PAV (presence/absence), Haplotype, or SSR/Microsatellite.\",\n",
    "    \n",
    "    \"Effect_Size\": \"What is the effect size, R-squared, or variance explained by the lead QTL?\",\n",
    "    \n",
    "    \"GWAS_Model\": \"What GWAS model or software was used? Examples: MLM, GLM, FarmCPU, BLINK, EMMAX, FastGWA, TASSEL, GAPIT, rMVP, regenie.\",\n",
    "    \n",
    "    \"Evidence_Type\": \"What type of study is this? Options: GWAS, QTL, Linkage, or Fine_Mapping.\",\n",
    "    \n",
    "    \"Allele\": \"What are the alleles for the lead variant? Formats: A/G, T>C, REF: A ALT: G, or favorable: T.\",\n",
    "    \n",
    "    \"Annotation\": \"What is the functional annotation? Examples: missense_variant, synonymous, intergenic_region, upstream_gene, 5_prime_UTR, intronic, regulatory_region.\",\n",
    "    \n",
    "    \"Candidate_Region\": \"What is the QTL region or confidence interval? Examples: chr1:145.6-146.1 Mb, bin 1.04, 10-12 cM, ¬±500 kb, 3A:450-480 Mb.\"\n",
    "}\n",
    "\n",
    "# Initialize results\n",
    "multimodal_extraction_results = {}\n",
    "multimodal_confidence_levels = {}\n",
    "multimodal_fields_found = 0\n",
    "agreements = 0\n",
    "disagreements = 0\n",
    "phase2_new_findings = 0\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Step 1: Multimodal Search\\n\")\n",
    "    \n",
    "    # Build search query\n",
    "    search_query = \"GWAS trait gene SNP allele chromosome position phenotype germplasm\"\n",
    "    print(f\"üìã Search query: '{search_query}'\\n\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embed_query = f\"\"\"\n",
    "    SELECT\n",
    "        AI_EMBED('snowflake-arctic-embed-l-v2.0-8k', '{search_query}') as text_vector,\n",
    "        AI_EMBED('voyage-multimodal-3', '{search_query}') as image_vector\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = session.sql(embed_query).collect()\n",
    "    text_vector = [float(x) for x in safe_vector_conversion(embeddings[0][0])]\n",
    "    image_vector = [float(x) for x in safe_vector_conversion(embeddings[0][1])]\n",
    "    \n",
    "    print(f\"   ‚úÖ Text vector: {len(text_vector)} dims\")\n",
    "    print(f\"   ‚úÖ Image vector: {len(image_vector)} dims\\n\")\n",
    "    \n",
    "    # Build multimodal search query\n",
    "    query_json = {\n",
    "        \"multi_index_query\": {\n",
    "            \"page_text\": [{\"text\": search_query}],\n",
    "            \"text_embedding\": [{\"vector\": text_vector}],\n",
    "            \"image_embedding\": [{\"vector\": image_vector}]\n",
    "        },\n",
    "        \"columns\": [\"document_id\", \"page_text\", \"page_number\"],\n",
    "        \"limit\": 10,\n",
    "        \"filter\": {\n",
    "            \"@eq\": {\n",
    "                \"document_id\": DOCUMENT_ID\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    query_str = json.dumps(query_json).replace(\"'\", \"''\")\n",
    "    \n",
    "    search_sql = f\"\"\"\n",
    "    SELECT\n",
    "      result.value:document_id::VARCHAR as document_id,\n",
    "      result.value:page_text::VARCHAR as page_text,\n",
    "      result.value:page_number::INT as page_number\n",
    "    FROM TABLE(\n",
    "      FLATTEN(\n",
    "        PARSE_JSON(\n",
    "          SNOWFLAKE.CORTEX.SEARCH_PREVIEW(\n",
    "            '{DATABASE_NAME}.{SCHEMA_PROCESSING}.MULTIMODAL_SEARCH_SERVICE',\n",
    "            '{query_str}'\n",
    "          )\n",
    "        )['results']\n",
    "      )\n",
    "    ) as result\n",
    "    \"\"\"\n",
    "    \n",
    "    search_results = session.sql(search_sql).collect()\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    if not search_results:\n",
    "        print(f\"   ‚ö†Ô∏è  No results found\")\n",
    "        multimodal_extraction_results = {}\n",
    "        multimodal_fields_found = 0\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Found {len(search_results)} relevant pages\")\n",
    "        print(f\"   ‚è±Ô∏è  Search time: {search_time:.1f}s\\n\")\n",
    "        \n",
    "        # Concatenate search results\n",
    "        search_context = '\\n\\n'.join([f\"[Page {row[2]}]\\n{row[1]}\" for row in search_results])\n",
    "        context_length = len(search_context)\n",
    "        \n",
    "        # Use reasonable context size\n",
    "        clean_context = search_context[:15000].replace(\"'\", \"''\").replace('\\n', ' ').replace('\\r', ' ')\n",
    "        \n",
    "        print(f\"‚öôÔ∏è  Step 2: Individual extraction with AI_COMPLETE\")\n",
    "        print(f\"   Context: {context_length:,} chars (using {len(clean_context):,} chars)\")\n",
    "        print(f\"   Processing 15 traits with IMPROVED prompts...\\n\")\n",
    "        \n",
    "        extraction_errors = 0\n",
    "        \n",
    "        # Extract each trait individually with IMPROVED questions\n",
    "        for idx, (trait_name, question) in enumerate(simple_questions.items(), 1):\n",
    "            try:\n",
    "                clean_question = question.replace(\"'\", \"''\")\n",
    "                \n",
    "                complete_query = f\"\"\"\n",
    "                SELECT AI_COMPLETE(\n",
    "                    'claude-4-sonnet',\n",
    "                    '{clean_context[:12000]}'\n",
    "                    || '\\\\n\\\\n=== QUESTION ===\\\\n'\n",
    "                    || 'Based on the GWAS paper text above, answer this question:\\\\n'\n",
    "                    || '{clean_question}\\\\n\\\\n'\n",
    "                    || 'IMPORTANT RULES:\\\\n'\n",
    "                    || '1. Return ONLY the direct answer value (no explanations)\\\\n'\n",
    "                    || '2. Be specific and concise\\\\n'\n",
    "                    || '3. If the information is not in the text, return exactly: NOT_FOUND\\\\n'\n",
    "                    || '4. Do not return phrases like \\\"Looking through\\\" or \\\"Based on\\\"\\\\n\\\\n'\n",
    "                    || 'Answer:'\n",
    "                ) as result\n",
    "                \"\"\"\n",
    "                \n",
    "                result = session.sql(complete_query).collect()\n",
    "                \n",
    "                if result and result[0][0]:\n",
    "                    value = result[0][0].strip()\n",
    "                    value = value.replace('**', '').replace('Answer:', '').strip()\n",
    "                    \n",
    "                    if is_valid_value(value) and len(value) < 200:\n",
    "                        multimodal_extraction_results[trait_name] = value\n",
    "                        multimodal_confidence_levels[trait_name] = \"MEDIUM\"\n",
    "                        multimodal_fields_found += 1\n",
    "                        print(f\"   ‚úì {idx:2d}/15 {trait_name:20s}: {value[:50]}\")\n",
    "                    else:\n",
    "                        multimodal_extraction_results[trait_name] = None\n",
    "                        multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "                        print(f\"   ‚úó {idx:2d}/15 {trait_name:20s}: Not found\")\n",
    "                else:\n",
    "                    multimodal_extraction_results[trait_name] = None\n",
    "                    multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "                    extraction_errors += 1\n",
    "                    print(f\"   ‚úó {idx:2d}/15 {trait_name:20s}: No result\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                multimodal_extraction_results[trait_name] = None\n",
    "                multimodal_confidence_levels[trait_name] = \"NONE\"\n",
    "                extraction_errors += 1\n",
    "                print(f\"   ‚úó {idx:2d}/15 {trait_name:20s}: Error\")\n",
    "        \n",
    "        if extraction_errors > 0:\n",
    "            print(f\"\\n   ‚ö†Ô∏è  {extraction_errors} traits had extraction errors\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n   ‚úÖ Extraction completed in {total_time:.1f}s\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    \n",
    "    # Compare with Phase 1\n",
    "    print(\"üìä Comparison: Phase 1 (Text) vs Phase 2 (Multimodal)\\n\")\n",
    "    \n",
    "    for trait_name in traits_config_improved.keys():\n",
    "        phase1_value = text_extraction_results.get(trait_name)\n",
    "        phase2_value = multimodal_extraction_results.get(trait_name)\n",
    "        phase2_conf = multimodal_confidence_levels.get(trait_name, \"NONE\")\n",
    "        \n",
    "        p1_exists = is_valid_value(phase1_value)\n",
    "        p2_exists = is_valid_value(phase2_value)\n",
    "        \n",
    "        if p1_exists and p2_exists:\n",
    "            if str(phase1_value).lower().strip() == str(phase2_value).lower().strip():\n",
    "                agreements += 1\n",
    "                print(f\"‚úÖ {trait_name:20s}: AGREE [{phase2_conf}] ‚Üí {str(phase1_value)[:50]}\")\n",
    "            else:\n",
    "                disagreements += 1\n",
    "                print(f\"‚ö†Ô∏è  {trait_name:20s}: DIFFER [{phase2_conf}]\")\n",
    "                print(f\"      Phase 1: {str(phase1_value)[:50]}\")\n",
    "                print(f\"      Phase 2: {str(phase2_value)[:50]}\")\n",
    "        elif not p1_exists and p2_exists:\n",
    "            phase2_new_findings += 1\n",
    "            print(f\"üÜï {trait_name:20s}: NEW [{phase2_conf}] ‚Üí {str(phase2_value)[:50]}\")\n",
    "        elif p1_exists and not p2_exists:\n",
    "            print(f\"üìù {trait_name:20s}: TEXT-ONLY ‚Üí {str(phase1_value)[:50]}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {trait_name:20s}: NOT FOUND\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR: {str(e)[:200]}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    multimodal_extraction_results = {}\n",
    "    multimodal_confidence_levels = {}\n",
    "    multimodal_fields_found = 0\n",
    "    agreements = 0\n",
    "    disagreements = 0\n",
    "    phase2_new_findings = 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üìä Phase 2 Results:\")\n",
    "print(f\"   ‚úÖ Agreements: {agreements} traits\")\n",
    "print(f\"   ‚ö†Ô∏è  Disagreements: {disagreements} traits\")\n",
    "print(f\"   üÜï New findings: {phase2_new_findings} traits\")\n",
    "print(f\"   üìà Total from Phase 2: {multimodal_fields_found}/{len(traits_config_improved)} traits\")\n",
    "print(f\"\\n‚úÖ IMPROVED APPROACH:\")\n",
    "print(f\"   ‚Ä¢ Multimodal search: SUCCESS ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ AI_COMPLETE with enhanced prompts ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Multi-species examples (maize, rice, wheat, Arabidopsis, soybean)\")\n",
    "print(f\"   ‚Ä¢ Modern methods (EMMAX, FastGWA, rMVP, regenie)\")\n",
    "print(f\"   ‚Ä¢ Flexible formats (3A, X, Y, MT, PAV, Haplotypes)\")\n",
    "print(f\"   ‚Ä¢ Confidence tracking: ENABLED ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2da1ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Phase 3: SIMPLIFIED Smart Merge\n",
      "================================================================================\n",
      "üéØ Strategy: Simple preference order + exact matching\n",
      "   Priority: Multimodal > Text-only > Batch\n",
      "   Confidence:\n",
      "     HIGH   = All 3 agree\n",
      "     MEDIUM = Any 2 agree, OR multimodal only\n",
      "     LOW    = Text-only or batch only\n",
      "\n",
      "üìä Merging results...\n",
      "\n",
      "üìå Germplasm_Name: B_C_agree (MEDIUM)\n",
      "   Value: \"502 rice varieties diversity panel\"\n",
      "   ‚úì Methods agree!\n",
      "\n",
      "üìå GWAS_Model: A_only (LOW)\n",
      "   Value: EMMAX\n",
      "\n",
      "üìå Evidence_Type: B_C_agree (MEDIUM)\n",
      "   Value: \"GWAS\"\n",
      "   ‚úì Methods agree!\n",
      "\n",
      "üìå Chromosome: B_only (LOW)\n",
      "   Value: \"11\"\n",
      "\n",
      "üìå Variant_Type: B_C_agree (MEDIUM)\n",
      "   Value: \"SNP\"\n",
      "   ‚úì Methods agree!\n",
      "\n",
      "================================================================================\n",
      "üìä Merge Summary:\n",
      "\n",
      "Total traits: 15\n",
      "‚úÖ Extracted: 12\n",
      "‚ùå Not found: 3\n",
      "üìà Success rate: 80.0%\n",
      "\n",
      "üéØ Confidence levels:\n",
      "   HIGH      :  0 traits\n",
      "   MEDIUM    : 10 traits\n",
      "   LOW       :  2 traits\n",
      "   NONE      :  0 traits\n",
      "\n",
      "üìã Value sources:\n",
      "   C_only              : 7\n",
      "   B_C_agree           : 3\n",
      "   A_only              : 1\n",
      "   B_only              : 1\n",
      "\n",
      "‚úÖ Simple merge complete!\n",
      "   ‚Ä¢ No fuzzy matching complexity\n",
      "   ‚Ä¢ No expensive LLM tie-breakers\n",
      "   ‚Ä¢ Clear, debuggable logic\n",
      "   ‚Ä¢ Fast execution\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Phase 3: SIMPLIFIED Smart Merge (No Over-Engineering!)\n",
    "# ========================================\n",
    "# Strategy: Simple, clear, debuggable\n",
    "# 1. Prefer multimodal > text-only > batch\n",
    "# 2. If 2+ methods agree exactly, boost confidence\n",
    "# 3. Multimodal-only results get MEDIUM confidence (best method!)\n",
    "# 4. No fuzzy matching, no LLM tie-breakers\n",
    "# ========================================\n",
    "\n",
    "print(\"\\nüíæ Phase 3: SIMPLIFIED Smart Merge\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ Strategy: Simple preference order + exact matching\")\n",
    "print(\"   Priority: Multimodal > Text-only > Batch\")\n",
    "print(\"   Confidence:\")\n",
    "print(\"     HIGH   = All 3 agree\")\n",
    "print(\"     MEDIUM = Any 2 agree, OR multimodal only\")\n",
    "print(\"     LOW    = Text-only or batch only\\n\")\n",
    "\n",
    "# Simple merge function\n",
    "def simple_merge(trait_name, method_a, method_b, method_c):\n",
    "    \"\"\"\n",
    "    Simple merge logic:\n",
    "    1. If all 3 agree exactly ‚Üí HIGH confidence\n",
    "    2. If any 2 agree exactly ‚Üí MEDIUM confidence  \n",
    "    3. Multimodal only ‚Üí MEDIUM confidence (best method!)\n",
    "    4. Text-only or Batch only ‚Üí LOW confidence\n",
    "    \"\"\"\n",
    "    # Clean up None/empty values\n",
    "    a = str(method_a).strip() if method_a else None\n",
    "    b = str(method_b).strip() if method_b else None\n",
    "    c = str(method_c).strip() if method_c else None\n",
    "    \n",
    "    # Remove \"NOT_FOUND\" type values\n",
    "    if a and 'NOT_FOUND' in a.upper(): a = None\n",
    "    if b and 'NOT_FOUND' in b.upper(): b = None\n",
    "    if c and 'NOT_FOUND' in c.upper(): c = None\n",
    "    \n",
    "    # Count how many found something\n",
    "    found_count = sum(1 for v in [a, b, c] if v)\n",
    "    \n",
    "    if found_count == 0:\n",
    "        return None, \"Not reported\", \"NONE\"\n",
    "    \n",
    "    # Check for exact agreements\n",
    "    if a and b and c and a == b == c:\n",
    "        return a, \"All_agree\", \"HIGH\"\n",
    "    elif a and b and a == b:\n",
    "        return a, \"A_B_agree\", \"MEDIUM\"\n",
    "    elif a and c and a == c:\n",
    "        return a, \"A_C_agree\", \"MEDIUM\"\n",
    "    elif b and c and b == c:\n",
    "        return b, \"B_C_agree\", \"MEDIUM\"\n",
    "    \n",
    "    # No agreement - use preference order\n",
    "    # ‚úÖ FIXED: Confidence reflects method quality!\n",
    "    if c:  # Multimodal preferred - our BEST method!\n",
    "        return c, \"C_only\", \"MEDIUM\"  # ‚úÖ Best method deserves MEDIUM confidence\n",
    "    elif b:  # Text-only second\n",
    "        return b, \"B_only\", \"LOW\" \n",
    "    else:  # Batch last resort - weakest method\n",
    "        return a, \"A_only\", \"LOW\"\n",
    "\n",
    "# Merge all results\n",
    "final_results = {}\n",
    "field_citations = {}\n",
    "confidence_levels = {}\n",
    "\n",
    "print(\"üìä Merging results...\\n\")\n",
    "\n",
    "for trait_name in traits_config_improved.keys():\n",
    "    # Get values from all 3 methods\n",
    "    method_a = ai_extract_results.get(trait_name)\n",
    "    method_b = ai_complete_results.get(trait_name) \n",
    "    method_c = multimodal_extraction_results.get(trait_name)\n",
    "    \n",
    "    # Simple merge\n",
    "    value, source, confidence = simple_merge(trait_name, method_a, method_b, method_c)\n",
    "    \n",
    "    if value:\n",
    "        final_results[trait_name] = value\n",
    "        field_citations[trait_name] = source\n",
    "        confidence_levels[trait_name] = confidence\n",
    "        \n",
    "        # Only print interesting cases\n",
    "        if source != \"C_only\":  # Don't print default case\n",
    "            print(f\"üìå {trait_name}: {source} ({confidence})\")\n",
    "            print(f\"   Value: {str(value)[:60]}\")\n",
    "            if source in [\"All_agree\", \"A_B_agree\", \"A_C_agree\", \"B_C_agree\"]:\n",
    "                print(f\"   ‚úì Methods agree!\")\n",
    "            print()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä Merge Summary:\\n\")\n",
    "\n",
    "extracted = len([v for v in final_results.values() if v])\n",
    "total = len(traits_config_improved)\n",
    "\n",
    "print(f\"Total traits: {total}\")\n",
    "print(f\"‚úÖ Extracted: {extracted}\")\n",
    "print(f\"‚ùå Not found: {total - extracted}\")\n",
    "print(f\"üìà Success rate: {extracted/total*100:.1f}%\\n\")\n",
    "\n",
    "# Confidence breakdown\n",
    "conf_counts = {}\n",
    "for conf in confidence_levels.values():\n",
    "    conf_counts[conf] = conf_counts.get(conf, 0) + 1\n",
    "\n",
    "print(\"üéØ Confidence levels:\")\n",
    "for level in [\"HIGH\", \"MEDIUM\", \"LOW\", \"NONE\"]:\n",
    "    count = conf_counts.get(level, 0)\n",
    "    print(f\"   {level:10}: {count:2} traits\")\n",
    "\n",
    "# Source breakdown\n",
    "source_counts = {}\n",
    "for source in field_citations.values():\n",
    "    source_counts[source] = source_counts.get(source, 0) + 1\n",
    "\n",
    "print(\"\\nüìã Value sources:\")\n",
    "for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"   {source:20}: {count}\")\n",
    "\n",
    "print(\"\\n‚úÖ Simple merge complete!\")\n",
    "print(\"   ‚Ä¢ No fuzzy matching complexity\")\n",
    "print(\"   ‚Ä¢ No expensive LLM tie-breakers\")\n",
    "print(\"   ‚Ä¢ Clear, debuggable logic\")\n",
    "print(\"   ‚Ä¢ Fast execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60447923",
   "metadata": {},
   "source": [
    "## üìä Final Results Display\n",
    "\n",
    "This cell provides a comprehensive view of all extracted GWAS traits in two formats:\n",
    "1. **Checklist Format** - Easy visual overview with ‚úÖ/‚ùå status\n",
    "2. **Structured Table** - Detailed data view (Streamlit-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba517b5",
   "metadata": {},
   "source": [
    "## üé® Visual Summary (Streamlit-Style Display)\n",
    "\n",
    "Interactive-style metrics and data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b612e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .metric-container {\n",
       "        display: flex;\n",
       "        gap: 20px;\n",
       "        margin: 20px 0;\n",
       "        flex-wrap: wrap;\n",
       "    }\n",
       "    .metric-card {\n",
       "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "        border-radius: 10px;\n",
       "        padding: 20px;\n",
       "        min-width: 150px;\n",
       "        color: white;\n",
       "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
       "    }\n",
       "    .metric-card.success {\n",
       "        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);\n",
       "    }\n",
       "    .metric-card.warning {\n",
       "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
       "    }\n",
       "    .metric-card.info {\n",
       "        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);\n",
       "    }\n",
       "    .metric-label {\n",
       "        font-size: 12px;\n",
       "        opacity: 0.9;\n",
       "        margin-bottom: 5px;\n",
       "    }\n",
       "    .metric-value {\n",
       "        font-size: 32px;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .metric-delta {\n",
       "        font-size: 14px;\n",
       "        margin-top: 5px;\n",
       "        opacity: 0.9;\n",
       "    }\n",
       "    .data-table {\n",
       "        width: 100%;\n",
       "        border-collapse: collapse;\n",
       "        margin: 20px 0;\n",
       "        background: white;\n",
       "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
       "        border-radius: 8px;\n",
       "        overflow: hidden;\n",
       "    }\n",
       "    .data-table th {\n",
       "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
       "        color: white;\n",
       "        padding: 12px;\n",
       "        text-align: left;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .data-table td {\n",
       "        padding: 10px 12px;\n",
       "        border-bottom: 1px solid #e0e0e0;\n",
       "    }\n",
       "    .data-table tr:last-child td {\n",
       "        border-bottom: none;\n",
       "    }\n",
       "    .data-table tr:hover {\n",
       "        background-color: #f5f5f5;\n",
       "    }\n",
       "    .status-badge {\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .status-success {\n",
       "        background-color: #e8f5e9;\n",
       "        color: #2e7d32;\n",
       "    }\n",
       "    .status-error {\n",
       "        background-color: #ffebee;\n",
       "        color: #c62828;\n",
       "    }\n",
       "    .conf-high {\n",
       "        background-color: #e3f2fd;\n",
       "        color: #1565c0;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    .conf-medium {\n",
       "        background-color: #fff3e0;\n",
       "        color: #ef6c00;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    .conf-low {\n",
       "        background-color: #fce4ec;\n",
       "        color: #c2185b;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    .conf-none {\n",
       "        background-color: #f5f5f5;\n",
       "        color: #757575;\n",
       "        padding: 4px 8px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 11px;\n",
       "    }\n",
       "    h2 {\n",
       "        color: #333;\n",
       "        margin-top: 30px;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<h2>üìä Extraction Metrics</h2>\n",
       "\n",
       "<div class=\"metric-container\">\n",
       "    <div class=\"metric-card success\">\n",
       "        <div class=\"metric-label\">TRAITS EXTRACTED</div>\n",
       "        <div class=\"metric-value\">12/15</div>\n",
       "        <div class=\"metric-delta\">80.0% success rate</div>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"metric-card info\">\n",
       "        <div class=\"metric-label\">HIGH CONFIDENCE</div>\n",
       "        <div class=\"metric-value\">0</div>\n",
       "        <div class=\"metric-delta\">0% of total</div>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"metric-card info\">\n",
       "        <div class=\"metric-label\">MEDIUM CONFIDENCE</div>\n",
       "        <div class=\"metric-value\">10</div>\n",
       "        <div class=\"metric-delta\">67% of total</div>\n",
       "    </div>\n",
       "\n",
       "    <div class=\"metric-card warning\">\n",
       "        <div class=\"metric-label\">LOW CONFIDENCE</div>\n",
       "        <div class=\"metric-value\">2</div>\n",
       "        <div class=\"metric-delta\">13% of total</div>\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<h2>üìã Extracted Traits Table</h2>\n",
       "\n",
       "<table class=\"data-table\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Status</th>\n",
       "            <th>Trait Name</th>\n",
       "            <th>Extracted Value</th>\n",
       "            <th>Confidence</th>\n",
       "            <th>Source Method</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "\n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Trait</strong></td>\n",
       "            <td>\"BPH resistance\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Germplasm_Name</strong></td>\n",
       "            <td>\"502 rice varieties diversity panel\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>B_C_agree</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Genome_Version</strong></td>\n",
       "            <td>\"Nipponbare reference genome\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>GWAS_Model</strong></td>\n",
       "            <td>EMMAX</td>\n",
       "            <td><span class=\"conf-low\">LOW</span></td>\n",
       "            <td><code>A_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Evidence_Type</strong></td>\n",
       "            <td>\"GWAS\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>B_C_agree</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Chromosome</strong></td>\n",
       "            <td>\"11\"</td>\n",
       "            <td><span class=\"conf-low\">LOW</span></td>\n",
       "            <td><code>B_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-error\">‚úó Missing</span></td>\n",
       "            <td><strong>Physical_Position</strong></td>\n",
       "            <td><span style=\"color: #999;\">Not found</span></td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>Not found</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Gene</strong></td>\n",
       "            <td>\"LOC_Os06g03970, LOC_Os11g35890, LOC_Os11g35960, LOC_Os11g35980, LOC_Os11g36020,</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>SNP_Name</strong></td>\n",
       "            <td>\"rs6_922708\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Variant_ID</strong></td>\n",
       "            <td>\"rs2_23955573\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Variant_Type</strong></td>\n",
       "            <td>\"SNP\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>B_C_agree</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Effect_Size</strong></td>\n",
       "            <td>\"19.69%\"</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-error\">‚úó Missing</span></td>\n",
       "            <td><strong>Allele</strong></td>\n",
       "            <td><span style=\"color: #999;\">Not found</span></td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>Not found</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-error\">‚úó Missing</span></td>\n",
       "            <td><strong>Annotation</strong></td>\n",
       "            <td><span style=\"color: #999;\">Not found</span></td>\n",
       "            <td><span class=\"conf-none\">NONE</span></td>\n",
       "            <td><code>Not found</code></td>\n",
       "        </tr>\n",
       "    \n",
       "        <tr>\n",
       "            <td><span class=\"status-badge status-success\">‚úì Found</span></td>\n",
       "            <td><strong>Candidate_Region</strong></td>\n",
       "            <td>\"23.86-24.06 Mbp, 21.27-21.52 Mbp, 0.81-1.58 Mbp, 20.99-21.19 Mbp, 1.96-2.16 Mbp</td>\n",
       "            <td><span class=\"conf-medium\">MEDIUM</span></td>\n",
       "            <td><code>C_only</code></td>\n",
       "        </tr>\n",
       "    \n",
       "    </tbody>\n",
       "</table>\n",
       "\n",
       "<div style=\"margin-top: 30px; padding: 15px; background: #f5f5f5; border-radius: 8px;\">\n",
       "    <h3 style=\"margin-top: 0; color: #333;\">üí° Interpretation Guide</h3>\n",
       "    <ul style=\"color: #666; line-height: 1.8;\">\n",
       "        <li><strong>High Confidence</strong>: All 3 extraction methods agree</li>\n",
       "        <li><strong>Medium Confidence</strong>: 2 out of 3 methods agree</li>\n",
       "        <li><strong>Low Confidence</strong>: Single method extraction (prefer multimodal &gt; text &gt; batch)</li>\n",
       "        <li><strong>Source Method</strong>: Which extraction approach(es) found this trait</li>\n",
       "    </ul>\n",
       "</div>\n",
       "\n",
       "<div style=\"margin-top: 20px; padding: 15px; background: #e3f2fd; border-radius: 8px; border-left: 4px solid #1976d2;\">\n",
       "    <strong>üéØ Next Steps:</strong>\n",
       "    <ol style=\"color: #1565c0; line-height: 1.8; margin: 10px 0 0 0;\">\n",
       "        <li>Review low-confidence extractions for accuracy</li>\n",
       "        <li>Save results to database (see next cells)</li>\n",
       "        <li>Query the GWAS_TRAIT_ANALYTICS table for analysis</li>\n",
       "        <li>Process additional PDFs to build your GWAS knowledge base</li>\n",
       "    </ol>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üì• EXPORT-READY DATA (copy/paste friendly)\n",
      "================================================================================\n",
      "\n",
      "Trait,Value,Confidence,Source\n",
      "Trait,\"\"\"BPH resistance\"\"\",MEDIUM,C_only\n",
      "Germplasm_Name,\"\"\"502 rice varieties diversity panel\"\"\",MEDIUM,B_C_agree\n",
      "Genome_Version,\"\"\"Nipponbare reference genome\"\"\",MEDIUM,C_only\n",
      "GWAS_Model,EMMAX,LOW,A_only\n",
      "Evidence_Type,\"\"\"GWAS\"\"\",MEDIUM,B_C_agree\n",
      "Chromosome,\"\"\"11\"\"\",LOW,B_only\n",
      "Physical_Position,,NONE,Not found\n",
      "Gene,\"\"\"LOC_Os06g03970, LOC_Os11g35890, LOC_Os11g35960, LOC_Os11g35980, LOC_Os11g36020, LOC_Os11g29030, LOC_Os11g29050, LOC_Os11g29110\"\"\",MEDIUM,C_only\n",
      "SNP_Name,\"\"\"rs6_922708\"\"\",MEDIUM,C_only\n",
      "Variant_ID,\"\"\"rs2_23955573\"\"\",MEDIUM,C_only\n",
      "Variant_Type,\"\"\"SNP\"\"\",MEDIUM,B_C_agree\n",
      "Effect_Size,\"\"\"19.69%\"\"\",MEDIUM,C_only\n",
      "Allele,,NONE,Not found\n",
      "Annotation,,NONE,Not found\n",
      "Candidate_Region,\"\"\"23.86-24.06 Mbp, 21.27-21.52 Mbp, 0.81-1.58 Mbp, 20.99-21.19 Mbp, 1.96-2.16 Mbp, 16.64-16.88 Mbp\"\"\",MEDIUM,C_only\n",
      "\n",
      "\n",
      "‚úÖ Copy the CSV above to export to spreadsheet or other tools!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STREAMLIT-STYLE VISUAL DISPLAY\n",
    "# ============================================================================\n",
    "# Mimics Streamlit's metric cards and data display\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Calculate metrics\n",
    "total_traits = len(traits_config_improved)\n",
    "extracted = len([v for v in final_results.values() if v])\n",
    "success_rate = (extracted / total_traits) * 100\n",
    "high_conf = sum(1 for c in confidence_levels.values() if c == \"HIGH\")\n",
    "medium_conf = sum(1 for c in confidence_levels.values() if c == \"MEDIUM\")\n",
    "low_conf = sum(1 for c in confidence_levels.values() if c == \"LOW\")\n",
    "\n",
    "# Generate HTML for metrics cards (Streamlit-style)\n",
    "html = f\"\"\"\n",
    "<style>\n",
    "    .metric-container {{\n",
    "        display: flex;\n",
    "        gap: 20px;\n",
    "        margin: 20px 0;\n",
    "        flex-wrap: wrap;\n",
    "    }}\n",
    "    .metric-card {{\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        border-radius: 10px;\n",
    "        padding: 20px;\n",
    "        min-width: 150px;\n",
    "        color: white;\n",
    "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "    }}\n",
    "    .metric-card.success {{\n",
    "        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);\n",
    "    }}\n",
    "    .metric-card.warning {{\n",
    "        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);\n",
    "    }}\n",
    "    .metric-card.info {{\n",
    "        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);\n",
    "    }}\n",
    "    .metric-label {{\n",
    "        font-size: 12px;\n",
    "        opacity: 0.9;\n",
    "        margin-bottom: 5px;\n",
    "    }}\n",
    "    .metric-value {{\n",
    "        font-size: 32px;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    .metric-delta {{\n",
    "        font-size: 14px;\n",
    "        margin-top: 5px;\n",
    "        opacity: 0.9;\n",
    "    }}\n",
    "    .data-table {{\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        margin: 20px 0;\n",
    "        background: white;\n",
    "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "        border-radius: 8px;\n",
    "        overflow: hidden;\n",
    "    }}\n",
    "    .data-table th {{\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        padding: 12px;\n",
    "        text-align: left;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    .data-table td {{\n",
    "        padding: 10px 12px;\n",
    "        border-bottom: 1px solid #e0e0e0;\n",
    "    }}\n",
    "    .data-table tr:last-child td {{\n",
    "        border-bottom: none;\n",
    "    }}\n",
    "    .data-table tr:hover {{\n",
    "        background-color: #f5f5f5;\n",
    "    }}\n",
    "    .status-badge {{\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "        font-weight: bold;\n",
    "    }}\n",
    "    .status-success {{\n",
    "        background-color: #e8f5e9;\n",
    "        color: #2e7d32;\n",
    "    }}\n",
    "    .status-error {{\n",
    "        background-color: #ffebee;\n",
    "        color: #c62828;\n",
    "    }}\n",
    "    .conf-high {{\n",
    "        background-color: #e3f2fd;\n",
    "        color: #1565c0;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    .conf-medium {{\n",
    "        background-color: #fff3e0;\n",
    "        color: #ef6c00;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    .conf-low {{\n",
    "        background-color: #fce4ec;\n",
    "        color: #c2185b;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    .conf-none {{\n",
    "        background-color: #f5f5f5;\n",
    "        color: #757575;\n",
    "        padding: 4px 8px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 11px;\n",
    "    }}\n",
    "    h2 {{\n",
    "        color: #333;\n",
    "        margin-top: 30px;\n",
    "    }}\n",
    "</style>\n",
    "\n",
    "<h2>üìä Extraction Metrics</h2>\n",
    "\n",
    "<div class=\"metric-container\">\n",
    "    <div class=\"metric-card success\">\n",
    "        <div class=\"metric-label\">TRAITS EXTRACTED</div>\n",
    "        <div class=\"metric-value\">{extracted}/{total_traits}</div>\n",
    "        <div class=\"metric-delta\">{success_rate:.1f}% success rate</div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"metric-card info\">\n",
    "        <div class=\"metric-label\">HIGH CONFIDENCE</div>\n",
    "        <div class=\"metric-value\">{high_conf}</div>\n",
    "        <div class=\"metric-delta\">{high_conf/total_traits*100:.0f}% of total</div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"metric-card info\">\n",
    "        <div class=\"metric-label\">MEDIUM CONFIDENCE</div>\n",
    "        <div class=\"metric-value\">{medium_conf}</div>\n",
    "        <div class=\"metric-delta\">{medium_conf/total_traits*100:.0f}% of total</div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"metric-card warning\">\n",
    "        <div class=\"metric-label\">LOW CONFIDENCE</div>\n",
    "        <div class=\"metric-value\">{low_conf}</div>\n",
    "        <div class=\"metric-delta\">{low_conf/total_traits*100:.0f}% of total</div>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<h2>üìã Extracted Traits Table</h2>\n",
    "\n",
    "<table class=\"data-table\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Status</th>\n",
    "            <th>Trait Name</th>\n",
    "            <th>Extracted Value</th>\n",
    "            <th>Confidence</th>\n",
    "            <th>Source Method</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "\"\"\"\n",
    "\n",
    "# Add rows for each trait\n",
    "for trait_name in traits_config_improved.keys():\n",
    "    value = final_results.get(trait_name)\n",
    "    confidence = confidence_levels.get(trait_name, \"NONE\")\n",
    "    source = field_citations.get(trait_name, \"Not found\")\n",
    "    \n",
    "    # Status badge\n",
    "    if value:\n",
    "        status_html = '<span class=\"status-badge status-success\">‚úì Found</span>'\n",
    "        value_display = str(value)[:80] if value else \"‚Äî\"\n",
    "    else:\n",
    "        status_html = '<span class=\"status-badge status-error\">‚úó Missing</span>'\n",
    "        value_display = '<span style=\"color: #999;\">Not found</span>'\n",
    "    \n",
    "    # Confidence badge\n",
    "    conf_class = f\"conf-{confidence.lower()}\"\n",
    "    conf_html = f'<span class=\"{conf_class}\">{confidence}</span>'\n",
    "    \n",
    "    html += f\"\"\"\n",
    "        <tr>\n",
    "            <td>{status_html}</td>\n",
    "            <td><strong>{trait_name}</strong></td>\n",
    "            <td>{value_display}</td>\n",
    "            <td>{conf_html}</td>\n",
    "            <td><code>{source}</code></td>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "\n",
    "html += \"\"\"\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "<div style=\"margin-top: 30px; padding: 15px; background: #f5f5f5; border-radius: 8px;\">\n",
    "    <h3 style=\"margin-top: 0; color: #333;\">üí° Interpretation Guide</h3>\n",
    "    <ul style=\"color: #666; line-height: 1.8;\">\n",
    "        <li><strong>High Confidence</strong>: All 3 extraction methods agree</li>\n",
    "        <li><strong>Medium Confidence</strong>: 2 out of 3 methods agree</li>\n",
    "        <li><strong>Low Confidence</strong>: Single method extraction (prefer multimodal &gt; text &gt; batch)</li>\n",
    "        <li><strong>Source Method</strong>: Which extraction approach(es) found this trait</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-top: 20px; padding: 15px; background: #e3f2fd; border-radius: 8px; border-left: 4px solid #1976d2;\">\n",
    "    <strong>üéØ Next Steps:</strong>\n",
    "    <ol style=\"color: #1565c0; line-height: 1.8; margin: 10px 0 0 0;\">\n",
    "        <li>Review low-confidence extractions for accuracy</li>\n",
    "        <li>Save results to database (see next cells)</li>\n",
    "        <li>Query the GWAS_TRAIT_ANALYTICS table for analysis</li>\n",
    "        <li>Process additional PDFs to build your GWAS knowledge base</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML\n",
    "display(HTML(html))\n",
    "\n",
    "# Also show a compact pandas summary for easy export\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì• EXPORT-READY DATA (copy/paste friendly)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "export_df = pd.DataFrame([\n",
    "    {\n",
    "        'Trait': name,\n",
    "        'Value': str(final_results.get(name)) if final_results.get(name) else '',\n",
    "        'Confidence': confidence_levels.get(name, 'NONE'),\n",
    "        'Source': field_citations.get(name, 'Not found')\n",
    "    }\n",
    "    for name in traits_config_improved.keys()\n",
    "])\n",
    "\n",
    "print(export_df.to_csv(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Copy the CSV above to export to spreadsheet or other tools!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Syngenta GWAS)",
   "language": "python",
   "name": "syngenta-gwas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
